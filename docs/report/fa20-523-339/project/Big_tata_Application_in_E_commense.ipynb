{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big tata Application in E-commense.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDo-XYMFlO8B"
      },
      "source": [
        "#**Big Data Application in E-commense**\n",
        "##——Customer Behavior analysis and recommendation\n",
        "###Project Contributor : **Tao Liu**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlF3xhpklyRE"
      },
      "source": [
        "In this project, we will use [Amazon Review Data](http://deepyeti.ucsd.edu/jianmo/amazon/index.html) and perform different kind of data anlysis methods to analyze the customer behaviors for buying good and then give recommendation based on anlyze. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcq51c8qp5-X"
      },
      "source": [
        "##**Step 0** - Package import\n",
        "All the packages will be imported here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_RF3A-VlMXd"
      },
      "source": [
        "import numpy\n",
        "import sklearn\n",
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "import requests\n",
        "import array"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOkCbwipoWqp"
      },
      "source": [
        "##**Step 1** - Data Implemtation and cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlGT-UTmpgLc"
      },
      "source": [
        "We will start with Amazon Review Data implementation and cleaning them if possible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9iLNk-g2pom",
        "outputId": "cb979556-7268-40f4-f5ef-734b5c69e079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Gift_Cards.json.gz"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-02 11:49:37--  http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Gift_Cards.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 380174 (371K) [application/octet-stream]\n",
            "Saving to: ‘meta_Gift_Cards.json.gz.2’\n",
            "\n",
            "meta_Gift_Cards.jso 100%[===================>] 371.26K  1.39MB/s    in 0.3s    \n",
            "\n",
            "2020-11-02 11:49:38 (1.39 MB/s) - ‘meta_Gift_Cards.json.gz.2’ saved [380174/380174]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUzdrHgNp1rf",
        "outputId": "0827a85b-2dfb-45c6-d472-4371c0ba6b66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def parse(path):\n",
        "    g = gzip.open(path, 'r')\n",
        "    for l in g:\n",
        "      yield json.dumps(eval(l))\n",
        "def getDF(path):\n",
        "    i = 0\n",
        "    df = {}\n",
        "    for d in parse(path):\n",
        "      df[i] = d\n",
        "      i += 1\n",
        "    return pd.DataFrame.from_dict(df, orient='index')\n",
        "def build_database():\n",
        "    f = open(\"output.strict\", 'w')\n",
        "    for l in parse(\"meta_Gift_Cards.json.gz\"):\n",
        "      (f.write(l + '\\n'))\n",
        "    df = getDF('meta_Gift_Cards.json.gz')[0]\n",
        "    database ={}\n",
        "    asin_list = []\n",
        "    for i in df:\n",
        "      dictionary = json.loads(i)\n",
        "      also_view = dictionary['also_view']\n",
        "      also_buy = dictionary['also_buy']\n",
        "      similar_item = dictionary['similar_item']\n",
        "      asin = dictionary['asin']\n",
        "      asin_list.append(asin)\n",
        "      sub_dictionary ={}\n",
        "      sub_dictionary['also_view'] = also_view\n",
        "      sub_dictionary['also_buy'] = also_buy \n",
        "      sub_dictionary['similar_item'] = similar_item\n",
        "      database[asin] = sub_dictionary\n",
        "    return asin_list, database\n",
        "asin_list, database = build_database()\n",
        "print(\"This is the asin number\",asin_list[1])\n",
        "print(\"This is the data it contained in dictonary\",database[asin_list[1]])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the asin number B001GXRQW0\n",
            "This is the data it contained in dictonary {'also_view': ['BT00DC6QU4', 'B01I4AHZXC', 'B0719C5P56', 'B01K8RLHZG', 'B00X4SHPFS', 'B01K8RL9C2', 'B01K8RJDEI', 'B01DCN6SFM', 'B01JQSONCC', 'B01K8RMDO0', 'B0091JKU5Q', 'B01C9MW8Z6', 'B0153R37XQ', 'B01K8RL0AI', 'BT00DDC7BK', 'B01L0KQ1WO', 'B06WVJBVT4', 'B06ZY43PDR', 'B072F9T6VX', 'B079ZR4DC8', 'B01K8RK2KW', 'B0084AVVOM', 'B0725JM87R', 'B01N5TMK8I', 'B071JKLGT5', 'B0753GRNQZ'], 'also_buy': [], 'similar_item': ''}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ainTO3HIA5V1"
      },
      "source": [
        "##**Step 2** - KNN Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raw6_xldBtVY"
      },
      "source": [
        "# here is KNN classifier we perform algorithm\n",
        "class KNN_Classifier:\n",
        "    def __init__(self, k):\n",
        "        self.k = k\n",
        "    # redefined since we have d-dimension attributes per dataset\n",
        "    def euclidean_distance(self, point1, point2):\n",
        "        num=0\n",
        "        if (len(point1)!=len(point2)):\n",
        "            print(\"it should never happened\")\n",
        "            pass\n",
        "        else:\n",
        "            for i in range(len(point1)):\n",
        "                num+= (point1[i] - point2[i]) * (point1[i] - point2[i])\n",
        "        return math.sqrt(num)\n",
        "    #pick the most frequent label\n",
        "    def pick_label(self, top_k_labels):\n",
        "        list=unique(top_k_labels)\n",
        "        current=0\n",
        "        mostfrequentlabel=None\n",
        "        for i in range(len(list[0])):\n",
        "            if list[1][i]>current:\n",
        "                current=list[1][i]\n",
        "                mostfrequentlabel=list[0][i]\n",
        "        return mostfrequentlabel\n",
        "    def classify(self, point, sample_points, sample_labels):\n",
        "        k=self.k\n",
        "        fun=lambda s:self.euclidean_distance(s,point)\n",
        "        lenth=len(sample_points)\n",
        "        label=[]\n",
        "        for i in range(lenth):\n",
        "            label.append((fun(sample_points[i]),sample_labels[i]))\n",
        "        ourlabel=[]\n",
        "        for i in range(k):\n",
        "            label.sort(key=lambda x:x[0])\n",
        "            value=heapq.heappop(label)\n",
        "            ourlabel.append(value[1])\n",
        "        return self.pick_label(ourlabel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrHD4nEEBw2C"
      },
      "source": [
        "##**Step 3** - Neural Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGsRkNPnBwcG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "#reference: https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5\n",
        "\n",
        "class NeuralNetworkClassifier:\n",
        "    def classify(self):\n",
        "            first = time.time() #assign first to be the start time\n",
        "            \n",
        "            dataset = pd.read_csv('african_crises.csv') #read the dataset\n",
        "\n",
        "            #data_x is the attributes that the dataset has\n",
        "            #data_y is the attributes that whether the country has crisis or not\n",
        "            data_x = dataset[['case', 'year', 'systemic_crisis', 'exch_usd', 'domestic_debt_in_default', 'sovereign_external_debt_default', 'gdp_weighted_default', 'inflation_annual_cpi', 'independence', 'currency_crises', 'inflation_crises']]\n",
        "            data_y = dataset['banking_crisis']\n",
        "            data_y = keras.utils.to_categorical(data_y, num_classes=None, dtype='float32')\n",
        "\n",
        "            #preparing data for training \n",
        "            X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size = 0.8, random_state=0)\n",
        "\n",
        "            #training the algorithm\n",
        "            #creating model sequentially and the output of each layer we add is input to the next layer we specify\n",
        "            model = Sequential()\n",
        "            model.add(Dense(10, input_dim = 11, activation = 'relu'))\n",
        "            model.add(Dense(5,activation='relu'))\n",
        "            model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "            #specify the loss function and optimizer\n",
        "            model.compile(loss='categorical_crossentropy', optimizer = 'adam',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "            #training model\n",
        "            history = model.fit(X_train, y_train, epochs = 10, batch_size = 64, verbose = 0)\n",
        "\n",
        "            #check the accuracy\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            pred = list()\n",
        "            for i in range(len(y_pred)):\n",
        "                pred.append(np.argmax(y_pred[i]))\n",
        "\n",
        "                test = list()\n",
        "            for i in range(len(y_test)):\n",
        "                test.append(np.argmax(y_test[i]))\n",
        "\n",
        "            a = accuracy_score(pred, test)\n",
        "            print('time for nn', time.time() - first)\n",
        "            return a\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}