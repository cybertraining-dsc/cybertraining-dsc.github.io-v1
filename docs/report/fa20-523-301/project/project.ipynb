{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZS0wkHPlyFO"
      },
      "source": [
        "# Final Project Notebook\n",
        "\n",
        "The report for this final project can be found at this [link](https://cybertraining-dsc.github.io/report/fa20-523-301/project/project/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvjolHp0lvEG"
      },
      "source": [
        "## Part 1 Importing the functions\n",
        "\n",
        "This file requires that we import Numpy, Matplotlib, Pylab, Keras, and Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBEQu5Gpl-eq"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "import os, sys\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import warnings\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from datetime import datetime"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36YZ6mkT5iGO",
        "outputId": "fd13aa15-a6f8-4c3d-8123-11af34a05c32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install cloudmesh-common -U\n",
        "\n",
        "from cloudmesh.common.Benchmark import Benchmark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: cloudmesh-common in /usr/local/lib/python3.6/dist-packages (4.3.26)\n",
            "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.6/dist-packages (from cloudmesh-common) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: pathlib in /usr/local/lib/python3.6/dist-packages (from cloudmesh-common) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: colorama in /usr/local/lib/python3.6/dist-packages (from cloudmesh-common) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: humanize in /usr/local/lib/python3.6/dist-packages (from cloudmesh-common) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from cloudmesh-common) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: simplejson in /usr/local/lib/python3.6/dist-packages (from cloudmesh-common) (3.17.2)\n",
            "Requirement already satisfied, skipping upgrade: oyaml in /usr/local/lib/python3.6/dist-packages (from cloudmesh-common) (1.0)\n",
            "Requirement already satisfied, skipping upgrade: python-hostlist in /usr/local/lib/python3.6/dist-packages (from cloudmesh-common) (1.21)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from cloudmesh-common) (0.8.7)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from cloudmesh-common) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from cloudmesh-common) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->cloudmesh-common) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->cloudmesh-common) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->cloudmesh-common) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->cloudmesh-common) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from oyaml->cloudmesh-common) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->cloudmesh-common) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntN4MMO7i02q",
        "outputId": "eedf19b5-ae8e-4e37-ebdc-9c8ce24e38d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install utils\n",
        "import utils"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i81DVIg2vo1t"
      },
      "source": [
        "Now that the funtions have been imported the team can focus on the download coding. The following cells will set up an install for Kaggle files and prompt for an upload of the kaggle.json file for credentials. \n",
        "\n",
        "The mkdir function creates a directory for the Kaggle data. This cell will allow the team to verify that the kaggle.json file appropriately uploaded to the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxduR8dEBwvz",
        "outputId": "49b9ba03-5f39-4502-f60e-485887edfb1d",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "##import the kaggle.json from local to drive\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "##when it asks you to choose a file select the kaggle.json located within the 'project' folder from the github repo\n",
        "files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7266125a-76c5-4651-8c3e-a5936bb0645b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7266125a-76c5-4651-8c3e-a5936bb0645b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"chelseagorius\",\"key\":\"0a34819ed937ff55d31f4288ab40cf19\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlgk1uL5D0Us",
        "outputId": "6002e809-935e-4cd0-b845-30e4147f4271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##make a kaggle and a data folder\n",
        "!mkdir ~/.kaggle\n",
        "!mkdir data\n",
        "##copy the kaggle.json to the .kaggle folder then grant permissions\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "#test to see if kaggle is working, should print list of datasets\n",
        "!kaggle datasets list"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)\n",
            "ref                                                       title                                               size  lastUpdated          downloadCount  \n",
            "--------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  \n",
            "terenceshin/covid19s-impact-on-airport-traffic            COVID-19's Impact on Airport Traffic               106KB  2020-10-19 12:40:17           1292  \n",
            "sootersaalu/amazon-top-50-bestselling-books-2009-2019     Amazon Top 50 Bestselling Books 2009 - 2019         15KB  2020-10-13 09:39:21           1261  \n",
            "thomaskonstantin/highly-rated-children-books-and-stories  Highly Rated Children Books And Stories            106KB  2020-10-24 12:09:59            289  \n",
            "tunguz/euro-parliament-proceedings-1996-2011              Euro Parliament Proceedings 1996 - 2011              1GB  2020-10-26 17:48:29             18  \n",
            "rishidamarla/judicial-expenditures-across-all-50-states   Judicial Expenditures across all 50 States           2KB  2020-10-25 00:07:45            194  \n",
            "docstein/brics-world-bank-indicators                      BRICS World Bank Indicators                          4MB  2020-10-22 12:18:40            277  \n",
            "kanishk307/6000-indian-food-recipes-dataset               6000+ Indian Food Recipes Dataset                    9MB  2020-10-24 01:08:23            362  \n",
            "elvinagammed/chatbots-intent-recognition-dataset          Chatbots: Intent Recognition Dataset                17KB  2020-10-23 07:44:59            152  \n",
            "omarhanyy/500-greatest-songs-of-all-time                  500 Greatest Songs of All Time                      33KB  2020-10-26 13:36:09            326  \n",
            "balraj98/synthetic-objective-testing-set-sots-reside      Synthetic Objective Testing Set (SOTS) [RESIDE]    415MB  2020-10-24 10:07:29             48  \n",
            "lunamcbride24/pokemon-type-matchup-data                   Pokemon Type Matchup Data                            9KB  2020-10-14 18:56:23            199  \n",
            "gaurav2796/kaggle-competions-rankings-and-kernels         Kaggle Competions, Rankings and Kernels            698KB  2020-10-15 04:05:15             28  \n",
            "balraj98/indoor-training-set-its-residestandard           Indoor Training Set (ITS) [RESIDE-Standard]          5GB  2020-10-24 10:07:30             29  \n",
            "romazepa/moscow-schools-winners-of-educational-olympiads  Moscow schools - winners of educational Olympiads    1MB  2020-10-12 21:45:01             45  \n",
            "sootersaalu/nigerian-songs-spotify                        Nigerian Songs Spotify                              24KB  2020-10-25 19:10:23             48  \n",
            "salmaneunus/mechanical-tools-dataset                      Mechanical Tools Classification Dataset            652MB  2020-11-01 11:28:22            155  \n",
            "thanatoz/hinglish-blogs                                   Hinglish blogs                                       2MB  2020-10-13 18:16:05             12  \n",
            "shivamb/netflix-shows                                     Netflix Movies and TV Shows                        971KB  2020-01-20 07:33:56          52366  \n",
            "nehaprabhavalkar/indian-food-101                          Indian Food 101                                      7KB  2020-09-30 06:23:43           5821  \n",
            "heeraldedhia/groceries-dataset                            Groceries dataset                                  257KB  2020-09-17 04:36:08           6021  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKEX-7tbwMK-"
      },
      "source": [
        "Now, the team must download all of the datasets for the class. The three datasets are focused on the NBA. \n",
        "\n",
        "The first dataset is for injuries. Each injury will be used to set up players, timeframes, and severity of injuries. \n",
        "\n",
        "The other two datasets are for the player performance. By cross referencing this data to the previous list, the team will be able to see which players are limited from the injury and how performance is hampered by time in rehab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2QYBlpIEFBi",
        "outputId": "a60fd527-1982-465c-c774-5bcdcea172d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##downloading all the datasets\n",
        "!kaggle datasets download -d ghopkins/nba-injuries-2010-2018\n",
        "!kaggle datasets download -d nathanlauga/nba-games\n",
        "!kaggle datasets download -d pablote/nba-enhanced-stats\n",
        "##unzipping to the data folder\n",
        "!unzip nba-injuries-2010-2018.zip -d data\n",
        "!unzip nba-games.zip -d data\n",
        "!unzip nba-enhanced-stats.zip -d data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading nba-injuries-2010-2018.zip to /content\n",
            "\r  0% 0.00/226k [00:00<?, ?B/s]\n",
            "100% 226k/226k [00:00<00:00, 72.9MB/s]\n",
            "Downloading nba-games.zip to /content\n",
            " 72% 13.0M/18.1M [00:00<00:00, 18.8MB/s]\n",
            "100% 18.1M/18.1M [00:00<00:00, 21.9MB/s]\n",
            "Downloading nba-enhanced-stats.zip to /content\n",
            " 54% 9.00M/16.7M [00:00<00:00, 23.3MB/s]\n",
            "100% 16.7M/16.7M [00:00<00:00, 37.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aIzY8a4wusX"
      },
      "source": [
        "The team must now use these downloads to create dataframes. Pandas dataframes will be easier to manage the data. The team will be able to use Pandas to process the data and allow the team to make correlations for feature engineering to create the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zmpmw-zImsz"
      },
      "source": [
        "#create a list for each data set\n",
        "ds_NBA_Injuries, ds_NBA_Games, ds_NBA_Enhanced = [], [], []\n",
        "\n",
        "#import csv files as dataframes and save to respective list, injury set first\n",
        "df_Injuries = pd.read_csv('data/injuries_2010-2020.csv')\n",
        "df_Injury_Start = df_Injuries[df_Injuries.Acquired.isnull()]\n",
        "df_Injury_End = df_Injuries[df_Injuries.Relinquished.isnull()]\n",
        "ds_NBA_Injuries = [df_Injury_Start, df_Injury_End]\n",
        "#nba games dataset\n",
        "df_Games_games = pd.read_csv('data/games.csv')\n",
        "df_Games_gamesDetails = pd.read_csv('data/games_details.csv')\n",
        "df_Games_players = pd.read_csv('data/players.csv')\n",
        "df_Games_ranking = pd.read_csv('data/ranking.csv')\n",
        "df_Games_teams = pd.read_csv('data/teams.csv')\n",
        "ds_NBA_Games = [df_Games_games, df_Games_gamesDetails, df_Games_players, df_Games_ranking, df_Games_teams]\n",
        "#nba enhanced stats dataset\n",
        "#df_En_officialBS_1218 = pd.read_csv('data/2012-18_officialBoxScore.csv')\n",
        "#df_En_playerBS_1218 = pd.read_csv('data/2012-18_playerBoxScore.csv')\n",
        "#df_En_standings_1218 = pd.read_csv('data/2012-18_standings.csv')\n",
        "#df_En_teamBS_1218 = pd.read_csv('data/2012-18_teamBoxScore.csv')  \n",
        "#df_En_officialBS_1617 = pd.read_csv('data/2016-17_officialBoxScore.csv')  \n",
        "#df_En_playerBS_1617 = pd.read_csv('data/2016-17_playerBoxScore.csv')\n",
        "#df_En_standings_1617 = pd.read_csv('data/2016-17_standings.csv')\n",
        "#df_En_teamBS_1617 = pd.read_csv('data/2016-17_teamBoxScore.csv')  \n",
        "#df_En_officialBS_1718 = pd.read_csv('data/2017-18_officialBoxScore.csv')  \n",
        "#df_En_playerBS_1718 = pd.read_csv('data/2017-18_playerBoxScore.csv')\n",
        "#df_En_standings_1718 = pd.read_csv('data/2017-18_standings.csv')\n",
        "#df_En_teamBS_1718 = pd.read_csv('data/2017-18_teamBoxScore.csv')  \n",
        "##data/metadata_officialBoxScore.pdf, data/metadata_playerBoxScore.pdf, data/metadata_standing.pdf, data/metadata_teamBoxScore.pdf  \n",
        "#df_En_teamBS = pd.read_csv('data/teamBoxScore.csv')\n",
        "#ds_NBA_Enhanced = [df_En_officialBS_1218, df_En_officialBS_1617, df_En_officialBS_1718, df_En_playerBS_1218, df_En_playerBS_1617, df_En_playerBS_1718, df_En_standings_1218, df_En_standings_1617, df_En_standings_1718, \\\n",
        "#                       df_En_teamBS_1218, df_En_teamBS_1617, df_En_teamBS_1718, df_En_teamBS]\n",
        "\n",
        "\n",
        "#probably need some more data exploration and some feature engineering"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aCknO5JCK-o"
      },
      "source": [
        "###Feature Engineering for Injury sets\n",
        "#####Goal is to have stats for injury game, average stats of last/first 5 games and maybe join season avg?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CviX4Qh0ASgW"
      },
      "source": [
        "#distinct player and player ID list\n",
        "df_distinct_playerID = df_Games_players[[\"PLAYER_NAME\", \"PLAYER_ID\"]].drop_duplicates()\n",
        "df_distinct_playerID.astype({'PLAYER_ID':'object'}).dtypes\n",
        "#distinct gameID and game date list\n",
        "df_Games_games['GAME_DATE_EST'] = pd.to_datetime(df_Games_games['GAME_DATE_EST'])\n",
        "df_distinct_gameId_date = df_Games_games[[\"GAME_ID\", \"GAME_DATE_EST\"]].drop_duplicates()\n",
        "#join player ID, for j=injury start db\n",
        "df_Injury_Start = df_Injury_Start.join(df_distinct_playerID.astype('object').set_index('PLAYER_NAME'), on='Relinquished')\n",
        "df_Injury_Start = df_Injury_Start.merge(df_Games_teams[[\"TEAM_ID\", \"NICKNAME\"]], left_on=\"Team\", right_on=\"NICKNAME\")\n",
        "df_Injury_Start = df_Injury_Start.drop(['NICKNAME'], axis=1)\n",
        "df_Injury_Start['Date']= pd.to_datetime(df_Injury_Start['Date'])#.apply(lambda x: x.date())\n",
        "#again for injury end db\n",
        "df_Injury_End = df_Injury_End.join(df_distinct_playerID.astype('object').set_index('PLAYER_NAME'), on='Acquired')\n",
        "df_Injury_End = df_Injury_End.merge(df_Games_teams[[\"TEAM_ID\", \"NICKNAME\"]], left_on=\"Team\", right_on=\"NICKNAME\")\n",
        "df_Injury_End = df_Injury_End.drop(['NICKNAME'], axis=1)\n",
        "df_Injury_End['Date']= pd.to_datetime(df_Injury_End['Date'])#.apply(lambda x: x.date())\n",
        "# df_distinct_playerID=df_distinct_playerID.sort_values('PLAYER_NAME')\n",
        "df_Games_gamesDetails = df_Games_gamesDetails.merge(df_distinct_gameId_date, on=\"GAME_ID\")\n",
        "\n",
        "# df_distinct_playerID = df_distinct_playerID.sort_values(by=['PLAYER_NAME']).reset_index(drop=True, inplace=True)\n",
        "#df_Injury_End"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFpaEfk4w_DG"
      },
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "At this point, it is time to build into new useful sets of data. The team will explore different sets to combine in to the models to be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrtNcltKkVUC"
      },
      "source": [
        "### **To be deleted later**\n",
        "This code is put in to make the dataset much smaller. The datasets will normally be done on the larger set, but for buildup we want to use a smaller subset so the training does not take hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53OfEv6ihDVU"
      },
      "source": [
        "### GH_Add ## Slicing Rows to make it easier to build up program\n",
        "df_Games_gamesDetails_orig = df_Games_gamesDetails.copy()\n",
        "df_Games_gamesDetails = df_Games_gamesDetails[0:201].copy()\n",
        "\n",
        "df_Injury_Start_orig = df_Injury_Start.copy()\n",
        "df_Injury_Start = df_Injury_Start[0:201].copy()\n",
        "\n",
        "df_Injury_End_orig = df_Injury_End.copy()\n",
        "df_Injury_End = df_Injury_End[0:201].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGAu05JixyFQ",
        "outputId": "42d4e8df-73a3-4f6e-9f33-748713170e13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for index, row in df_Games_gamesDetails.iterrows():\n",
        "  try:\n",
        "    m, s = str(row.MIN).split(':')\n",
        "  except (SyntaxError, ValueError) as e:\n",
        "    m = (row.MIN)\n",
        "    s = 0\n",
        "  df_Games_gamesDetails.loc[index,'MIN'] = pd.to_numeric(m) + pd.to_numeric(s)/60\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rI2ZY9PDGIv"
      },
      "source": [
        "for index, row in df_Injury_Start.iterrows():\n",
        "        #games of just that player\n",
        "        temp = df_Games_gamesDetails.loc[df_Games_gamesDetails['PLAYER_ID'] == row.PLAYER_ID]\n",
        "        #games before and inlucding injury date\n",
        "        inj_game = temp.loc[(temp['GAME_DATE_EST'] == row.Date)]\n",
        "        #5 games prior and the game of injury, for some reason we need to have 4 different variabels, did not work with resetting the variable 'game_set' to itself\n",
        "        temp2 = temp.loc[(temp['GAME_DATE_EST'] <= row.Date)]\n",
        "        game_set = temp2.nlargest(6, 'GAME_DATE_EST')\n",
        "        if len(game_set) > 0:\n",
        "          #injury game\n",
        "          inj_game = game_set.iloc[0]\n",
        "          #5 games prior to injury\n",
        "          prior5 = game_set.iloc[1:]\n",
        "          #storing game data from injury game\n",
        "          df_Injury_Start.at[index, 'inj_MIN'] = inj_game[['MIN']].MIN\n",
        "          df_Injury_Start.at[index,'inj_FGA'] = inj_game[['FGA']].FGA\n",
        "          df_Injury_Start.at[index,'inj_FG_PCT'] = inj_game[['FG_PCT']].FG_PCT\n",
        "          df_Injury_Start.at[index,'inj_FG3A'] = inj_game[['FG3A']].FG3A\n",
        "          df_Injury_Start.at[index,'inj_FG3_PCT'] = inj_game[['FG3_PCT']].FG3_PCT\n",
        "          df_Injury_Start.loc[index,'inj_FTA'] = inj_game[['FTA']].FTA\n",
        "          df_Injury_Start.loc[index,'inj_FT_PCT'] = inj_game[['FT_PCT']].FT_PCT\n",
        "          df_Injury_Start.loc[index,'inj_REB'] = inj_game[['REB']].REB\n",
        "          df_Injury_Start.loc[index,'inj_AST'] = inj_game[['AST']].AST\n",
        "          df_Injury_Start.loc[index,'inj_STL'] = inj_game[['STL']].STL\n",
        "          df_Injury_Start.loc[index,'inj_BLK'] = inj_game[['BLK']].BLK\n",
        "          df_Injury_Start.loc[index,'inj_TO'] = inj_game[['TO']].TO\n",
        "          df_Injury_Start.loc[index,'inj_PF'] = inj_game[['PF']].PF\n",
        "          df_Injury_Start.loc[index,'inj_PTS'] = inj_game[['PTS']].PTS\n",
        "          df_Injury_Start.loc[index,'inj_PLUS_MINUS'] = inj_game[['PLUS_MINUS']].PLUS_MINUS\n",
        "#storing game data from prior 5 games\n",
        "          df_Injury_Start.at[index,'p5_MIN'] = prior5[['MIN']].MIN.mean()\n",
        "          df_Injury_Start.at[index,'p5_FGA'] = prior5[['FGA']].FGA.mean()\n",
        "          df_Injury_Start.at[index,'p5_FG_PCT'] = prior5[['FG_PCT']].FG_PCT.mean()\n",
        "          df_Injury_Start.at[index,'p5_FG3A'] = prior5[['FG3A']].FG3A.mean()\n",
        "          df_Injury_Start.at[index,'p5_FG3_PCT'] = prior5[['FG3_PCT']].FG3_PCT.mean()\n",
        "          df_Injury_Start.at[index,'p5_FTA'] = prior5[['FTA']].FTA.mean()\n",
        "          df_Injury_Start.at[index,'p5_FT_PCT'] = prior5[['FT_PCT']].FT_PCT.mean()\n",
        "          df_Injury_Start.at[index,'p5_REB'] = prior5[['REB']].REB.mean()\n",
        "          df_Injury_Start.at[index,'p5_AST'] = prior5[['AST']].AST.mean()\n",
        "          df_Injury_Start.at[index,'p5_STL'] = prior5[['STL']].STL.mean()\n",
        "          df_Injury_Start.at[index,'p5_BLK'] = prior5[['BLK']].BLK.mean()\n",
        "          df_Injury_Start.at[index,'p5_TO'] = prior5[['TO']].TO.mean()\n",
        "          df_Injury_Start.at[index,'p5_PF'] = prior5[['PF']].PF.mean()\n",
        "          df_Injury_Start.at[index,'p5_PTS'] = prior5[['PTS']].PTS.mean()\n",
        "          df_Injury_Start.at[index,'p5_PLUS_MINUS'] = prior5[['PLUS_MINUS']].PLUS_MINUS.mean()\n",
        "          \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udzmFjS-ivuC",
        "outputId": "53cf0c66-71ef-49fe-9049-2e5c1b893bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#df_Injury_End\n",
        "for index, row in df_Injury_End.iterrows():\n",
        "        #games of just that player\n",
        "        temp = df_Games_gamesDetails.loc[df_Games_gamesDetails['PLAYER_ID'] == row.PLAYER_ID]\n",
        "        #games before and inlucding injury date\n",
        "        temp2 = temp.loc[(temp['GAME_DATE_EST'] >= row.Date)]\n",
        "        #5 games prior and the game of injury, for some reason we need to have 4 different variabels, did not work with resetting the variable 'game_set' to itself\n",
        "        game_set = temp.nsmallest(6, 'GAME_DATE_EST')\n",
        "        if len(game_set) > 0:\n",
        "          #injury game\n",
        "          inj_game = game_set.iloc[0]\n",
        "          #5 games post injury\n",
        "          post5 = game_set.iloc[1:]\n",
        "          #storing game data from injury game\n",
        "          df_Injury_End.at[index, 'inj_MIN'] = inj_game[['MIN']].MIN\n",
        "          df_Injury_End.at[index,'inj_FGA'] = inj_game[['FGA']].FGA\n",
        "          df_Injury_End.at[index,'inj_FG_PCT'] = inj_game[['FG_PCT']].FG_PCT\n",
        "          df_Injury_End.at[index,'inj_FG3A'] = inj_game[['FG3A']].FG3A\n",
        "          df_Injury_End.at[index,'inj_FG3_PCT'] = inj_game[['FG3_PCT']].FG3_PCT\n",
        "          df_Injury_End.loc[index,'inj_FTA'] = inj_game[['FTA']].FTA\n",
        "          df_Injury_End.loc[index,'inj_FT_PCT'] = inj_game[['FT_PCT']].FT_PCT\n",
        "          df_Injury_End.loc[index,'inj_REB'] = inj_game[['REB']].REB\n",
        "          df_Injury_End.loc[index,'inj_AST'] = inj_game[['AST']].AST\n",
        "          df_Injury_End.loc[index,'inj_STL'] = inj_game[['STL']].STL\n",
        "          df_Injury_End.loc[index,'inj_BLK'] = inj_game[['BLK']].BLK\n",
        "          df_Injury_End.loc[index,'inj_TO'] = inj_game[['TO']].TO\n",
        "          df_Injury_End.loc[index,'inj_PF'] = inj_game[['PF']].PF\n",
        "          df_Injury_End.loc[index,'inj_PTS'] = inj_game[['PTS']].PTS\n",
        "          df_Injury_End.loc[index,'inj_PLUS_MINUS'] = inj_game[['PLUS_MINUS']].PLUS_MINUS\n",
        "          #storing game data from prior 5 games\n",
        "          df_Injury_End.at[index,'p5_MIN'] = post5[['MIN']].MIN.mean()\n",
        "          df_Injury_End.at[index,'p5_FGA'] = post5[['FGA']].FGA.mean()\n",
        "          df_Injury_End.at[index,'p5_FG_PCT'] = post5[['FG_PCT']].FG_PCT.mean()\n",
        "          df_Injury_End.at[index,'p5_FG3A'] = post5[['FG3A']].FG3A.mean()\n",
        "          df_Injury_End.at[index,'p5_FG3_PCT'] = post5[['FG3_PCT']].FG3_PCT.mean()\n",
        "          df_Injury_End.at[index,'p5_FTA'] = post5[['FTA']].FTA.mean()\n",
        "          df_Injury_End.at[index,'p5_FT_PCT'] = post5[['FT_PCT']].FT_PCT.mean()\n",
        "          df_Injury_End.at[index,'p5_REB'] = post5[['REB']].REB.mean()\n",
        "          df_Injury_End.at[index,'p5_AST'] = post5[['AST']].AST.mean()\n",
        "          df_Injury_End.at[index,'p5_STL'] = post5[['STL']].STL.mean()\n",
        "          df_Injury_End.at[index,'p5_BLK'] = post5[['BLK']].BLK.mean()\n",
        "          df_Injury_End.at[index,'p5_TO'] = post5[['TO']].TO.mean()\n",
        "          df_Injury_End.at[index,'p5_PF'] = post5[['PF']].PF.mean()\n",
        "          df_Injury_End.at[index,'p5_PTS'] = post5[['PTS']].PTS.mean()\n",
        "          df_Injury_End.at[index,'p5_PLUS_MINUS'] = post5[['PLUS_MINUS']].PLUS_MINUS.mean()\n",
        "        #print(inj_game)\n",
        "\n",
        "        #print(inj_game)\n",
        "        #print(prior5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.obj[key] = _infer_fill_value(value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNgn64ED-L8X"
      },
      "source": [
        "## Adding\n",
        "Starting to build up injury performance for model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOnCFzEe-Tt8",
        "outputId": "1cdd2f21-2b55-42e3-f431-f6a387aecdf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "prior5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d630eef2f218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprior5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'prior5' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLp742E5ftIr"
      },
      "source": [
        "# Part 2 Building the Keras Model\n",
        "\n",
        "A link for Keras for us to use can be found [here](https://keras.io/guides/sequential_model/). We are first going to set up our Benchmark Test to be used when we are Benchmarking our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnUZ7nOe41w_"
      },
      "source": [
        "def b():\n",
        "  Benchmark.Start()\n",
        "  print (\"b\")\n",
        "  import time\n",
        "  time.sleep(3)\n",
        "  Benchmark.Stop()\n",
        "\n",
        "def c():\n",
        "  Benchmark.Start()\n",
        "  print (\"c\")\n",
        "  import time\n",
        "  time.sleep(1)\n",
        "  Benchmark.Stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c637S5T65JYz",
        "outputId": "a1487acf-d677-430a-c647-2c9739d938b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " b()\n",
        " c()\n",
        "\n",
        " Benchmark.print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\n",
            "c\n",
            "\n",
            "+---------------------+------------------------------------------------------------------+\n",
            "| Attribute           | Value                                                            |\n",
            "|---------------------+------------------------------------------------------------------|\n",
            "| BUG_REPORT_URL      | \"https://bugs.launchpad.net/ubuntu/\"                             |\n",
            "| DISTRIB_CODENAME    | bionic                                                           |\n",
            "| DISTRIB_DESCRIPTION | \"Ubuntu 18.04.5 LTS\"                                             |\n",
            "| DISTRIB_ID          | Ubuntu                                                           |\n",
            "| DISTRIB_RELEASE     | 18.04                                                            |\n",
            "| HOME_URL            | \"https://www.ubuntu.com/\"                                        |\n",
            "| ID                  | ubuntu                                                           |\n",
            "| ID_LIKE             | debian                                                           |\n",
            "| NAME                | \"Ubuntu\"                                                         |\n",
            "| PRETTY_NAME         | \"Ubuntu 18.04.5 LTS\"                                             |\n",
            "| PRIVACY_POLICY_URL  | \"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" |\n",
            "| SUPPORT_URL         | \"https://help.ubuntu.com/\"                                       |\n",
            "| UBUNTU_CODENAME     | bionic                                                           |\n",
            "| VERSION             | \"18.04.5 LTS (Bionic Beaver)\"                                    |\n",
            "| VERSION_CODENAME    | bionic                                                           |\n",
            "| VERSION_ID          | \"18.04\"                                                          |\n",
            "| cpu_count           | 2                                                                |\n",
            "| mem.active          | 1.3 GiB                                                          |\n",
            "| mem.available       | 11.5 GiB                                                         |\n",
            "| mem.free            | 8.8 GiB                                                          |\n",
            "| mem.inactive        | 2.4 GiB                                                          |\n",
            "| mem.percent         | 9.8 %                                                            |\n",
            "| mem.total           | 12.7 GiB                                                         |\n",
            "| mem.used            | 1.2 GiB                                                          |\n",
            "| platform.version    | #1 SMP Thu Jul 23 08:00:38 PDT 2020                              |\n",
            "| python              | 3.6.9 (default, Oct  8 2020, 12:12:24)                           |\n",
            "|                     | [GCC 8.4.0]                                                      |\n",
            "| python.pip          | 19.3.1                                                           |\n",
            "| python.version      | 3.6.9                                                            |\n",
            "| sys.platform        | linux                                                            |\n",
            "| uname.machine       | x86_64                                                           |\n",
            "| uname.node          | 0d2725e52de4                                                     |\n",
            "| uname.processor     | x86_64                                                           |\n",
            "| uname.release       | 4.19.112+                                                        |\n",
            "| uname.system        | Linux                                                            |\n",
            "| uname.version       | #1 SMP Thu Jul 23 08:00:38 PDT 2020                              |\n",
            "| user                | collab                                                           |\n",
            "+---------------------+------------------------------------------------------------------+\n",
            "\n",
            "+-----------------------------------+----------+--------+-------+---------------------+-------+--------------+--------+-------+-------------------------------------+\n",
            "| Name                              | Status   |   Time |   Sum | Start               | tag   | Node         | User   | OS    | Version                             |\n",
            "|-----------------------------------+----------+--------+-------+---------------------+-------+--------------+--------+-------+-------------------------------------|\n",
            "| <ipython-input-20-124396bcabb3>/b | ok       |  3.035 | 6.069 | 2020-11-01 20:53:06 |       | 0d2725e52de4 | collab | Linux | #1 SMP Thu Jul 23 08:00:38 PDT 2020 |\n",
            "| <ipython-input-20-124396bcabb3>/c | ok       |  1.033 | 2.063 | 2020-11-01 20:53:09 |       | 0d2725e52de4 | collab | Linux | #1 SMP Thu Jul 23 08:00:38 PDT 2020 |\n",
            "+-----------------------------------+----------+--------+-------+---------------------+-------+--------------+--------+-------+-------------------------------------+\n",
            "\n",
            "# csv,timer,status,time,sum,start,tag,uname.node,user,uname.system,platform.version\n",
            "# csv,<ipython-input-20-124396bcabb3>/b,ok,3.035,6.069,2020-11-01 20:53:06,,0d2725e52de4,collab,Linux,#1 SMP Thu Jul 23 08:00:38 PDT 2020\n",
            "# csv,<ipython-input-20-124396bcabb3>/c,ok,1.033,2.063,2020-11-01 20:53:09,,0d2725e52de4,collab,Linux,#1 SMP Thu Jul 23 08:00:38 PDT 2020\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2wmn5BA8MGQ"
      },
      "source": [
        "Now that we know which GPU we are using, we can get into the actual work. The following is building our Keras model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK-TMhHEdiT-"
      },
      "source": [
        "np.random.seed(23)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZlPVjuNdotN"
      },
      "source": [
        "df_baseline = df_Injury_End\n",
        "sort_by = 'Acquired'\n",
        "\n",
        "#df_baseline.sort_values(by=['Date','Name']).reset_index(drop=True, inplace=True)\n",
        "df_baseline.sort_values(by=[sort_by]).reset_index(drop=True, inplace=True)\n",
        "# df_baseline['FPTS_pred'] = utils.calculate_FPTS(df_baseline)\n",
        "\n",
        "# # Season average\n",
        "# print(' MAE | ', utils.calculate_MAE(df_baseline['FPTS_pred'], df_baseline['FPTS']))\n",
        "# print('RMSE | ', utils.calculate_RMSE(df_baseline['FPTS_pred'], df_baseline['FPTS']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZXcuk6XgHIJ",
        "outputId": "b641777a-022b-4ff5-bdaa-c0c3e0d28869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        }
      },
      "source": [
        "df_baseline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Team</th>\n",
              "      <th>Acquired</th>\n",
              "      <th>Relinquished</th>\n",
              "      <th>Notes</th>\n",
              "      <th>PLAYER_ID</th>\n",
              "      <th>TEAM_ID</th>\n",
              "      <th>NICKNAME</th>\n",
              "      <th>inj_MIN</th>\n",
              "      <th>inj_FGA</th>\n",
              "      <th>inj_FG_PCT</th>\n",
              "      <th>inj_FG3A</th>\n",
              "      <th>inj_FG3_PCT</th>\n",
              "      <th>inj_FTA</th>\n",
              "      <th>inj_FT_PCT</th>\n",
              "      <th>inj_REB</th>\n",
              "      <th>inj_AST</th>\n",
              "      <th>inj_STL</th>\n",
              "      <th>inj_BLK</th>\n",
              "      <th>inj_TO</th>\n",
              "      <th>inj_PF</th>\n",
              "      <th>inj_PTS</th>\n",
              "      <th>inj_PLUS_MINUS</th>\n",
              "      <th>p5_MIN</th>\n",
              "      <th>p5_FGA</th>\n",
              "      <th>p5_FG_PCT</th>\n",
              "      <th>p5_FG3A</th>\n",
              "      <th>p5_FG3_PCT</th>\n",
              "      <th>p5_FTA</th>\n",
              "      <th>p5_FT_PCT</th>\n",
              "      <th>p5_REB</th>\n",
              "      <th>p5_AST</th>\n",
              "      <th>p5_STL</th>\n",
              "      <th>p5_BLK</th>\n",
              "      <th>p5_TO</th>\n",
              "      <th>p5_PF</th>\n",
              "      <th>p5_PTS</th>\n",
              "      <th>p5_PLUS_MINUS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-10-03</td>\n",
              "      <td>Heat</td>\n",
              "      <td>Jerry Stackhouse</td>\n",
              "      <td>NaN</td>\n",
              "      <td>activated from IL</td>\n",
              "      <td>711</td>\n",
              "      <td>1610612748</td>\n",
              "      <td>Heat</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-10-27</td>\n",
              "      <td>Heat</td>\n",
              "      <td>Jamaal Magloire</td>\n",
              "      <td>NaN</td>\n",
              "      <td>activated from IL</td>\n",
              "      <td>2048</td>\n",
              "      <td>1610612748</td>\n",
              "      <td>Heat</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-11-24</td>\n",
              "      <td>Heat</td>\n",
              "      <td>Mario Chalmers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>returned to lineup</td>\n",
              "      <td>201596</td>\n",
              "      <td>1610612748</td>\n",
              "      <td>Heat</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-11-26</td>\n",
              "      <td>Heat</td>\n",
              "      <td>Juwan Howard</td>\n",
              "      <td>NaN</td>\n",
              "      <td>activated from IL</td>\n",
              "      <td>436</td>\n",
              "      <td>1610612748</td>\n",
              "      <td>Heat</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-11-27</td>\n",
              "      <td>Heat</td>\n",
              "      <td>Jamaal Magloire</td>\n",
              "      <td>NaN</td>\n",
              "      <td>activated from IL</td>\n",
              "      <td>2048</td>\n",
              "      <td>1610612748</td>\n",
              "      <td>Heat</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>2013-12-16</td>\n",
              "      <td>Heat</td>\n",
              "      <td>Tyler Johnson</td>\n",
              "      <td>NaN</td>\n",
              "      <td>returned to lineup</td>\n",
              "      <td>204020</td>\n",
              "      <td>1610612748</td>\n",
              "      <td>Heat</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>2013-12-18</td>\n",
              "      <td>Heat</td>\n",
              "      <td>Dwyane Wade</td>\n",
              "      <td>NaN</td>\n",
              "      <td>activated from IL</td>\n",
              "      <td>2548</td>\n",
              "      <td>1610612748</td>\n",
              "      <td>Heat</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>2013-12-19</td>\n",
              "      <td>Heat</td>\n",
              "      <td>Chris Andersen</td>\n",
              "      <td>NaN</td>\n",
              "      <td>returned to lineup</td>\n",
              "      <td>2365</td>\n",
              "      <td>1610612748</td>\n",
              "      <td>Heat</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>2013-12-19</td>\n",
              "      <td>Heat</td>\n",
              "      <td>Hassan Whiteside</td>\n",
              "      <td>NaN</td>\n",
              "      <td>activated from IL</td>\n",
              "      <td>202355</td>\n",
              "      <td>1610612748</td>\n",
              "      <td>Heat</td>\n",
              "      <td>31.1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.643</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>2013-12-21</td>\n",
              "      <td>Heat</td>\n",
              "      <td>Luol Deng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>returned to lineup</td>\n",
              "      <td>2736</td>\n",
              "      <td>1610612748</td>\n",
              "      <td>Heat</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>201 rows  38 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Date  Team          Acquired  ... p5_PF p5_PTS p5_PLUS_MINUS\n",
              "0   2010-10-03  Heat  Jerry Stackhouse  ...   NaN    NaN           NaN\n",
              "1   2010-10-27  Heat   Jamaal Magloire  ...   NaN    NaN           NaN\n",
              "2   2010-11-24  Heat    Mario Chalmers  ...   NaN    NaN           NaN\n",
              "3   2010-11-26  Heat      Juwan Howard  ...   NaN    NaN           NaN\n",
              "4   2010-11-27  Heat   Jamaal Magloire  ...   NaN    NaN           NaN\n",
              "..         ...   ...               ...  ...   ...    ...           ...\n",
              "196 2013-12-16  Heat     Tyler Johnson  ...   NaN    NaN           NaN\n",
              "197 2013-12-18  Heat       Dwyane Wade  ...   NaN    NaN           NaN\n",
              "198 2013-12-19  Heat    Chris Andersen  ...   NaN    NaN           NaN\n",
              "199 2013-12-19  Heat  Hassan Whiteside  ...   NaN    NaN           NaN\n",
              "200 2013-12-21  Heat         Luol Deng  ...   NaN    NaN           NaN\n",
              "\n",
              "[201 rows x 38 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewhp5QtUf7bB"
      },
      "source": [
        "#df_baseline.sort_values(by=['Date','Name']).reset_index(drop=True, inplace=True)\n",
        "df_baseline.sort_values(by=[sort_by]).reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "# df_baseline['\tPLAYER_ID'] = utils.calculate_FPTS(df_baseline)\n",
        "\n",
        "# # Season average\n",
        "# print(' MAE | ', utils.calculate_MAE(df_baseline['FPTS_pred'], df_baseline['FPTS']))\n",
        "# print('RMSE | ', utils.calculate_RMSE(df_baseline['FPTS_pred'], df_baseline['FPTS']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zucYExMnJYbm"
      },
      "source": [
        "# Part No X. Building the Model\n",
        "\n",
        "The team is now moving on to building the model for the baseline. Linear Regression can be used to model the values. Additionally, a Random Forest modeling function was used to verify model performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34VDFf8WJWut"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb-NC9l9KPaF"
      },
      "source": [
        "The pipeline will be built off of using df_baseline.PLAYER_ID. To change the data going into the model, the team had to modify the dataframe input to get to the results. Comments were put around to make it easy to find the code in the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5MLXJXId2j1"
      },
      "source": [
        "#df_baseline = df_baseline\n",
        "# basic =  ['PTS','3P','AST','TRB','STL','BLK','TOV', 'DD', 'TD']\n",
        "\n",
        "###################################\n",
        "##                               ##\n",
        "##     INSERT CODE               ##\n",
        "##                               ##\n",
        "##     Change DF before here     ##\n",
        "##                               ##\n",
        "##                               ##\n",
        "##                               ##\n",
        "###################################\n",
        "\n",
        "X = df_baseline.PLAYER_ID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdAiOOwBLLW8",
        "outputId": "f3f6a491-96bf-4f57-c4eb-6d4797c99cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "X = X.reshape(-1, 1)\n",
        "# X_reshape=X.reshape(-1, 1)\n",
        "# X = df_baseline.loc[:, basic]\n",
        "X = MinMaxScaler().fit_transform(X)\n",
        "print(X.shape)\n",
        "# Y = df_baseline['FPTS'].values.reshape(-1,1).flatten()\n",
        "Y = df_baseline.values.reshape(-1,1).flatten() # Y is 38 times larger. Not sure what I did here. \n",
        "Y = Y.reshape(-1, 1) \n",
        "\n",
        "size_x = X.shape[0]\n",
        "size_y = Y.shape[0]\n",
        "size_y = (size_y/size_x)\n",
        "print(size_y)\n",
        "\n",
        "Y = Y.reshape((size_x, size_y)) # Y is 29 times larger. Not sure what I did here. \n",
        "\n",
        "\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=101)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "# rf=RandomForestClassifier(max_depth=8,n_estimators=5)\n",
        "\n",
        "# scores = cross_validate(lasso, X, Y, cv=3, scoring=('r2', 'neg_mean_squared_error'), return_train_score=True)\n",
        "\n",
        "\n",
        "reg_cv_score=cross_val_score(estimator=lin_reg,X=X_train,y=Y_train,cv=5)\n",
        "print(reg_cv_score)\n",
        "\n",
        "# errors = utils.cross_val(reg, X, y, n_folds=5, verbose=0)\n",
        "# utils.summarize_errors(errors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0cc1034b3dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# X_reshape=X.reshape(-1, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# X = df_baseline.loc[:, basic]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uYM0wZGKFDK"
      },
      "source": [
        "The model has been built and trained on 2/3 of the data with a test on 1/3 of the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXuXIaPzd9Dm"
      },
      "source": [
        "# When the dataframes are combined, use this code to select features.\n",
        "\n",
        "features = ['SG', 'F', 'C', 'PTS', '3P', 'AST', 'TRB', 'STL', 'BLK', 'TOV', 'DD', 'TD', 'MP', 'FT',\n",
        "            'FTA', 'FGA', '3PA', 'DRB', 'ORB', 'USG_perc', 'DRtg', 'ORtg', 'AST_perc', 'DRB_perc',\n",
        "            'ORB_perc', 'BLK_perc', 'TOV_perc', 'STL_perc', 'eFG_perc', 'FG_perc', '3P_perc', 'FT_perc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2IhyvVJeK9m"
      },
      "source": [
        "_all = ['Salary', 'Rest', 'Rota_All', 'Rota_Pos', 'Home', 'SG', 'F', 'C', 'Value', 'FPTS_std',\n",
        "        'PTS', '3P', 'AST', 'TRB', 'STL', 'BLK', 'TOV', 'DD', 'TD', 'MP', 'FT', 'FTA', 'FGA', '3PA', 'DRB',\n",
        "        'ORB', 'USG_perc', 'DRtg', 'ORtg', 'AST_perc', 'DRB_perc', 'ORB_perc', 'BLK_perc', 'TOV_perc', \n",
        "        'STL_perc', 'eFG_perc', 'FG_perc', '3P_perc', 'FT_perc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s21vTbJeOWH",
        "outputId": "a0653d14-d555-4e84-faa9-da6f3c7e946a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "### This should work when we start passing numbers. It will help pick best features.\n",
        "\n",
        "\n",
        "# X was called above\n",
        "\n",
        "X = MinMaxScaler().fit_transform(X)\n",
        "# y = df_features['FPTS'].values.reshape(-1,1).flatten()\n",
        "\n",
        "Y = df_baseline.values.reshape(-1,1).flatten() # Y is 38 times larger. Not sure what I did here. \n",
        "Y = Y.reshape(-1, 1) \n",
        "Y = Y.reshape((16894, 38)) # Y is 38 times larger. Not sure what I did here. \n",
        "\n",
        "# Takes 2 minutes\n",
        "# clf.set_params(n_estimators=2000)\n",
        "# clf.fit(X, y, sample_weight=train_weight)\n",
        "\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X, Y)\n",
        "\n",
        "top_features = pd.Series(model.feature_importances_, index = _all).sort_values()\n",
        "top_features.plot(kind = \"barh\", figsize=(15,10) ,title='Top Features')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-ecb49d3f98d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtop_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1439\u001b[0m         \u001b[0;31m# Since check_array converts both X and y to the same dtype, but the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# trees use different types for X and y, checking them separately.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhzQuuh_eU9O"
      },
      "source": [
        "omit_lowest = 20\n",
        "_selected = list(top_features[omit_lowest:].index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24geWtZsMKS2"
      },
      "source": [
        "# Building the Keras Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AA48g-jftpp",
        "outputId": "194419b5-aa7e-4d5b-9648-65d8a7e7d934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "# Define Sequential model with 3 layers\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(2, activation=\"relu\", name=\"layer1\"),\n",
        "        layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n",
        "        layers.Dense(4, name=\"layer3\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# x = df_baseline\n",
        "# x = tf.ones((3, 3))\n",
        "Y = model(X)\n",
        "\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 2.91940524e-07  3.08297296e-07 -3.65564912e-07 -7.21618691e-07]\n",
            " [ 2.91940524e-07  3.08297296e-07 -3.65564912e-07 -7.21618691e-07]\n",
            " [ 2.70738259e-05  2.85907116e-05 -3.39015655e-05 -6.69210926e-05]\n",
            " ...\n",
            " [ 2.18450302e-04  2.30689582e-04 -2.73541215e-04 -5.39965535e-04]\n",
            " [ 2.70726179e-05  2.85894359e-05 -3.39000528e-05 -6.69181066e-05]\n",
            " [ 2.18703317e-04  2.30956773e-04 -2.73858038e-04 -5.40590938e-04]], shape=(16894, 4), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLGlczbef3pZ"
      },
      "source": [
        "Insert Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCQf0RZ8f20Z",
        "outputId": "7e108cdb-8616-4304-93ba-8e69b80257f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# Create 3 layers\n",
        "layer1 = layers.Dense(2, activation=\"relu\", name=\"layer1\")\n",
        "layer2 = layers.Dense(3, activation=\"relu\", name=\"layer2\")\n",
        "layer3 = layers.Dense(4, name=\"layer3\")\n",
        "\n",
        "# Call layers on a test input\n",
        "# x = df_baseline\n",
        "Y = layer3(layer2(layer1(X)))\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]], shape=(16894, 4), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgrBDgnp_r-W"
      },
      "source": [
        "# Part 3 Conclusions\n",
        "\n",
        "This is where the conclusions section will be typed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne27vROk_r-W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}