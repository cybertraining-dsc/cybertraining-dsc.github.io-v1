<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.79.1"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Cybertraining</title><meta property="og:title" content><meta property="og:description" content="Online Store Customer Revenue Prediction   Status: final, Type: Project
  please follow our template
  Please add references - Facing issues with adding reference. Need assistance
  Please correct the images with correct markdown syntax.
  usage of italic vs quotes
  wrong indentation level when doing lists, student does not do markdown
  wrong indentation while using paragraphs they must not be indented"><meta property="og:type" content="article"><meta property="og:url" content="/report/fa20-523-337/project/project/"><meta property="og:site_name" content="Cybertraining"><meta itemprop=name content><meta itemprop=description content="Online Store Customer Revenue Prediction   Status: final, Type: Project
  please follow our template
  Please add references - Facing issues with adding reference. Need assistance
  Please correct the images with correct markdown syntax.
  usage of italic vs quotes
  wrong indentation level when doing lists, student does not do markdown
  wrong indentation while using paragraphs they must not be indented"><meta itemprop=wordCount content="4856"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Online Store Customer Revenue Prediction   Status: final, Type: Project
  please follow our template
  Please add references - Facing issues with adding reference. Need assistance
  Please correct the images with correct markdown syntax.
  usage of italic vs quotes
  wrong indentation level when doing lists, student does not do markdown
  wrong indentation while using paragraphs they must not be indented"><link rel=preload href=/scss/main.min.541f105c34f11dd207a9775a00ff9f1f41884f48abc84ab786959ab04f5fa8a0.css as=style><link href=/scss/main.min.541f105c34f11dd207a9775a00ff9f1f41884f48abc84ab786959ab04f5fa8a0.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zM197.0804 232.033c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zM197.0839 232.0372c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zM197.0839 232.0372c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zM197.0804 177.6188c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zM197.0839 177.623c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zM197.0839 177.623c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zM197.5309 286.4723c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zM197.5344 286.4765c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zM197.5344 286.4765c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zM197.0804 340.5784c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zM197.0839 340.5826c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zM197.0839 340.5826c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg></span><span class="text-uppercase font-weight-bold">Cybertraining</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/courses/><span>Courses</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/modules/><span>Modules</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/tutorial/><span>Tutorials</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/report/><span class=active>Reports</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/report/2021/><span>Reports 2021</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/contrib/><span>Contributing</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><br><ul><li><a href=/courses/ai-first/>AI-First</a></li><ul><li><a href=/modules/ai-first/2021/course_lectures/>Lectures</a></li><li><a href=/modules/ai-first/2021/introduction/>Introduction</a></li><li><a href=/courses/ai-first>Other Material</a></li><li><a href=/report/2021>Reports</a></li></ul></ul><hr><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center"><button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type=button data-toggle=collapse data-target=#td-section-nav aria-controls=td-docs-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="collapse td-sidebar-nav" id=td-section-nav><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=/report/ class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section"><div style=border:1px;border-style:solid;border-color:#d1d1d1;padding:0>Reports</div></a></li><ul><li class="collapse show" id=report><a class="td-sidebar-link td-sidebar-link__page" id=m-report2021 href=/report/2021/>Reports 2021</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapicloudmeshopenapireadme href=/report/cloudmesh-openapi/cloudmesh/openapi/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapicloudmeshopenapiscikitlearnreadme href=/report/cloudmesh-openapi/cloudmesh/openapi/scikitlearn/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapideprecatedopenapireadme href=/report/cloudmesh-openapi/deprecated/openapi/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapideprecatedpaperresultsinstallcloud_lscpu href=/report/cloudmesh-openapi/deprecated/paper/results/install/cloud_lscpu/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapidockerubuntu1910todo href=/report/cloudmesh-openapi/docker/ubuntu19.10/todo/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapidockerubuntu2004-sklearntodo href=/report/cloudmesh-openapi/docker/ubuntu20.04-sklearn/todo/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapidockerubuntu2004todo href=/report/cloudmesh-openapi/docker/ubuntu20.04/todo/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapiproject_review href=/report/cloudmesh-openapi/project_review/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapireadme-adam href=/report/cloudmesh-openapi/readme-adam/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapireadme-eigenfaces-test href=/report/cloudmesh-openapi/readme-eigenfaces-test/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapireadme-scikitlearn href=/report/cloudmesh-openapi/readme-scikitlearn/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapireadme-security href=/report/cloudmesh-openapi/readme-security/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapireadme href=/report/cloudmesh-openapi/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapitestsadd-floatreadme href=/report/cloudmesh-openapi/tests/add-float/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapitestsadd-jsonreadme href=/report/cloudmesh-openapi/tests/add-json/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapitestsgenerator-natural-langgooglecloudvmset href=/report/cloudmesh-openapi/tests/generator-natural-lang/googlecloudvmset/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapitestsgregorreadme href=/report/cloudmesh-openapi/tests/gregor/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapitestsimage-analysisreadme href=/report/cloudmesh-openapi/tests/image-analysis/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapitestsreadme href=/report/cloudmesh-openapi/tests/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapitestsserver-cpureadme href=/report/cloudmesh-openapi/tests/server-cpu/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapiteststest_mlperfreadme-source href=/report/cloudmesh-openapi/tests/test_mlperf/readme-source/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapiteststest_mlperfreadme href=/report/cloudmesh-openapi/tests/test_mlperf/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapiteststest_mlperfresultsreadme href=/report/cloudmesh-openapi/tests/test_mlperf/results/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportcloudmesh-openapiteststimeseries-examplereadme href=/report/cloudmesh-openapi/tests/timeseries-example/readme/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-301assignment6assignment6 href=/report/fa20-523-301/assignment6/assignment6/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-301projectmisc_filesblank href=/report/fa20-523-301/project/misc_files/blank/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-301projectplan href=/report/fa20-523-301/project/plan/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-301projectproject href=/report/fa20-523-301/project/project/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-301test href=/report/fa20-523-301/test/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-302assignment6wearables_and_ai href=/report/fa20-523-302/assignment6/wearables_and_ai/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-302projectplan href=/report/fa20-523-302/project/plan/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-302projectproject href=/report/fa20-523-302/project/project/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-304projectproject href=/report/fa20-523-304/project/project/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-304reportreport href=/report/fa20-523-304/report/report/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-304test href=/report/fa20-523-304/test/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-305homework3cody_harris_hw3 href=/report/fa20-523-305/homework3/cody_harris_hw3/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-305homework6cody_harris_hw6 href=/report/fa20-523-305/homework6/cody_harris_hw6/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-305projectproject href=/report/fa20-523-305/project/project/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-305test href=/report/fa20-523-305/test/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-307assignment6assignment6 href=/report/fa20-523-307/assignment6/assignment6/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-307projectproject href=/report/fa20-523-307/project/project/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-308hw7task_3_next_steps href=/report/fa20-523-308/hw7/task_3_next_steps/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-308projectproject href=/report/fa20-523-308/project/project/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-309projectproject href=/report/fa20-523-309/project/project/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-312assignment6assignment6 href=/report/fa20-523-312/assignment6/assignment6/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-312projectproject href=/report/fa20-523-312/project/project/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-313assignment5 href=/report/fa20-523-313/assignment5/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-313assignment6assignment6 href=/report/fa20-523-313/assignment6/assignment6/></a><a class="td-sidebar-link td-sidebar-link__page" id=m-reportfa20-523-313projectproject href=/report/fa20-523-313/project/project/></a></li></ul></ul></nav></div></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"><a href=https://github.com/cybertraining-dsc/cybertraining-dsc.github.io/edit/main/content/en/report/fa20-523-337/project/project.md target=_blank><i class="fa fa-edit fa-fw"></i>Edit this page</a>
<a href="https://github.com/cybertraining-dsc/cybertraining-dsc.github.io/issues/new?title=" target=_blank><i class="fab fa-github fa-fw"></i>Create documentation issue</a>
<a href=https://github.com/cybertraining-dsc/cybertraining-dsc.github.io/issues/new target=_blank><i class="fas fa-tasks fa-fw"></i>Create project issue</a></div><nav id=TableOfContents><ul><li><a href=#1-introduction>1. Introduction</a></li><li><a href=#2-datasets>2. Datasets</a><ul><li><a href=#21-metrics>2.1 Metrics</a></li><li><a href=#22-source-code>2.2 Source Code</a></li></ul></li><li><a href=#3-methodology>3. Methodology</a><ul><li><a href=#31-load-data>3.1 Load Data</a></li><li><a href=#32-data-exploration>3.2 Data Exploration</a><ul><li><a href=#321-exploratory-data-analysis>3.2.1 Exploratory Data Analysis</a></li></ul></li><li><a href=#33-data-pre-processing>3.3 Data Pre-Processing</a></li><li><a href=#34-feature-engineering>3.4 Feature Engineering</a></li><li><a href=#34-feature-selection>3.4 Feature Selection</a><ul><li><a href=#341-data-preparation>3.4.1 Data Preparation</a></li></ul></li><li><a href=#35-model-algorithms-and-optimization-methods>3.5 Model Algorithms and Optimization Methods</a><ul><li><a href=#351-linear-regression-model>3.5.1 Linear Regression Model</a></li><li><a href=#352-xgboost-regressor>3.5.2 XGBoost Regressor</a></li><li><a href=#353-lightgbm-regressor>3.5.3 LightGBM Regressor</a></li><li><a href=#354-lasso-regression>3.5.4 Lasso Regression</a></li><li><a href=#355-ridge-regressor>3.5.5 Ridge Regressor</a></li></ul></li></ul></li><li><a href=#4-benchmark-results>4. Benchmark Results</a></li><li><a href=#5-software-technologies>5. Software Technologies</a></li><li><a href=#6-conclusion>6. Conclusion</a><ul><li><a href=#61-model-pipeline>6.1 Model Pipeline</a></li><li><a href=#62-feature-exploration-and-pre-processing>6.2 Feature Exploration and Pre-Processing</a></li><li><a href=#63-outcome-of-experiments>6.3 Outcome of Experiments</a></li><li><a href=#64-limitations>6.4 Limitations</a></li></ul></li><li><a href=#7-previous-explorations>7. Previous Explorations</a></li><li><a href=#8-acknowlegements>8. Acknowlegements</a></li><li><a href=#9-references>9. References</a></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><nav aria-label=breadcrumb class="d-none d-md-block d-print-none"><ol class="breadcrumb spb-1"><li class=breadcrumb-item><a href=/report/>Reports</a></li><li class="breadcrumb-item active" aria-current=page><a href=/report/fa20-523-337/project/project/></a></li></ol></nav><div class=td-content><h1></h1><h1 id=online-store-customer-revenue-prediction>Online Store Customer Revenue Prediction</h1><p><a href=https://github.com/cybertraining-dsc/fa20-523-337/actions><img src=https://github.com/cybertraining-dsc/fa20-523-337/workflows/Check%20Report/badge.svg alt="Check Report"></a>
<a href=https://github.com/cybertraining-dsc/fa20-523-337/actions><img src=https://github.com/cybertraining-dsc/fa20-523-337/workflows/Status/badge.svg alt=Status></a>
Status: final, Type: Project</p><ul><li><p><input checked disabled type=checkbox> please follow our template</p></li><li><p><input checked disabled type=checkbox> Please add references - Facing issues with adding reference. Need assistance</p></li><li><p><input checked disabled type=checkbox> Please correct the images with correct markdown syntax.</p></li><li><p><input checked disabled type=checkbox> usage of italic vs quotes</p></li><li><p><input checked disabled type=checkbox> wrong indentation level when doing lists, student does not do markdown</p></li><li><p><input checked disabled type=checkbox> wrong indentation while using paragraphs they must not be indented</p></li><li><p><input checked disabled type=checkbox> wrong indentation when using images, images must not be in bullet lists but stands long, images must be refered to in text as Figure x</p></li><li><p><input checked disabled type=checkbox> missing empty line before captions</p></li><li><p><input checked disabled type=checkbox> future considerations should be renamed to Limitations.</p></li><li><p><input checked disabled type=checkbox> there are no : in headers such as in future considerations:</p></li><li><p><input checked disabled type=checkbox> use grammerly</p></li><li><p><input checked disabled type=checkbox> your tables are unreadable. There are different ways on how to do this. You can include the parameters as text and the rest as markdown table</p></li><li><p><input checked disabled type=checkbox> hid from second author wrong</p></li><li><p><input checked disabled type=checkbox> you are not looking at the output of the check report script some errors are listed there</p></li><li><p><input checked disabled type=checkbox> the word below and above must never be used in a formal paper to refer to figures and tables and sections, use numbers as we posted in piszza</p></li><li><p><input checked disabled type=checkbox> bullet lists must not be used in substitution for subsections. You could <strong>bf</strong>. them and do not use a bullet similar to LaTeX paragraphs if you do not want to use subsections.SUbsections show up in the TOC, <strong>bf</strong>. does not</p></li><li><p><input checked disabled type=checkbox> no explanation is provided what the different regression are, no citations provided</p></li><li><p><input checked disabled type=checkbox> all figures must have captions (below)</p></li><li><p><input checked disabled type=checkbox> all tables must have captions (above)</p></li><li><p><input checked disabled type=checkbox> This is not a ppt presentations. for example</p><ul><li>Kaggle - Customer Revenue Prediction</li></ul><p>is not a full sentence and must not be used to start a section</p></li></ul><p>Balaji Dhamodharan, <a href=mailto:bdhamodh@iu.edu>bdhamodh@iu.edu</a>, fa20-523-337
Anantha Janakiraman, <a href=mailto:ajanakir@iu.edu>ajanakir@iu.edu</a>, fa20-523-351</p><p><a href=https://github.com/cybertraining-dsc/fa20-523-337/edit/main/project/project.md>Edit</a></p><div class="pageinfo pageinfo-primary"><h2 id=abstract>Abstract</h2><p><strong>Situation</strong> The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies. The objective of this project is to explore different machine learning techniques and identify an optimized model that can help the marketing team understand customer behavior and make informed decisions.</p><p><strong>Task</strong> The challenge is to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully this exploration will lead to actionable insights and help allocating marketing budgets for those companies who choose to use data analysis on top of GA data [^1].</p><p><strong>Action</strong> This exploration is based on a Kaggle competition and there are two datasets available in Kaggle. One is the test dataset (test.csv) and the other one is the training dataset (train.csv) and together the datasets contain customer transaction information ranging from August 2016 to April 2018. The action plan for this project is to first conduct data exploration that includes but not limited to investigating the statistics of data, examining the target variable distribution and other data distributions visually, determining imputation strategy based on the nature of missing data, exploring different techniques to scale the data, identifying features that may not be needed - for example, columns with constant variance, exploring different encoding techniques to convert categorical data to numerical data and identifying features with high collinearity. The preprocessed data will then be trained using a linear regression model with basic parameter setting and K-Fold cross validation. Based on the outcome of this initial model further experimentation will be conducted to tune the hyper parameters including regularization and also add new derived features to improve the accuracy of the model. Apart from linear regression other machine learning techniques like ensemble methods will be explored and compared.</p><p><strong>Result</strong> The best performing model determined based on the RMSE value will be used in the inference process to predict the revenue per customer. The Kaggle competition requires to predict the natural log of sum of all transactions per customer</p><p>Contents</p><div class=toc><nav id=TableOfContents><ul><li><a href=#1-introduction>1. Introduction</a></li><li><a href=#2-datasets>2. Datasets</a><ul><li><a href=#21-metrics>2.1 Metrics</a></li><li><a href=#22-source-code>2.2 Source Code</a></li></ul></li><li><a href=#3-methodology>3. Methodology</a><ul><li><a href=#31-load-data>3.1 Load Data</a></li><li><a href=#32-data-exploration>3.2 Data Exploration</a><ul><li><a href=#321-exploratory-data-analysis>3.2.1 Exploratory Data Analysis</a></li></ul></li><li><a href=#33-data-pre-processing>3.3 Data Pre-Processing</a></li><li><a href=#34-feature-engineering>3.4 Feature Engineering</a></li><li><a href=#34-feature-selection>3.4 Feature Selection</a><ul><li><a href=#341-data-preparation>3.4.1 Data Preparation</a></li></ul></li><li><a href=#35-model-algorithms-and-optimization-methods>3.5 Model Algorithms and Optimization Methods</a><ul><li><a href=#351-linear-regression-model>3.5.1 Linear Regression Model</a></li><li><a href=#352-xgboost-regressor>3.5.2 XGBoost Regressor</a></li><li><a href=#353-lightgbm-regressor>3.5.3 LightGBM Regressor</a></li><li><a href=#354-lasso-regression>3.5.4 Lasso Regression</a></li><li><a href=#355-ridge-regressor>3.5.5 Ridge Regressor</a></li></ul></li></ul></li><li><a href=#4-benchmark-results>4. Benchmark Results</a></li><li><a href=#5-software-technologies>5. Software Technologies</a></li><li><a href=#6-conclusion>6. Conclusion</a><ul><li><a href=#61-model-pipeline>6.1 Model Pipeline</a></li><li><a href=#62-feature-exploration-and-pre-processing>6.2 Feature Exploration and Pre-Processing</a></li><li><a href=#63-outcome-of-experiments>6.3 Outcome of Experiments</a></li><li><a href=#64-limitations>6.4 Limitations</a></li></ul></li><li><a href=#7-previous-explorations>7. Previous Explorations</a></li><li><a href=#8-acknowlegements>8. Acknowlegements</a></li><li><a href=#9-references>9. References</a></li></ul></nav></div></div><p><strong>Keywords:</strong> ecommerce, regression analysis, big data</p><h2 id=1-introduction>1. Introduction</h2><p>The objective of this exploration is to predict the natural log of total revenue per customer which is a real valued continuous output and an algorithm like linear regression will be ideal to predict the response variable that is continuous using a set of predictor variables given the basic assumption that there is a linear relationship between the predictor and response variables.</p><h2 id=2-datasets>2. Datasets</h2><p>As mentioned in the earlier sections, the dataset used in this model exploration was downloaded from Kaggle <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> and available in CSV file format. The training contains more than 872K observations and based on the size of the dataset it would be ideal to use mini-batch or gradient descent optimization techniques to identify the coefficients that best describe the model. The target variable as observed in the dataset is a continuous variable which implies that the use case is a regression problem. As mentioned earlier, there are several machine learning techniques that can be explored for this type of problem including regression and ensemble methods with different parameter settings. The sparsity of potential features in the datasets indicates that multiple experimentations will be required to determine the best performing model. Also based on initial review of the datasets, it also observed that some of the categorical features exhibit low to medium cardinality and if these features are going to be retained in the final dataset used for training then it is important to choose the right encoding technique.</p><ul><li>Train.csv User transactions from August 1st, 2016 to August 1st, 2017 <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></li><li>Test.csv User transactions from August 2nd, 2017 to April 30th, 2018 <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></li></ul><h3 id=21-metrics>2.1 Metrics</h3><p>The metrics used for evaluation in this analysis is the root mean squared error (RMSE). The root mean squared error function forms the objective/cost function which will be minimized to estimate optimal parameters for the linear function using Gradient Descent. The plan is to conduct multiple experiments with different iterations to obtain convergence and try different hyper-parameters (e.g. learning rate).</p><p>RMSE is defined as:</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/loss.png alt="Figure 1.1"></p><p><strong>Figure 1:</strong> RMSE</p><p>where y-hat is the natural log of the predicted revenue for a customer and y is the natural log of the actual summed revenue value plus one as seen in Figure-1.</p><h3 id=22-source-code>2.2 Source Code</h3><p>Follow this <a href=https://github.com/cybertraining-dsc/fa20-523-337/blob/main/project/code/project.ipynb>link</a> to the source code for subsequent sections in this report.</p><h2 id=3-methodology>3. Methodology</h2><p>The CRISP-DM process methodology was followed in this project. The high-level implementation steps are shown in Figure-2.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/methodology.png alt=Methodology></p><p><strong>Figure 2:</strong> Project Methodology</p><h3 id=31-load-data>3.1 Load Data</h3><p>The data that was obtained from Kaggle was over 2.6 GB (for Train and Test). As the size of the dataset was significantly large, it was hosted onto a storage bucket in Google Cloud Platform and ingested into the modeling process through standard application API libraries. Also in this project from the available datasets, the Train dataset was used to build the models and test the results because the real end goal is to test the effectiveness of algorithm. Since the test set doesn&rsquo;t contain the Target Variable (rightly so!), it will not be consumed during the testing and evaluation phase in this exploration.</p><h3 id=32-data-exploration>3.2 Data Exploration</h3><p>The dataset obtained for this project is large and it contains over 872k records. The dataset also contains 12 predictor variables and 1 target variable. The target Variable is totals.transactionRevenue and the objective of this exploration is to predict the total transaction revenue of an online store customer as accurately as possible.</p><p><strong>Target Variable:</strong> The Target Variable is totals.transactionRevenue has the transaction value of each visit. But, this column contains 98.72% of missing values for revenue (no purchase). The Target variable had a skewed distribution originally and after performing a lognormal distribution on the target variable it has a normal distribution.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/y_after_transformation.png alt="Target Variable"></p><p><strong>Figure 3:</strong> Target Variable</p><h4 id=321-exploratory-data-analysis>3.2.1 Exploratory Data Analysis</h4><p><strong>Browser:</strong> The most popular browser is Google Chrome. Also, it was observed during the analysis that second and third best users were using safari and firefox respectively.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/device_browser.png alt="Browser Variable"></p><p><strong>Figure 4:</strong> Browser Variable</p><p><strong>Device Category:</strong> Almost 70% of users were accessing online store via desktop</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/device_category.png alt="Device Category Variable"></p><p><strong>Figure 5:</strong> Device Category Variable</p><p><strong>OperatingSystem:</strong> Windows is the popular operating system among the desktop users. However, among the mobile users, what&rsquo;s interesting is, almost equal number of ios users (slightly lower) as android users accessed google play store. The reason why this is interesting is because, google play store is primarily used by android users and ios users almost always use Apple Store for downloading apps to their mobile devices. So it is interesting to know, almost equal number of ios users visit google store as well.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/device_operating_system.png alt=OperatingSystem></p><p><strong>Figure 6:</strong> Operating System</p><p><strong>GeoNetwork-City:</strong> Mountain View, California tops the cities list for the users who accessed online store. However in the top 10 cities, 4 cities are from California.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/geo_network_city.png alt=GeoNetwork-City></p><p><strong>Figure 7:</strong> GeoNetwork City</p><p><strong>GeoNetwork-Country:</strong> Customers from US are way ahead of other customers from different countries. May be this could be due to the fact that online store data that was provided was pulled from US Google Play Store (Possible BIAS!).</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/geo_network_country.png alt=GeoNetwork-Country></p><p><strong>Figure 8:</strong> GeoNetwork Country</p><p><strong>GeoNetwork-Region:</strong> It is already known that majority of the customers are from US, so America region tops the list.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/geo_network_continent.png alt=GeoNetwork-Region></p><p><strong>Figure 9:</strong> GeoNetwork Region</p><p><strong>GeoNetwork-Metro:</strong> SFO tops the list for all metro cities, followed by New York and then London.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/geo_network_metro.png alt=GeoNetwork-Metro></p><p><strong>Figure 10:</strong> GeoNetwork Region</p><p><strong>Ad Sources:</strong> Google Merchandise and Google Online Store are the top sources where the traffic is coming from to the Online Store.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/traffic_source_adcontent.png alt=GeoNetwork-Metro></p><p><strong>Figure 11:</strong> Ad Sources</p><h3 id=33-data-pre-processing>3.3 Data Pre-Processing</h3><p>Data Pre-Processing is an important step to build a Machine Learning Model. The data pre-processing step typically consists of data cleaning, transformation, standardization and feature selection, so only the most cleaner and accurate data is fed to a model. The dataset that was downloaded for this project contains several issues with formatting, lot of missing values, less to no variance (zero variance) in some of features and it was also observed the target variable does not have random distribution. The variables such as totals_newVisits, totals_bounces, trafficSource_adwordsClickInfo_page, trafficSource_isTrueDirect, totals_bounces, totals_newVisits had missing values. The missing values were imputed with zeroes, so the machine learning algorithm is able to execute without errors and there are no issues during categorical to numerical encoding. This is a very important step in building the Machine Learning Pipeline.</p><h3 id=34-feature-engineering>3.4 Feature Engineering</h3><p>Feature Engineering is the process of extracting the hidden signals/features, so the model can use these features to increase the predictive power. This step is the fundamental difference between a good model and a bad model. Also, there is no one-size-fits all approach for Feature Engineering. It is extremely time consuming and requires a lot of domain knowledge as well.
For this project, a set of sophisticated functions to extract date related values such as Month, Year, Data, Weekday, WeekofYear have been created. It was observed that browser and operating systems are redundant features and instead of removing them, they were merged to create a combined feature which will potentially increase the predictive power. Also as part of feature engineering several features were derived like mean, sum and count of pageviews and hits that should help increase the feature space and ultimately reduce the total error increasing the overall accuracy of the model.</p><h3 id=34-feature-selection>3.4 Feature Selection</h3><p>Feature Selection refers to selection of features in your data that would improve your machine learning model. There is subtle variation between Feature Selection and Feature Engineering. The Feature Engineering technique is designed to extract more feature from the dataset and the feature selection technique allows only relevant features into the dataset. Also, how does anyone know what are the relevant features? There are several methodologies and techniques that are developed over the years but there is no one-size-fits-all methodology.</p><p>Feature Selection like Feature Engineering is more of an art than science. There are several iterative procedure that uses Information Gain, Entropy and Correlation scores to decide which feature gets into the model. There are also advanced Deep learning models that can be built or tree based models that can help observe variables of high importance after the model is built. Similar to Feature Engineering, Feature Selection should also require domain specific knowledge to develop festure selection strategies.</p><p>In this project, the features that had constant variance in the data were dropped and also the features that had mostly null values with only one Non-null value were dropped too. These features do not possess any statistical significance and add very less value to the modeling process. Also, depending on the final result, different techniques and strategies can be explored to optimize and improve the performance of the model.</p><h4 id=341-data-preparation>3.4.1 Data Preparation</h4><p>Scikit learn has inbuilt libraries to handle Train/Test Split as part the model_selection package. The dataset was split randomly with 80% Training and 20% Testing datasets.</p><h3 id=35-model-algorithms-and-optimization-methods>3.5 Model Algorithms and Optimization Methods</h3><p>Different Machine Learning algorithms and techniques were explored in this project and the outcome of the exploration along with different parameter settings have been discussed in the following sections.</p><h4 id=351-linear-regression-model>3.5.1 Linear Regression Model</h4><p>Linear regression is a supervised learning model that follows a linear approach in that it assumes a linear relationship between one ore more predictor variables (x) and a single target or response variable (y). The target variable can be calculated as a linear combination of the predictors or in other words the target is the calculated by weighted sum of the inputs and bias where the weights are estimated through different optimization techniques. Linear regression is referred to as simple linear regression when there is only one predictor variable involved and referred to as multiple linear regression when there are more than one predictor variables involved. The error between the predicted output and the ground truth is generally calculated using RMSE (root mean squared error). This is one of the classic modeling techniques that was explored in this project because the target variable (revenue per customer) is a real valued continuous output and exhibits a significant linear relationship with the independent variables or the input variables <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><p>In the exploration, SKLearn Linear Regression performed well overall. A 5 fold cross validation was performed and the best RMSE Score for this model observed was: 1.89. As shown in the Figure-12, the training and test RMSE error values are very close indicating that there is no overfitting the data.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/linear_regression3.png alt="Linear Regression"></p><p><strong>Figure 12:</strong> Linear Regression Model</p><h4 id=352-xgboost-regressor>3.5.2 XGBoost Regressor</h4><p>XGBoost regression is a gradient boosting regression technique and one of the popular gradient boosting frameworks that exists today. It follows the ensemble principle where a collection of weak learners improve the prediction accuracy. The prediction in the current step S is weighed based on the outcomes from the previous step S-1. Weak learning is slightly better than random learning and that is one of the key strengths of gradient boosting technique. The XGBoost algorithm was explored for this project for several reasons including it offers built-in regularization that helps avoid overfitting, it can handle missing values effectively and it also does cross validation automatically. The feature space for the dataset being used is sparse and believe the potential to overfit the data is high which is one of the primary reasons for exploring XGBoost <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup><sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup><sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</p><p>XGBoost Regressor performed very well. It was the best performing model with the lowest RMSE score of 1.619. Also the training and test scores are reasonably close and it doesn&rsquo;t look like there was the problem of over fitting the training data. Multiple training iterations of this model were explored with different parameters and most of the iterations resulted in significant error reduction compared to the other models making it the best performing model overall.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/xgboost3new.png alt=XGBoost></p><p><strong>Figure 13:</strong> XGBoost Model</p><h4 id=353-lightgbm-regressor>3.5.3 LightGBM Regressor</h4><p>LightGBM is a popular gradient boosting framework similar to XGBoost and is gaining popularity in the recent days. The important difference between lightGBM and other gradient boosting frameworks is that LightGBM grows the tree vertically or in other words it grows the tree leaf-wise compared to other frameworks where the trees grow horizontally. In this project the lightGBM framework was experimented primarily because this framework works well on large dataset with more than 10K observations. The algorithm also has a high throughput while using reasonably less memory but there is one problem with overfitting the data which was controlled to a large extent in this exploration using appropriate hyper parameter setting and achieved optimal performance <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup><sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>.</p><p>LightGBM Regression was the second best performing model in terms of RMSE scores. Also the training and test scores observed were slightly different indicating a potential problem of overfitting as discussed earlier. As in other experiments, multiple training iterations of this model were explored with different parameter settings and although it achieved reasonable error reduction compared to most of the other models that were explored, it still did not outperform the XGBoost regressor making it the second best performing model.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/lighgbm3new.png alt=lightgbm></p><p><strong>Figure 14:</strong> LightGBM Model</p><h4 id=354-lasso-regression>3.5.4 Lasso Regression</h4><p>Lasso is a regression technique that uses L1 regularization. In statistics, lasso regression is a method to do automatic variable selection and regularization to improve prediction accuracy and performance of the statistical model. Lasso regression by nature makes the coefficient for some of the variables zero meaning these variables are automatically eliminated from the modeling process. The L1 regularization parameter helps control overfitting and will need to be explored for a range of values for a specific problem. When the regularization penalty tends to be zero there is no regularization, and the loss function is mostly influenced by the squared loss and in contrary if the regularization penalty tends to be closer to infinity then the objective function is mostly influenced by the regularization part. It is always ideal to explore a range of values for the regularization penalty to improve the accuracy and avoid overfitting <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup><sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>.</p><p>In this project, Lasso is one of the important techniques that was explored primarily because the problem being solved is a regression problem and there is possibility to overfit the data due to the number of observations and feature space. During the model training phase different ranges for regularization penalty were explored and the appropriate value that helped achieve maximum reduction in the total RMSE score was identified.</p><p>Lasso performed slightly better than baseline model. However, it did not outperform lightGBM or XGBoost or tree based models in general.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/lasso3new.png alt=lasso></p><p><strong>Figure 15:</strong> Lasso Model</p><h4 id=355-ridge-regressor>3.5.5 Ridge Regressor</h4><p>Ridge is a regression technique that uses L2 regularization. Ridge regression does not offer automatic variable selection in the sense that it is not make the weights zero on any of the variable used in the model and the regularization term in a ridge regression is slightly different than the lasso regression. The regularization term is the sum of the square of coefficients multiplied by the penalty whereas in lasso it is the sum of the absolute value of the coefficients. The regularization term is a gentle trade-off between fitting the model and overfitting the model and like in lasso it helps improve prediction accuracy as well as performance of the statistical model. The L2 regularization parameter helps control overfitting and will need to be explored for a range of values for a specific problem similar to Lasso. The regularization parameter also helps reduce multicollinearity in the model. Similar to Lasso, when the regularization penalty tends to be zero there is no regularization, and the loss function is mostly influenced by the squared loss and in contrary if the regularization penalty tends to be closer to infinity then the objective function is mostly influenced by the regularization part. As in the case of Lasso, it is always ideal to explore a range of values for the regularization penalty to improve the accuracy and avoid overfitting <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup><sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>.</p><p>In this project, Ridge regression is one of the important techniques that was explored again primarily because the problem being solved is a regression problem and there is possibility to overfit the data due to the number of observations and feature space. During the model training phase different ranges for regularization penalty were explored and the appropriate value that helped achieve maximum reduction in the total RMSE score was identified. Ridge performed slightly better than baseline model. However like Lasso, it did not outperform lightGBM or XGBoost or tree based models in general.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/ridge3new.png alt=Ridge></p><p><strong>Figure 16:</strong> Ridge Model</p><h2 id=4-benchmark-results>4. Benchmark Results</h2><p>There are some interesting observations from the benchmark results seen in Figure-17. As expected, the data exploration and pre-processing performed very well and the data load and flattening of JSON took only a few hundred milliseconds on a platform like Google Colab compared to more than a minute running the same code locally on a desktop. The grid search for linear regression as expected took more time than the grid search for regularization techniques which is an interesting finding. The model training phase using ridge regression took only half the time approximately 250 seconds compared to 811 seconds for lasso regression. The highest training time of approximately 1800 seconds was recorded with XGBoost regressor and although the original assumption was that this modeling technique would consume time and significant system resources to complete the training process, the total time of 1800 seconds was certainly in the higher end. But, considering the fact that random forest regressor took more than 90 minutes to complete the training process during the experimentation phase, XGBoost performed much better than random forest. The other interesting observation was between LightGBM and XGBoost where LightGBM took significantly less time than XGBoost regressor and if performance and high availability are key considerations during operationalization with slight compromise on model performance then LightGBM would be an ideal candidate for real time operationalization.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/benchmark1.png alt=Benchmark></p><p><strong>Figure 17:</strong> Benchmark Results</p><h2 id=5-software-technologies>5. Software Technologies</h2><p>In this project tools like Python and Google Colab Jupyter Notebook were used. Also several Python packages were employed in this exploration such as Pandas, Numpy, Matplotlib, sklearn</p><h2 id=6-conclusion>6. Conclusion</h2><p>As a project team the intention was to create a template that can be utilized for any ML project. The dataset that was used for this project was challenging in a way that it required a lot of data cleaning, flattening and transformation to get the data into the required format.</p><h3 id=61-model-pipeline>6.1 Model Pipeline</h3><p>In this project, multiple regression and tree based models from scikit learn library were explored with various hyper parameter setting and other methods like the lightGBM. The goal of the model pipeline was to explore and examine data, identify data pre-processing methods, imputation strategies, derive features and try different feature extraction methods was to perform different experiments with different parameter setting and identify the optimized with low RMSE that can be operationalized in Production. The parameters were explored within the boundary of this problem setting using different techniques.</p><h3 id=62-feature-exploration-and-pre-processing>6.2 Feature Exploration and Pre-Processing</h3><p>As part of this project a few features were engineered and included in the training dataset. The feature importance visualizations that were generated after the model training process indicate that these engineered features were part of the top 30% of high impact features and they contributed reasonably to improving the overall accuracy of the model. During additional experimentation phase, the possibility of including few other potential features that could be derived from the dataset was explored and those additional features were included in the final dataset that was used during model training. Although these features did not contribute largely to reducing the error it gave an opportunity to share ideas and methods to develop these new features. Also during feature exploration phase other imputation strategies were evaluated, attempted to identify more outliers and tried different encoding techniques for categorical variables and ultimately determined that label encoder or ordinal encoder is the best way forward. Also some of the low importance features were excluded and the model was retrained to validate if the same or better RMSE value could be achieved.</p><h3 id=63-outcome-of-experiments>6.3 Outcome of Experiments</h3><p>Multiple modeling techniques were explored as part of this project like Linear regression, gradient boosting algorithms and linear regression regularization techniques. The techniques were explored with basic parameter setting and based on the outcome of those experiments, the hyper parameters were tuned using grid search to obtain the best estimator evaluated on RMSE. Also, during grid search K-Fold cross validation of training data was used and the cross validated results were examined through a results table. The fit_intercept flag played a significant role resulting in an optimal error. As part of the different experimentations that were performed, random forest algorithm was also explored but it suffered performance issues and it seemed like it would require more iterations to converge which is why it was dropped from our results and further exploration. Although random forest was not explored, gradient boosting techniques were part of the experimentations and the best RMSE from XGBoost. The LightGBM regressor was also explored with different parameter settings but it did not produce better RMSE score than XGBoost.</p><p>In the case of XGBoost, there was improvement to the RMSE score as different tree depths, feature fraction, learning rate, number of children, bagging fraction, sub-sample were explored. There was significant improvement to the error metric when these parameters were adjusted in an intuitive way. Also, linear regression with regularization techniques were explored and although there was some improvement to the error metric compared to the basic linear regression model they did not perform better than the gradient boosting method that was explored. So, based on different explorations and experimentations a reasonably conclusion can be made that gradient boosting technique performed better for the given problem setting and generated the best RMSE score. Based on the evaluation results of XGBoost on the dataset used, the recommendation would be to test the XGBoost model with real time data and the performance of the model can be evaluated in real-time scenario too and additionally, if needed, hyper parameter tuning can be performed on the XGBoost model specifically for the real-time scenario <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. The feature engineering process on the dataset helped derive features with additional predictive value and a pipeline was built to reuse the same process in different modeling techniques. Five different models were tested including Linear Regression, XGBoost, Light GBM, Lasso and Ridge. The summary of all the models can be seen in Figure-17.</p><p><img src=https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/model_results_new2new.png alt=Model_Results></p><p><strong>Figure 18:</strong> Model Results Summary</p><h3 id=64-limitations>6.4 Limitations</h3><p>Due to the limited capacity of our Colab Notebook setup, there was difficulty in performing cross Validation for XGBoost and LightGBM. The KFold cross validation with different parameter settings would have helped identify the best estimator for these models, helped achieve even better rmse scores and potentially avoid overfitting, if any. The tree based models performed well in this dataset and it would be beneficial to explore other tree based models like Random Forest in the future and evaluate/compare the performance.</p><h2 id=7-previous-explorations>7. Previous Explorations</h2><p>The Online GStore customer revenue prediction problem is a Kaggle competition with more than 4100 entries. It is one of the popular challenges in Kaggle with a prize money of $45,000. Although the goal was not to make it to the top in the leader board, the challenge gave a huge opportunity to explore different methods, techniques, tools and resources. The one important difference between many of the previous of explorations versus what has been achieved in this exploration is the number of different machine learning algorithms that was explored and the performance for each of those different techniques were examined. Based on review of several submissions in Kaggle there were only a very few kernel entries that explored different parameter settings and making intuitive adjustments to them to make the model perform at an optimum level like what has been accomplished in this project. The other uniqueness that was brought to this submission was identifying techniques that offered good performance and consumed less system resources in terms of operationalization. There is lot of scope to continue exploration and attempt other techniques to identify the best performing model.</p><h2 id=8-acknowlegements>8. Acknowlegements</h2><p>The team would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the other instructors in the Big Data Applications course for their guidance and support through the course of this project and advise on documenting the results of various explorations.</p><h2 id=9-references>9. References</h2><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Kaggle Competition,2019,Predict the Online Store Revenue,[online] Available at: <a href=https://www.kaggle.com/c/ga-customer-revenue-prediction/rules>https://www.kaggle.com/c/ga-customer-revenue-prediction/rules</a> <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Kaggle Competition,2019,Predict the Online Store Revenue, Data, [online] Available at: <a href=https://www.kaggle.com/c/ga-customer-revenue-prediction/data>https://www.kaggle.com/c/ga-customer-revenue-prediction/data</a> <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>Machine Learning Mastery,2016,Brownlee, Linear Regression Model, [online] Available at: <a href=https://machinelearningmastery.com/linear-regression-for-machine-learning>https://machinelearningmastery.com/linear-regression-for-machine-learning</a> <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>XGBoost 2020,xgboost developers, XGBoost, [online] Available at: <a href=https://xgboost.readthedocs.io>https://xgboost.readthedocs.io</a> <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>Datacamp,2019,Pathak, Using XGBoost in Python, [online] Available at: <a href=https://www.datacamp.com/community/tutorials/xgboost-in-python>https://www.datacamp.com/community/tutorials/xgboost-in-python</a> <a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>Towards Datascience,2017,Lutins, Ensemble Methods in Machine Learning, [online] Available at: <a href=https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f>https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f</a> <a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7 role=doc-endnote><p>Medium 2017,Mandot, What is LightGBM,[online] Available at: <a href=https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc>https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc</a> <a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8 role=doc-endnote><p>Kaggle 2018,Daniel, Google Analytics Customer Revenue Prediction, [online] Available at: <a href=https://www.kaggle.com/fabiendaniel/lgbm-starter>https://www.kaggle.com/fabiendaniel/lgbm-starter</a> <a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9 role=doc-endnote><p>Towards Datascience,2018,Bhattacharya, Ridge and Lasso regression, [online] Available at: <a href=https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b>https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b</a> <a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10 role=doc-endnote><p>Datacamp,2019,Oleszak, Regularization: Ridge, Lasso and Elastic Net, [online] Available at: <a href=https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net>https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net</a> <a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section><div class="text-muted mt-5 pt-3 border-top">Last modified January 1, 0001</div></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Gregor von Laszewski" aria-label="Gregor von Laszewski"><a class=text-white target=_blank href=https://laszewski.github.io><i class="fa fa-envelope"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/cybertraining-dsc/cybertraining-dsc.github.io/><i class="fab fa-github"></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2021 Indiana University, 2020 All Rights Reserved</small><p class=mt-2><a href=/about/>About Cybertraining</a></p></div></div></div></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js integrity=sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy crossorigin=anonymous></script><script src=/js/main.min.29b0315468c00226fa6f4556a9cebc0ac4fe1ce1457a01b22c0a06b329877383.js integrity="sha256-KbAxVGjAAib6b0VWqc68CsT+HOFFegGyLAoGsymHc4M=" crossorigin=anonymous></script></body></html>