<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cybertraining â€“ Reports</title><link>/report/</link><description>Recent content in Reports on Cybertraining</description><generator>Hugo -- gohugo.io</generator><atom:link href="/report/index.xml" rel="self" type="application/rss+xml"/><item><title>Report: Reports 2021</title><link>/report/2021/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/2021/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>This page contains the list of the reports and projects.&lt;/p>
&lt;p>Any report with a tag&lt;/p>
&lt;ul>
&lt;li>&lt;img src="../report/failed-check.png" alt=""> will not
be reviewed or commented on.&lt;/li>
&lt;li>&lt;img src="../report/failed-status.png" alt=""> will not
be reviewed for grading.&lt;/li>
&lt;/ul>
&lt;p>Any report with&lt;/p>
&lt;ul>
&lt;li>insufficient detail will not be reviewed&lt;/li>
&lt;li>without a link in the report to the source code or notebook will not be reviewed&lt;/li>
&lt;li>any report without a download function that only downloads if the data is not
present will receive point deductions. The time to download must be included
in the benchmark time while using cloudmesh StopWatch. It must be included in
your report.&lt;/li>
&lt;li>Cloudmesh Stopwatch must be used to measuer the time of major parts of your code.
This includes download, training, prediction time, and other important times you identify&lt;/li>
&lt;/ul>
&lt;p>Click on your icon to find out what the errors are. Fixing them is easy.&lt;/p>
&lt;/div>
&lt;h2 id="list-for-2021">List for 2021&lt;/h2>
&lt;h3 id="sample">Sample&lt;/h3>
&lt;p>For samples see: &lt;a href="https://cybertraining-dsc.github.io/report/">https://cybertraining-dsc.github.io/report/&lt;/a>&lt;/p>
&lt;h3 id="reports-and-projects">Reports and Projects&lt;/h3>
&lt;hr>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/blob/main/project">&lt;i class="fas fa-edit">&lt;/i>&lt;/a> sp21-599-359: &lt;a href="/report/sp21-599-359/project/">Project: Deep Learning in Drug Discovery, Anesu Chaora&lt;/a>
* &lt;img src="https://img.shields.io/github/stars/cybertraining-dsc/sp21-599-359?style=social" alt="GitHub Repo stars">
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-359/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-359/workflows/Status/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://GitHub.com/cybertraining-dsc/sp21-599-359/issues/">&lt;img src="https://img.shields.io/github/issues-raw/cybertraining-dsc/sp21-599-359" alt="Issues">&lt;/a>
&lt;img src="https://img.shields.io/github/languages/code-size/cybertraining-dsc/sp21-599-359" alt="GitHub code size in bytes">
&lt;img src="https://img.shields.io/github/repo-size/cybertraining-dsc/sp21-599-359" alt="GitHub repo size">&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-357/blob/main/project">&lt;i class="fas fa-edit">&lt;/i>&lt;/a> sp21-599-357: &lt;a href="/report/sp21-599-357/project/">Project: Structural Protein Sequences Classification, Jiayu Li&lt;/a>
* &lt;img src="https://img.shields.io/github/stars/cybertraining-dsc/sp21-599-357?style=social" alt="GitHub Repo stars">
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-357/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-357/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-357/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-357/workflows/Status/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://GitHub.com/cybertraining-dsc/sp21-599-357/issues/">&lt;img src="https://img.shields.io/github/issues-raw/cybertraining-dsc/sp21-599-357" alt="Issues">&lt;/a>
&lt;img src="https://img.shields.io/github/languages/code-size/cybertraining-dsc/sp21-599-357" alt="GitHub code size in bytes">
&lt;img src="https://img.shields.io/github/repo-size/cybertraining-dsc/sp21-599-357" alt="GitHub repo size">&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-355/blob/main/project">&lt;i class="fas fa-edit">&lt;/i>&lt;/a> sp21-599-355: &lt;a href="/report/sp21-599-355/project/">Project: Chat Bots in Customer Service, Rishabh Agrawal&lt;/a>
* &lt;img src="https://img.shields.io/github/stars/cybertraining-dsc/sp21-599-355?style=social" alt="GitHub Repo stars">
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-355/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-355/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-355/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-355/workflows/Status/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://GitHub.com/cybertraining-dsc/sp21-599-355/issues/">&lt;img src="https://img.shields.io/github/issues-raw/cybertraining-dsc/sp21-599-355" alt="Issues">&lt;/a>
&lt;img src="https://img.shields.io/github/languages/code-size/cybertraining-dsc/sp21-599-355" alt="GitHub code size in bytes">
&lt;img src="https://img.shields.io/github/repo-size/cybertraining-dsc/sp21-599-355" alt="GitHub repo size">&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-354/blob/main/project">&lt;i class="fas fa-edit">&lt;/i>&lt;/a> sp21-599-354: &lt;a href="/report/sp21-599-354/project/">Project: Identifying Agricultural Weeds with CNN, Paula Madetzke&lt;/a>
* &lt;img src="https://img.shields.io/github/stars/cybertraining-dsc/sp21-599-354?style=social" alt="GitHub Repo stars">
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-354/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-354/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-354/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-354/workflows/Status/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://GitHub.com/cybertraining-dsc/sp21-599-354/issues/">&lt;img src="https://img.shields.io/github/issues-raw/cybertraining-dsc/sp21-599-354" alt="Issues">&lt;/a>
&lt;img src="https://img.shields.io/github/languages/code-size/cybertraining-dsc/sp21-599-354" alt="GitHub code size in bytes">
&lt;img src="https://img.shields.io/github/repo-size/cybertraining-dsc/sp21-599-354" alt="GitHub repo size">&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-358/blob/main/project">&lt;i class="fas fa-edit">&lt;/i>&lt;/a> sp21-599-358: &lt;a href="/report/sp21-599-358/project/">Project: Autonomous Vehicle Simulations Using the CARLA Simulator, Jesus Badillo&lt;/a>
* &lt;img src="https://img.shields.io/github/stars/cybertraining-dsc/sp21-599-358?style=social" alt="GitHub Repo stars">
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-358/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-358/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-358/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-358/workflows/Status/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://GitHub.com/cybertraining-dsc/sp21-599-358/issues/">&lt;img src="https://img.shields.io/github/issues-raw/cybertraining-dsc/sp21-599-358" alt="Issues">&lt;/a>
&lt;img src="https://img.shields.io/github/languages/code-size/cybertraining-dsc/sp21-599-358" alt="GitHub code size in bytes">
&lt;img src="https://img.shields.io/github/repo-size/cybertraining-dsc/sp21-599-358" alt="GitHub repo size">&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/blob/main/project">&lt;i class="fas fa-edit">&lt;/i>&lt;/a> sp21-599-356: &lt;a href="/report/sp21-599-356/project/">Project: Forecasting Natural Gas Demand/Supply, Baekeun Park&lt;/a>
* &lt;img src="https://img.shields.io/github/stars/cybertraining-dsc/sp21-599-356?style=social" alt="GitHub Repo stars">
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-356/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-356/workflows/Status/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://GitHub.com/cybertraining-dsc/sp21-599-356/issues/">&lt;img src="https://img.shields.io/github/issues-raw/cybertraining-dsc/sp21-599-356" alt="Issues">&lt;/a>
&lt;img src="https://img.shields.io/github/languages/code-size/cybertraining-dsc/sp21-599-356" alt="GitHub code size in bytes">
&lt;img src="https://img.shields.io/github/repo-size/cybertraining-dsc/sp21-599-356" alt="GitHub repo size">&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/blob/main/project">&lt;i class="fas fa-edit">&lt;/i>&lt;/a> sp21-599-356: &lt;a href="/report/sp21-599-356/project/">Project: Forecasting Natural Gas Demand/Supply, Baekeun Park&lt;/a>
* &lt;img src="https://img.shields.io/github/stars/cybertraining-dsc/sp21-599-356?style=social" alt="GitHub Repo stars">
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-356/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-356/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-356/workflows/Status/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://GitHub.com/cybertraining-dsc/sp21-599-356/issues/">&lt;img src="https://img.shields.io/github/issues-raw/cybertraining-dsc/sp21-599-356" alt="Issues">&lt;/a>
&lt;img src="https://img.shields.io/github/languages/code-size/cybertraining-dsc/sp21-599-356" alt="GitHub code size in bytes">
&lt;img src="https://img.shields.io/github/repo-size/cybertraining-dsc/sp21-599-356" alt="GitHub repo size">&lt;/p>
&lt;hr></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/cloudmesh/openapi/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/cloudmesh/openapi/readme/</guid><description>
&lt;h1 id="openapi-function-generator">Openapi Function generator&lt;/h1>
&lt;h2 id="activity-log">Activity Log&lt;/h2>
&lt;h2 id="week-of-mar-9---mar-16">Week of Mar 9 - Mar 16&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Andrew Goldfarb&lt;/p>
&lt;ul>
&lt;li>Worked with Ishan and Jonathan to finalize the start stop
functionality.&lt;/li>
&lt;li>Added functionality to delete the process entry from the
registry upon stop command.&lt;/li>
&lt;li>Debugged weird start error for my personal machine where the
start functionality was running two bash terminals causing the
start function to fail.&lt;/li>
&lt;li>Met with Professor to discuss proper implementation of the
start/stop and how to tie into registry functionality.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="week-prior-to-mar-9th">Week prior to Mar 9th&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>bkgerreis&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Jonathan Beckford&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Prateek&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Andrew Goldfarb&lt;/p>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>Edited the stop function to take process PID and use os.kill to
stop the process based on the name of the python file. However,
according to Ishan this is still not working.&lt;/li>
&lt;li>Resolved conflicts between main and our working branch&lt;/li>
&lt;li>Began work on assigning a default name if the user does not provide
one for server start. Potetially, a function to assign an alias
name to the whole process to amke it easier to reference.&lt;/li>
&lt;/ol>
&lt;h2 id="install-for-development">Install for development&lt;/h2>
&lt;p>cloudmesh-installer git pull analytics&lt;/p>
&lt;pre>&lt;code>cd cloudmesh-openapi
pip install -e .
&lt;/code>&lt;/pre>&lt;h2 id="keep-up-to-date">Keep up to date&lt;/h2>
&lt;p>explain how to set up and use upstream sync&lt;/p>
&lt;h3 id="project-meeting">Project Meeting&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://iu.zoom.us/rec/share/4dIpJZ-p8ztIHpH_q1HAZ6wzL6iiaaa8h3QX8_YMzRkn8tBfY_mRIe8z3j-3cZ_9?startTime=1581987567000">Mon 17 Feb&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://iu.zoom.us/rec/share/_8ZLKK7Z6zpLb53f73_UW4EFBY_iX6a8gydM_vVbzRu2MhrC_sUCKhChUkLzgEK8?startTime=1582591839000">Mon 24 Feb&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="basic-function-generator">Basic Function Generator&lt;/h3>
&lt;h4 id="prateek-shaw----code-link">Prateek Shaw - code link.&lt;/h4>
&lt;p>&lt;a href="https://github.com/cloudmesh-community/sp20-516-229/tree/main/cloudmesh-openapi">https://github.com/cloudmesh-community/sp20-516-229/tree/main/cloudmesh-openapi&lt;/a>&lt;/p>
&lt;ul>
&lt;li>created a basic function that will return the OpenAPI YAML file
of given python function including parameters.&lt;/li>
&lt;/ul>
&lt;h4 id="sp20-516-237----jonathan-beckford">SP20-516-237 &amp;ndash; Jonathan Beckford&lt;/h4>
&lt;p>I created a class that generates the OpenAPI yaml file. I also created
a sample program that defines an example function, instantiates my
OpenAPI generator class and passes in the sample function as input. I
figured this would make things really easy to just paste any new
sample function for testing purposes. I also included the parameters
as was requested. I also ran my output yaml through the swagger
validator (&lt;a href="https://editor.swagger.io/">https://editor.swagger.io/&lt;/a>) to make sure it was compliant
and it was.&lt;br>
&lt;a href="https://github.com/cloudmesh-community/sp20-516-237/tree/main/projectAI/generateOpenAPI">Link&lt;/a>&lt;/p>
&lt;h4 id="sp20-516-231---brian-kegerreis">sp20-516-231 - Brian Kegerreis&lt;/h4>
&lt;p>I created a function to generate an OpenAPI spec including a rough
attempt at response types (only supports text/plain media types at
this point)
&lt;a href="https://github.com/cloudmesh-community/sp20-516-231/blob/main/openapi-exercises/example_echo.py">https://github.com/cloudmesh-community/sp20-516-231/blob/main/openapi-exercises/example_echo.py&lt;/a>&lt;/p>
&lt;h3 id="server-start">Server Start&lt;/h3>
&lt;h4 id="andrew-goldfarb---sp20-516-234">Andrew Goldfarb - SP20-516-234&lt;/h4>
&lt;p>&lt;a href="https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI">https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI&lt;/a>
I have created a basic function that returns the IP address of the
server running the function to tell if it is running on the device
itself or connected to the internet running while running the
function.&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/cloudmesh/openapi/scikitlearn/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/cloudmesh/openapi/scikitlearn/readme/</guid><description>
&lt;h1 id="sklearngeneratorfile-high-level-overview">SKlearnGeneratorFile High level Overview&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>The SklearnGeneratorFile.py is the generator function which outputs the python file for given
Sckit-learn Library.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The function takes two inputs&lt;/p>
&lt;p>1.input_sklibrary&lt;/p>
&lt;p>2.model_tag&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Examples of the inputs are&lt;/p>
&lt;p>input_sklibrary = sklearn.linear_model.LinearRegression(Full model specification)
model_tag = any name which you want the tag the model instance like LinReg1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>This Version of Scikit-learn service accepts csv files in UTF-8 format only.It is the user responsibility to make
sure the files are in UTF-8 format.It is the user responsibility to split the data in to train and test datasets.
Split data functionality is not currently supported.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>scikit-learn uses numpydoc format in the docstring so the scraping of the parameters and docstrings
are done using docscrape from numpydoc.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>All the templates used in the code are based on X and y inputs scikit-learn takes and also based on the
return type&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="pytests-for-scikit-learn-tests">Pytests for Scikit learn tests.&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>The below pytest generates the .py file used by generator to do a OPENAPI specification.&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/Scikitlearn-tests/test_06c_sklearngeneratortest.py">Pytestcode&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash"> pytest -v --capture&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>no tests/Scikitlearn_tests/test_06c_sklearngeneratortest.py
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>The below pytest tests the methods generated .&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/Scikitlearn-tests/test_06d_sklearngeneratortest.py">Pytestcode&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">pytest -v --capture&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>no tests/Scikitlearn_tests/test_06d_sklearngeneratortest.py
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/deprecated/openapi/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/deprecated/openapi/readme/</guid><description>
&lt;h1 id="openapi-function-generator">Openapi Function generator&lt;/h1>
&lt;h2 id="activity-log">Activity Log&lt;/h2>
&lt;h2 id="week-of-mar-9---mar-16">Week of Mar 9 - Mar 16&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Andrew Goldfarb&lt;/p>
&lt;ul>
&lt;li>Worked with Ishan and Jonathan to finalize the start stop
functionality.&lt;/li>
&lt;li>Added functionality to delete the process entry from the
registry upon stop command.&lt;/li>
&lt;li>Debugged weird start error for my personal machine where the
start functionality was running two bash terminals causing the
start function to fail.&lt;/li>
&lt;li>Met with Professor to discuss proper implementation of the
start/stop and how to tie into registry functionality.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="week-prior-to-mar-9th">Week prior to Mar 9th&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>bkgerreis&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Jonathan Beckford&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Prateek&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Andrew Goldfarb&lt;/p>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>Edited the stop function to take process PID and use os.kill to
stop the process based on the name of the python file. However,
according to Ishan this is still not working.&lt;/li>
&lt;li>Resolved conflicts between main and our working branch&lt;/li>
&lt;li>Began work on assigning a default name if the user does not provide
one for server start. Potetially, a function to assign an alias
name to the whole process to amke it easier to reference.&lt;/li>
&lt;/ol>
&lt;h2 id="install-for-development">Install for development&lt;/h2>
&lt;p>cloudmesh-installer git pull analytics&lt;/p>
&lt;pre>&lt;code>cd cloudmesh-openapi
pip install -e .
&lt;/code>&lt;/pre>&lt;h2 id="keep-up-to-date">Keep up to date&lt;/h2>
&lt;p>explain how to set up and use upstream sync&lt;/p>
&lt;h3 id="project-meeting">Project Meeting&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://iu.zoom.us/rec/share/4dIpJZ-p8ztIHpH_q1HAZ6wzL6iiaaa8h3QX8_YMzRkn8tBfY_mRIe8z3j-3cZ_9?startTime=1581987567000">Mon 17 Feb&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://iu.zoom.us/rec/share/_8ZLKK7Z6zpLb53f73_UW4EFBY_iX6a8gydM_vVbzRu2MhrC_sUCKhChUkLzgEK8?startTime=1582591839000">Mon 24 Feb&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="basic-function-generator">Basic Function Generator&lt;/h3>
&lt;h4 id="prateek-shaw----code-link">Prateek Shaw - code link.&lt;/h4>
&lt;p>&lt;a href="https://github.com/cloudmesh-community/sp20-516-229/tree/main/cloudmesh-openapi">https://github.com/cloudmesh-community/sp20-516-229/tree/main/cloudmesh-openapi&lt;/a>&lt;/p>
&lt;ul>
&lt;li>created a basic function that will return the OpenAPI YAML file
of given python function including parameters.&lt;/li>
&lt;/ul>
&lt;h4 id="sp20-516-237----jonathan-beckford">SP20-516-237 &amp;ndash; Jonathan Beckford&lt;/h4>
&lt;p>I created a class that generates the OpenAPI yaml file. I also created
a sample program that defines an example function, instantiates my
OpenAPI generator class and passes in the sample function as input. I
figured this would make things really easy to just paste any new
sample function for testing purposes. I also included the parameters
as was requested. I also ran my output yaml through the swagger
validator (&lt;a href="https://editor.swagger.io/">https://editor.swagger.io/&lt;/a>) to make sure it was compliant
and it was.
&lt;a href="https://github.com/cloudmesh-community/sp20-516-237/tree/main/projectAI/generateOpenAPI">Link&lt;/a>&lt;/p>
&lt;h4 id="sp20-516-231---brian-kegerreis">sp20-516-231 - Brian Kegerreis&lt;/h4>
&lt;p>I created a function to generate an OpenAPI spec including a rough
attempt at response types (only supports text/plain media types at
this point)
&lt;a href="https://github.com/cloudmesh-community/sp20-516-231/blob/main/openapi-exercises/example_echo.py">https://github.com/cloudmesh-community/sp20-516-231/blob/main/openapi-exercises/example_echo.py&lt;/a>&lt;/p>
&lt;h3 id="server-start">Server Start&lt;/h3>
&lt;h4 id="andrew-goldfarb---sp20-516-234">Andrew Goldfarb - SP20-516-234&lt;/h4>
&lt;p>&lt;a href="https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI">https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI&lt;/a>
I have created a basic function that returns the IP address of the
server running the function to tell if it is running on the device
itself or connected to the internet running while running the
function.&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/deprecated/paper/results/install/cloud_lscpu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/deprecated/paper/results/install/cloud_lscpu/</guid><description>
&lt;h3 id="aws">AWS&lt;/h3>
&lt;pre>&lt;code class="language-Architecture:" data-lang="Architecture:">CPU op-mode(s): 32-bit, 64-bit
Byte Order: Little Endian
Address sizes: 46 bits physical, 48 bits virtual
CPU(s): 2
On-line CPU(s) list: 0,1
Thread(s) per core: 2
Core(s) per socket: 1
Socket(s): 1
NUMA node(s): 1
Vendor ID: GenuineIntel
CPU family: 6
Model: 79
Model name: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz
Stepping: 1
CPU MHz: 2300.027
BogoMIPS: 4600.15
Hypervisor vendor: Xen
Virtualization type: full
L1d cache: 32 KiB
L1i cache: 32 KiB
L2 cache: 256 KiB
L3 cache: 45 MiB
NUMA node0 CPU(s): 0,1
Vulnerability Itlb multihit: KVM: Vulnerable
Vulnerability L1tf: Mitigation; PTE Inversion
Vulnerability Mds: Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host st
ate unknown
Vulnerability Meltdown: Mitigation; PTI
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitizati
on
Vulnerability Spectre v2: Mitigation; Full generic retpoline, STIBP disabled, RSB filling
Vulnerability Srbds: Not affected
Vulnerability Tsx async abort: Not affected
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat
pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm co
nstant_tsc rep_good nopl xtopology cpuid pni pclmulqdq ssse3 fma c
x16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes
xsave avx f16c rdrand hypervisor lahf_lm abm cpuid_fault invpcid_s
ingle pti fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveopt
&lt;/code>&lt;/pre>&lt;h3 id="google">Google&lt;/h3>
&lt;pre>&lt;code>Architecture: x86_64
CPU op-mode(s): 32-bit, 64-bit
Byte Order: Little Endian
Address sizes: 46 bits physical, 48 bits virtual
CPU(s): 2
On-line CPU(s) list: 0,1
Thread(s) per core: 2
Core(s) per socket: 1
Socket(s): 1
NUMA node(s): 1
Vendor ID: GenuineIntel
CPU family: 6
Model: 63
Model name: Intel(R) Xeon(R) CPU @ 2.30GHz
Stepping: 0
CPU MHz: 2300.000
BogoMIPS: 4600.00
Hypervisor vendor: KVM
Virtualization type: full
L1d cache: 32 KiB
L1i cache: 32 KiB
L2 cache: 256 KiB
L3 cache: 45 MiB
NUMA node0 CPU(s): 0,1
Vulnerability Itlb multihit: Not affected
Vulnerability L1tf: Mitigation; PTE Inversion
Vulnerability Mds: Mitigation; Clear CPU buffers; SMT Host state unknown
Vulnerability Meltdown: Mitigation; PTI
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccom
p
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitizati
on
Vulnerability Spectre v2: Mitigation; Full generic retpoline, IBPB conditional, IBRS_FW, STI
BP conditional, RSB filling
Vulnerability Srbds: Not affected
Vulnerability Tsx async abort: Not affected
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat
pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm
constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_
freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe
popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_si
ngle pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep b
mi2 erms invpcid xsaveopt arat md_clear arch_capabilities
&lt;/code>&lt;/pre>&lt;h3 id="azure">Azure&lt;/h3>
&lt;pre>&lt;code>Architecture: x86_64
CPU op-mode(s): 32-bit, 64-bit
Byte Order: Little Endian
Address sizes: 46 bits physical, 48 bits virtual
CPU(s): 2
On-line CPU(s) list: 0,1
Thread(s) per core: 2
Core(s) per socket: 1
Socket(s): 1
NUMA node(s): 1
Vendor ID: GenuineIntel
CPU family: 6
Model: 63
Model name: Intel(R) Xeon(R) CPU @ 2.30GHz
Stepping: 0
CPU MHz: 2300.000
BogoMIPS: 4600.00
Hypervisor vendor: KVM
Virtualization type: full
L1d cache: 32 KiB
L1i cache: 32 KiB
L2 cache: 256 KiB
L3 cache: 45 MiB
NUMA node0 CPU(s): 0,1
Vulnerability Itlb multihit: Not affected
Vulnerability L1tf: Mitigation; PTE Inversion
Vulnerability Mds: Mitigation; Clear CPU buffers; SMT Host state unknown
Vulnerability Meltdown: Mitigation; PTI
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2: Mitigation; Full generic retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling
Vulnerability Srbds: Not affected
Vulnerability Tsx async abort: Not affected
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology
nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single pti
ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities
&lt;/code>&lt;/pre></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/docker/ubuntu19.10/todo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/docker/ubuntu19.10/todo/</guid><description>
&lt;ul>
&lt;li>
&lt;p>make clean: only delete the artifacts created here, the clean wipes
currently everything&lt;/p>
&lt;/li>
&lt;li>
&lt;p>when using this in consecutive order, would cms init not wipe out the data&lt;/p>
&lt;/li>
&lt;li>
&lt;p>should we not mount the .cloudmesh and other data dire into the conatiner.
This way we can use the host system for developments. Maybe we need to&lt;/p>
&lt;/li>
&lt;li>
&lt;p>support both ways&lt;/p>
&lt;/li>
&lt;li>
&lt;p>leverage cmsd
we have told the class that we have cmsd taht starts up cloudmesh in
a container. Develop a new directory docker-cmsd and use that&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/docker/ubuntu20.04-sklearn/todo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/docker/ubuntu20.04-sklearn/todo/</guid><description>
&lt;ul>
&lt;li>
&lt;p>THIS IS JUST THE SKELETON AND HAS NOT BEEN RUN ONCE IT HAS BUGS&lt;/p>
&lt;/li>
&lt;li>
&lt;p>THIS HAS NOT YET THE SKLERAN START STOP&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;li>
&lt;p>make clean: only delete the artifacts created here, the clean wipes
currently everything&lt;/p>
&lt;/li>
&lt;li>
&lt;p>when using this in consecutive order, would cms init not wipe out the data&lt;/p>
&lt;/li>
&lt;li>
&lt;p>should we not mount the .cloudmesh and other data dire into the conatiner.
This way we can use the host system for developments. Maybe we need to&lt;/p>
&lt;/li>
&lt;li>
&lt;p>support both ways&lt;/p>
&lt;/li>
&lt;li>
&lt;p>leverage cmsd
we have told the class that we have cmsd taht starts up cloudmesh in
a container. Develop a new directory docker-cmsd and use that&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/docker/ubuntu20.04/todo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/docker/ubuntu20.04/todo/</guid><description>
&lt;ul>
&lt;li>
&lt;p>THIS IS JUST THE SKELETON AND HAS NOT BEEN RUN ONCE IT HAS BUGS&lt;/p>
&lt;/li>
&lt;li>
&lt;p>make clean: only delete the artifacts created here, the clean wipes
currently everything&lt;/p>
&lt;/li>
&lt;li>
&lt;p>when using this in consecutive order, would cms init not wipe out the data&lt;/p>
&lt;/li>
&lt;li>
&lt;p>should we not mount the .cloudmesh and other data dire into the conatiner.
This way we can use the host system for developments. Maybe we need to&lt;/p>
&lt;/li>
&lt;li>
&lt;p>support both ways&lt;/p>
&lt;/li>
&lt;li>
&lt;p>leverage cmsd
we have told the class that we have cmsd taht starts up cloudmesh in
a container. Develop a new directory docker-cmsd and use that&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/project_review/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/project_review/</guid><description>
&lt;h1 id="project-review">Project Review&lt;/h1>
&lt;h2 id="team-members">Team members:&lt;/h2>
&lt;ul>
&lt;li>Jonathan Beckford&lt;/li>
&lt;li>Brian Kegerreis&lt;/li>
&lt;li>Prateek Shaw&lt;/li>
&lt;li>Jagadeesh Kandimalla&lt;/li>
&lt;li>Ishan Mishra&lt;/li>
&lt;li>Andrew G&lt;/li>
&lt;li>Falconi&lt;/li>
&lt;/ul>
&lt;h2 id="project-documentation">Project Documentation:&lt;/h2>
&lt;p>&lt;a href="https://cloudmesh.github.io/cloudmesh-openapi/index.html">https://cloudmesh.github.io/cloudmesh-openapi/index.html&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="contributors-based-on-git-tracking">Contributors based on Git tracking&lt;/h2>
&lt;p>&lt;em>&lt;strong>NOTE:&lt;/strong>&lt;/em> This is not completely accurate because some did not have git config done correctly.&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/graphs/contributors">https://github.com/cloudmesh/cloudmesh-openapi/graphs/contributors&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="code-breakdown">Code Breakdown&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>cms command:&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/command/openapi.py">https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/command/openapi.py&lt;/a>&lt;/p>
&lt;p>&lt;strong>Contributors:&lt;/strong> all team&lt;/p>
&lt;hr>
&lt;/li>
&lt;li>
&lt;p>cms generate - to generate server yaml&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>executor&lt;/strong> that parses parameters and calls generator:&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/executor.py">https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/executor.py&lt;/a>&lt;/p>
&lt;p>&lt;strong>Contributors:&lt;/strong> Brian, Professor&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>generator&lt;/strong> that generates the server yaml:&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/generator.py">https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/generator.py&lt;/a>&lt;/p>
&lt;p>&lt;strong>Contributors:&lt;/strong> Brian, Jonathan, Prateek&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;/li>
&lt;li>
&lt;p>cms server - to start and stop server&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/server.py">https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/server.py&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Contributors:&lt;/strong> Jonathan, Andrew, Prateek, Ishan&lt;/li>
&lt;/ul>
&lt;hr>
&lt;/li>
&lt;li>
&lt;p>cms registry - register the server and cache model&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>registry&lt;/strong> - registers server&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/Registry.py">https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/Registry.py&lt;/a>&lt;/p>
&lt;p>&lt;strong>Contributors:&lt;/strong> Falconi, Praful, Professor&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>cache&lt;/strong> - cache serialized model locally&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/cache.py">https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/cache.py&lt;/a>&lt;/p>
&lt;p>&lt;strong>Contributors:&lt;/strong> Jonathan&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>fileoperation&lt;/strong> - upload input files&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/fileoperation.py">https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/fileoperation.py&lt;/a>&lt;/p>
&lt;p>&lt;strong>Contributors:&lt;/strong> Prateek, Brian&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;/li>
&lt;li>
&lt;p>cms scikitlearn - generate sklearn functions&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/scikitlearn/SklearnGeneratorFile.py">https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/scikitlearn/SklearnGeneratorFile.py&lt;/a>&lt;/p>
&lt;p>&lt;strong>Contributors:&lt;/strong> Jagadeesh&lt;/p>
&lt;hr>
&lt;/li>
&lt;li>
&lt;p>cms image processing&lt;/p>
&lt;p>&lt;strong>Contributors:&lt;/strong> Falconi, Ishan&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cms text analysis&lt;/p>
&lt;p>&lt;strong>Contirbutor:&lt;/strong> Andrew Goldfarb&lt;/p>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/tree/main/tests/generator-natural-lang">https://github.com/cloudmesh/cloudmesh-openapi/tree/main/tests/generator-natural-lang&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="deployment-steps">Deployment steps&lt;/h2>
&lt;p>&lt;a href="https://cloudmesh.github.io/cloudmesh-openapi/README.html#installation">https://cloudmesh.github.io/cloudmesh-openapi/README.html#installation&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="quick-start">Quick Start&lt;/h2>
&lt;p>&lt;a href="https://cloudmesh.github.io/cloudmesh-openapi/README.html#quick-steps-to-generate-start-and-stop-cpu-sample-example">https://cloudmesh.github.io/cloudmesh-openapi/README.html#quick-steps-to-generate-start-and-stop-cpu-sample-example&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="pytests">Pytests&lt;/h2>
&lt;p>&lt;a href="https://cloudmesh.github.io/cloudmesh-openapi/README.html#pytests">https://cloudmesh.github.io/cloudmesh-openapi/README.html#pytests&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Integration of openapi with cms allows for running locally only. Cloud integration was not fully completed although team did create a way to setup openapi in a VM using a remote script for &lt;a href="https://github.com/cloudmesh/get/blob/main/openapi/ubuntu18.04/index.html">openstack&lt;/a> and &lt;a href="https://github.com/cloudmesh/get/blob/main/openapi/google/index.html">google&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The generator only supports creating arrays of number data type. This limitation is due to the bug documented below in &lt;em>&lt;strong>Bugs&lt;/strong>&lt;/em> section. So manual changes are required to the output yaml to allow for other data types until another work around is found or the bug is resolved.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="bugs">Bugs&lt;/h2>
&lt;ol>
&lt;li>reported a bug to Connexion and documented it in github for future reference:
&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/issues/60">https://github.com/cloudmesh/cloudmesh-openapi/issues/60&lt;/a>&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="additional-artifacts-produced">Additional artifacts produced:&lt;/h2>
&lt;h3 id="openstack-vm-set-up-script">Openstack VM set up script&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/cloudmesh/get/blob/main/openapi/ubuntu18.04/index.html">OPENSTACK&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/cloudmesh/get/blob/main/openapi/google/index.html">GOOGLE&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Contributors:&lt;/strong> Jonathan Beckford, Andrew Goldfarb&lt;/p>
&lt;h3 id="openapi-project-readme-generator">Openapi project readme generator&lt;/h3>
&lt;p>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/tree/main/sphinx">https://github.com/cloudmesh/cloudmesh-openapi/tree/main/sphinx&lt;/a>&lt;/p>
&lt;p>&lt;strong>Contributors:&lt;/strong> Jonathan Beckford, Professor&lt;/p>
&lt;h3 id="chapters">Chapters&lt;/h3>
&lt;h5 id="kubernetes">Kubernetes&lt;/h5>
&lt;p>&lt;a href="https://github.com/cloudmesh-community/sp20-516-231/blob/main/chapter/k8s-kubernetes-scheduler.md">https://github.com/cloudmesh-community/sp20-516-231/blob/main/chapter/k8s-kubernetes-scheduler.md&lt;/a>&lt;/p>
&lt;p>&lt;strong>Contributors:&lt;/strong> Jonathan Beckford, Brian Kegerreis, Ashok Singam&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/readme-adam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/readme-adam/</guid><description>
&lt;p>(ENV3) pi@red:~ $ sudo apt-get update
Get:1 &lt;a href="http://archive.raspberrypi.org/debian">http://archive.raspberrypi.org/debian&lt;/a> buster InRelease [32.6 kB]
Get:2 &lt;a href="http://raspbian.raspberrypi.org/raspbian">http://raspbian.raspberrypi.org/raspbian&lt;/a> buster InRelease [15.0 kB]
Get:3 &lt;a href="http://raspbian.raspberrypi.org/raspbian">http://raspbian.raspberrypi.org/raspbian&lt;/a> buster/main armhf Packages [13.0 MB]
Get:4 &lt;a href="http://archive.raspberrypi.org/debian">http://archive.raspberrypi.org/debian&lt;/a> buster/main armhf Packages [335 kB]
Fetched 13.4 MB in 30s (450 kB/s) &lt;br>
Reading package lists&amp;hellip; Done&lt;/p>
&lt;p>(ENV3) pi@red:~ $ sudo apt-get upgrade&lt;/p>
&lt;p>(ENV3) pi@red:~ $ sudo cat /var/lib/rancher/k3s/server/node-token
K10126aaacd0a2d1e093900147d64ff9fd68a1663c779c17811fbca9d6f1b31de30::server:c980296a842c92143b6b97d5368d65a9&lt;/p>
&lt;p>(ENV3) pi@red:~ $ hostname -I
169.254.144.52 10.6.0.186&lt;/p>
&lt;p>(ENV3) pi@red:~ $ ssh &lt;a href="mailto:pi@red001.local">pi@red001.local&lt;/a>&lt;/p>
&lt;p>pi@red001:~ $ curl -sfL &lt;a href="https://get.k3s.io">https://get.k3s.io&lt;/a> |&lt;br>
K3S_URL=&amp;ldquo;https://169.254.144.52:6443&amp;rdquo;&lt;br>
K3S_TOKEN=K10126aaacd0a2d1e093900147d64ff9fd68a1663c779c17811fbca9d6f1b31de30::server:c980296a842c92143b6b97d5368d65a9&lt;br>
sh -&lt;/p>
&lt;p>pi@red001:~ $ exit&lt;/p>
&lt;p>(ENV3) pi@red:~ $ sudo kubectl get nodes&lt;/p>
&lt;p>NAME STATUS ROLES AGE VERSION
red Ready master 9d v1.19.3+k3s3&lt;/p>
&lt;p>worker nodes are not showing up&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/readme-eigenfaces-test/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/readme-eigenfaces-test/</guid><description>
&lt;h2 id="run-instructions-for-the-eigenfaces-svm-example-by-os">Run Instructions for the Eigenfaces SVM example by OS&lt;/h2>
&lt;h3 id="unbutu">Unbutu&lt;/h3>
&lt;pre>&lt;code>#INSATLL cloudmesh-openapi
python3.9 -m venv ~/ENV3
pip install pip -U
source ~/ENV3/bin/activate
mkdir cm
cd cm
pip install cloudmesh-installer
cloudmesh-installer install openapi
pip uninstall uuid
cms help
# Configure cloudmesh DB
# WARNING Don't run if mongo already configured for other cloudmesh use
cms config set cloudmesh.data.mongo.MONGO_USERNAME=benchmark
cms config set cloudmesh.data.mongo.MONGO_PASSWORD=benchmark
cms config set cloudmesh.profile.user=benchmark
cms config set cloudmesh.profile.firstname=benchmark
cms config set cloudmesh.profile.lastname=benchmark
cms set host=localhost
#END WARNING
cms openapi register protocol pickle
# Generate SSH key
# WARNING Don't Run if SSH key already exists
ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &amp;quot;benchmark&amp;quot;
# END WARNING
# Install dependencies
pip install pillow
pip install scikit-learn
pip install pytest
pip install dataclasses
# Run EigenfacesSVM Example MANUALLY
cd ~/cm/cloudmesh-openapi
cms openapi generate EigenfacesSVM --filename=./tests/generator-eigenfaces-svm/eigenfaces-svm-full.py --import_class --enable_upload
cms openapi server start ./tests/generator-eigenfaces-svm/eigenfaces-svm-full.yaml
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/EigenfacesSVM/download_data&amp;quot; -H &amp;quot;accept: */*&amp;quot;
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/EigenfacesSVM/train&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; -H &amp;quot;accept: text/plain&amp;quot; -H &amp;quot;Content-Type: multipart/form-data&amp;quot; -F &amp;quot;upload=@$HOME/cm/cloudmesh-openapi/tests/generator-eigenfaces-svm/example_image.jpg;type=image/jpeg&amp;quot;
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/EigenfacesSVM/predict?image_file_paths=$HOME%2F.cloudmesh%2Fupload-file%2Fexample_image.jpg&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
cms openapi server stop EigenfacesSVM
# Run EigenfacesSVM Example AUTOMATICALLY as pytest
cd ~/cm/cloudmesh-openapi
pytest -v -s ./tests/test_030_generator_eigenfaces_svm.py
&lt;/code>&lt;/pre>&lt;h3 id="mac-os">Mac OS&lt;/h3>
&lt;pre>&lt;code>#INSATLL cloudmesh-openapi
python -m venv ~/ENV3
source ~/ENV3/bin/activate
mkdir cm
cd cm
pip install cloudmesh-installer
cloudmesh-installer install openapi
cms help
# Configure cloudmesh DB
# WARNING Don't run if mongo already configured for other cloudmesh use
cms config set cloudmesh.data.mongo.MONGO_USERNAME=benchmark
cms config set cloudmesh.data.mongo.MONGO_PASSWORD=benchmark
cms config set cloudmesh.profile.user=benchmark
cms config set cloudmesh.profile.firstname=benchmark
cms config set cloudmesh.profile.lastname=benchmark
cms set host=localhost
#END WARNING
cms openapi register protocol pickle
# Generate SSH key
# WARNING Don't Run if SSH key already exists
ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &amp;quot;benchmark&amp;quot;
# END WARNING
# Install dependencies
pip install pillow
pip install scikit-learn
pip install pytest
pip install dataclasses
pip install --upgrade certifi #testing to see if this fixes local cert issue
cd /Applications/Python\ 3.9/ #testing to see if this fixes local cert issue
./Install\ Certificates.command #testing to see if this fixes local cert issue
# Run EigenfacesSVM Example MANUALLY
cd ~/cm/cloudmesh-openapi
cms openapi generate EigenfacesSVM --filename=./tests/generator-eigenfaces-svm/eigenfaces-svm-full.py --import_class --enable_upload
cms openapi server start ./tests/generator-eigenfaces-svm/eigenfaces-svm-full.yaml
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/EigenfacesSVM/download_data&amp;quot; -H &amp;quot;accept: */*&amp;quot;
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/EigenfacesSVM/train&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; -H &amp;quot;accept: text/plain&amp;quot; -H &amp;quot;Content-Type: multipart/form-data&amp;quot; -F &amp;quot;upload=@$HOME/cm/cloudmesh-openapi/tests/generator-eigenfaces-svm/example_image.jpg;type=image/jpeg&amp;quot;
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/EigenfacesSVM/predict?image_file_paths=$HOME%2F.cloudmesh%2Fupload-file%2Fexample_image.jpg&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
cms openapi server stop EigenfacesSVM
# Run EigenfacesSVM Example AUTOMATICALLY as pytest
cd ~/cm/cloudmesh-openapi
pytest -v -s ./tests/test_030_generator_eigenfaces_svm.py
&lt;/code>&lt;/pre>&lt;h3 id="rasbian">Rasbian:&lt;/h3>
&lt;pre>&lt;code>#INSATLL cloudmesh-openapi
python -m venv ~/ENV3
source ~/ENV3/bin/activate
mkdir cm
cd cm
pip install cloudmesh-installer
cloudmesh-installer install openapi
cms help
# Configure cloudmesh DB
# WARNING Don't run if mongo already configured for other cloudmesh use
cms config set cloudmesh.data.mongo.MONGO_USERNAME=benchmark
cms config set cloudmesh.data.mongo.MONGO_PASSWORD=benchmark
cms config set cloudmesh.profile.user=benchmark
cms config set cloudmesh.profile.firstname=benchmark
cms config set cloudmesh.profile.lastname=benchmark
cms set host=localhost
#END WARNING
cms openapi register protocol pickle
# Generate SSH key
# WARNING Don't Run if SSH key already exists
ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &amp;quot;benchmark&amp;quot;
# END WARNING
# Install dependencies
sudo apt-get update
sudo apt-get install libatlas-base-dev
pip install pillow
pip install scikit-learn
pip install pytest
pip install dataclasses
# Run EigenfacesSVM Example MANUALLY
cd ~/cm/cloudmesh-openapi
cms openapi generate EigenfacesSVM --filename=./tests/generator-eigenfaces-svm/eigenfaces-svm-full.py --import_class --enable_upload
cms openapi server start ./tests/generator-eigenfaces-svm/eigenfaces-svm-full.yaml
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/EigenfacesSVM/download_data&amp;quot; -H &amp;quot;accept: */*&amp;quot;
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/EigenfacesSVM/train&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; -H &amp;quot;accept: text/plain&amp;quot; -H &amp;quot;Content-Type: multipart/form-data&amp;quot; -F &amp;quot;upload=@$HOME/cm/cloudmesh-openapi/tests/generator-eigenfaces-svm/example_image.jpg;type=image/jpeg&amp;quot;
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/EigenfacesSVM/predict?image_file_paths=$HOME%2F.cloudmesh%2Fupload-file%2Fexample_image.jpg&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
# Run EigenfacesSVM Example AUTOMATICALLY as pytest
cd ~/cm/cloudmesh-openapi
pytest -v -s ./tests/test_030_generator_eigenfaces_svm.py
&lt;/code>&lt;/pre>&lt;h3 id="using-the-pytest-to-test-a-remote-server">Using the pytest to test a remote server&lt;/h3>
&lt;p>This assumes the remote server is already running the OpenAPI service.
This will only run the test_upload and test_predict functions on the remote server. Used to measure function response times over the network.&lt;/p>
&lt;pre>&lt;code>cms set host=&amp;lt;ip&amp;gt;
pytest -v -s ./tests/test_030_generator_eigenfaces_svm.py
&lt;/code>&lt;/pre>&lt;h3 id="to-run-the-pytest-30-times">To run the pytest 30 times&lt;/h3>
&lt;pre>&lt;code>seq 30 | xargs -I -- pytest -v -s ./tests/test_030_generator_eigenfaces_svm.py | tee out.txt
to look at the prediction time you can say
fgrep &amp;quot;# csv&amp;quot; out.txt |fgrep &amp;quot;test_predict&amp;quot; | cut -d &amp;quot;,&amp;quot; -f4 | cat -n
&lt;/code>&lt;/pre></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/readme-scikitlearn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/readme-scikitlearn/</guid><description>
&lt;h1 id="cloudmesh-openapi-scikit-learn">Cloudmesh OpenAPI Scikit-learn&lt;/h1>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>We use recommend Python 3.8.2 Python or newer.&lt;/li>
&lt;li>We recommend pip version 20.0.2 or newer&lt;/li>
&lt;li>We recommend that you use a venv (see developer install)&lt;/li>
&lt;li>MongoDB installed as regular program not as service&lt;/li>
&lt;li>Please run cim init command to start mongodb server&lt;/li>
&lt;/ul>
&lt;p>We have not checked if it works on older versions.&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>Make sure to follow the instruction for &lt;code>cms openapi&lt;/code>&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>When getting started using the &lt;code>openapi&lt;/code>, please first call &lt;code>cms help openapi&lt;/code> to see the available functions and options. For your
convenience we include the manual page later on in this document.&lt;/p>
&lt;h2 id="scikit-learn-documentation">Scikit-learn Documentation&lt;/h2>
&lt;p>Scikit-learn is a Machine learning library in Python.We can choose a
ML algorithm like LinearRegression and cloudmesh will be able to spin
up OPENAPI specification for the library we choose. We can interact
with the Scikit-learn library using either CURL commands or through
GUI.&lt;/p>
&lt;p>This Version of Scikit-learn service accepts csv files in UTF-8 format
only.It is the user responsibility to make sure the files are in UTF-8
format.It is the user responsiblity to split the data in to train and
test datasets. Split data functionality is not currently supported.&lt;/p>
&lt;h3 id="setting-up-scikit-learn-service">Setting up Scikit-learn service&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Please complete the basic installation of
&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi">cloudmesh-openapi&lt;/a>,
To make set up easy the same steps are even referenced at the
Developer Installation section in the document.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You can find Scikit-learn documentation in
&lt;a href="https://scikit-learn.org/dev/modules/classes.html">Scikit-learn&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The following packages needs to be installed to access Scikit-learn&lt;/p>
&lt;p>.. code:: bash&lt;/p>
&lt;p>pip install pandas
pip install Scikit-learn&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Navigate to the &lt;code>./cloudmesh-openapi&lt;/code> directory on your machine&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Utilize the Scikit-learn generate command to create the python file
which will used to generate OpenAPI spec&lt;/p>
&lt;p>.. code:: bash&lt;/p>
&lt;pre>&lt;code>cms openapi sklearnreadfile sklearn.linear_model.LinearRegression Linregpytest
&lt;/code>&lt;/pre>
&lt;p>The sample generated file can be viewed at
&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/tree/main/tests/generator">tests/generator&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Utilize the generate command to generate OpenAPI spec with upload functionality enabled&lt;/p>
&lt;p>.. code:: bash&lt;/p>
&lt;pre>&lt;code>cms openapi generate --filename=./tests/generator/LinearRegression.py --all_functions --enable_upload
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Start the server after the yaml file is generated ot the same directory as the .py file&lt;/p>
&lt;p>.. code:: bash&lt;/p>
&lt;pre>&lt;code>cms openapi server start ./tests/generator/LinearRegression.yaml
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Access the REST service using
&lt;a href="http://localhost:8080/cloudmesh/ui/">http://localhost:8080/cloudmesh/ui/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Run a curl command against the newly running server to upload the
testfiles.&lt;/p>
&lt;p>Place your test files in
&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/tree/main/tests/Scikitlearn-data">Scikitlearn-data&lt;/a>
We are testing with X_SAT.csv(SAT Scores of students),y_GPA(GPA of
students)&lt;/p>
&lt;p>.. code:: bash&lt;/p>
&lt;pre>&lt;code>curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; \
-H &amp;quot;accept: text/plain&amp;quot; \
-H &amp;quot;Content-Type: multipart/form-data&amp;quot; \
-F &amp;quot;upload=@tests/Scikitlearn-data/X_SAT.csv;type=text/csv&amp;quot;
curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; \
-H &amp;quot;accept: text/plain&amp;quot; \
-H &amp;quot;Content-Type: multipart/form-data&amp;quot; \
-F &amp;quot;upload=@tests/Scikitlearn-data/y_GPA.csv;type=text/csv&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Run a curl command against the newly running server to verify fit
method in Scikit-learn using the uploaded files&lt;/p>
&lt;p>.. code:: bash&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://localhost:8080/cloudmesh/LinearRegression_upload-enabled/fit?X=X_SAT&amp;amp;y=y_GPA&amp;quot; -H &amp;quot;accept: */*&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Run a curl command against the newly running server to run the
Predict method.&lt;/p>
&lt;p>.. code:: bash&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://localhost:8080/cloudmesh/LinearRegression_upload-enabled/predict?X=X_SAT&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Run a curl command against the newly running server to run the Score method.&lt;/p>
&lt;p>.. code:: bash&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://localhost:8080/cloudmesh/LinearRegression_upload-enabled/score?X=X_SAT&amp;amp;y=y_GPA&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Stop the server&lt;/p>
&lt;p>.. code:: bash&lt;/p>
&lt;pre>&lt;code>cms openapi server stop LinearRegression
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/readme-security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/readme-security/</guid><description>
&lt;p>README-security is under development.
It is cited in one of our draft papers&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/readme/</guid><description>
&lt;h1 id="cloudmesh-openapi-service-generator">Cloudmesh OpenAPI Service Generator&lt;/h1>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> The README.md page is outomatically generated, do not edit it.
To modify change the content in
&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/README-source.md">https://github.com/cloudmesh/cloudmesh-openapi/blob/main/README-source.md&lt;/a>
Curley brackets must use two in README-source.md&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://pypi.org/project/cloudmesh-openapi/">&lt;img src="https://img.shields.io/pypi/v/cloudmesh-openapi.svg" alt="image">&lt;/a>
&lt;a href="https://pypi.python.org/pypi/cloudmesh-openapi">&lt;img src="https://img.shields.io/pypi/pyversions/cloudmesh-openapi.svg" alt="Python">&lt;/a>
&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/LICENSE">&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" alt="License">&lt;/a>
&lt;a href="https://pypi.python.org/pypi/cloudmesh-openapi">&lt;img src="https://img.shields.io/pypi/format/cloudmesh-openapi.svg" alt="Format">&lt;/a>
&lt;a href="https://pypi.python.org/pypi/cloudmesh-openapi">&lt;img src="https://img.shields.io/pypi/status/cloudmesh-openapi.svg" alt="Status">&lt;/a>
&lt;a href="https://travis-ci.com/cloudmesh/cloudmesh-openapi">&lt;img src="https://travis-ci.com/cloudmesh/cloudmesh-openapi.svg?branch=main" alt="Travis">&lt;/a>&lt;/p>
&lt;h2 id="publication">Publication&lt;/h2>
&lt;p>A draft paper is available at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/laszewski/laszewski.github.io/raw/master/papers/vonLaszewski-openapi.pdf">https://github.com/laszewski/laszewski.github.io/raw/master/papers/vonLaszewski-openapi.pdf&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The source to the paper is at&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cyberaide/paper-openapi">https://github.com/cyberaide/paper-openapi&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>We recommend Python 3.8.2 Python or newer.&lt;/li>
&lt;li>We recommend pip version 20.0.2 or newer&lt;/li>
&lt;li>We recommend that you use a venv (see developer install)&lt;/li>
&lt;li>MongoDB installed as regular program not as service, which can
easily be done with cloudmesh on macOS, Linux, and Windows.&lt;/li>
&lt;li>Please run &lt;code>cms gui quick&lt;/code> to initialize the password for the mongodb
server&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Note: On windows you can use &lt;a href="https://gitforwindows.org/">gitbash&lt;/a>
so you can use bash and can use the same commands as on Linux or
macOS. Otherwise, please use the appropriate backslashes to access
the path.&lt;/p>
&lt;/blockquote>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>The installation is rather simple and is documented next.&lt;/p>
&lt;pre>&lt;code>python -m venv ~/ENV3
source ~/ENV3/bin/activate
mkdir cm
cd cm
pip install cloudmesh-installer
cloudmesh-installer get openapi
cms help
cms gui quick
# fill out mongo variables
# make sure autinstall is True
cms config set cloudmesh.data.mongo.MONGO_AUTOINSTALL=True
cms admin mongo install --force
# Restart a new terminal to make sure mongod is in your path
cms init
&lt;/code>&lt;/pre>&lt;p>If you like to know more about the installation of cloudmesh, please
visit the &lt;a href="https://cloudmesh.github.io/cloudmesh-manual/installation/install.html">Cloudmesh
Manual&lt;/a>.&lt;/p>
&lt;h2 id="command-overview">Command Overview&lt;/h2>
&lt;p>When getting started using cloudmes &lt;code>openapi&lt;/code>, please first call to
get familiar with the options you have:&lt;/p>
&lt;pre>&lt;code>cms help openapi
&lt;/code>&lt;/pre>&lt;p>We include the manual page later on in this document.&lt;/p>
&lt;h2 id="quick-start">Quick Start&lt;/h2>
&lt;p>Next we provide a very simple quickstart guide to steps to generate a
simple microservice that returns the CPU information of your computer.
We demonstrate how to generate, start, and stop the servive.&lt;/p>
&lt;p>Navigate to &lt;code>~/cm/cloudmesh-openapi&lt;/code> folder. In this folder you will
have a file called &lt;code>cpu.py&lt;/code> from which we will generate the server.&lt;/p>
&lt;p>First, generate an OpenAPI YAML file with the convenient command&lt;/p>
&lt;pre>&lt;code>cms openapi generate get_processor_name \
--filename=./tests/server-cpu/cpu.py
&lt;/code>&lt;/pre>&lt;p>This will create the file &lt;code>cpu.yaml&lt;/code> that contains the OpenAPI
specification. To start the service from this specification simply use
the command&lt;/p>
&lt;pre>&lt;code>cms openapi server start ./tests/server-cpu/cpu.yaml
&lt;/code>&lt;/pre>&lt;p>Now that the service is up and running, you can issue a request for
example via the commandline with&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://localhost:8080/cloudmesh/get_processor_name&amp;quot; \
-H &amp;quot;accept: text/plain&amp;quot;
&lt;/code>&lt;/pre>&lt;p>To view the automatically generated documentation, you can go to your
browser and open the link&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://localhost:8080/cloudmesh/ui">http://localhost:8080/cloudmesh/ui&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="images/openapi-ui.png" alt="">&lt;/p>
&lt;p>You can also look at the status of the server with the command&lt;/p>
&lt;pre>&lt;code>cms openapi server list
&lt;/code>&lt;/pre>&lt;p>&lt;img src="images/openapi-info.png" alt="">&lt;/p>
&lt;p>Once yo no longer need the service, you can stop it with&lt;/p>
&lt;pre>&lt;code>cms openapi server stop cpu
&lt;/code>&lt;/pre>&lt;h2 id="quickstart-to-creating-your-own-microservice">Quickstart to Creating your own Microservice&lt;/h2>
&lt;p>Cloudmesh uses introspection to generate an OpenAPI compliant YAML
specification that will allow your Python code to run as a web
service. For this reason, any code you write must conform to a set of
guidelines.&lt;/p>
&lt;ul>
&lt;li>The parameters and return values of any functions you write must use
python typing&lt;/li>
&lt;li>Your functions must include docstrings&lt;/li>
&lt;li>If a function uses or returns a class, that class must be defined as
a dataclass in the same file&lt;/li>
&lt;/ul>
&lt;p>Next we demonstrate how to create your own microservice.
We provide two examples. One in which we return a float,
te other one in which the return value is wrapped in a
json object.&lt;/p>
&lt;h3 id="returning-a-float">Returning a Float&lt;/h3>
&lt;p>We define a function that adds tow values. Note how x,
y, and the return value are all typed. In this case they are all
&lt;code>float&lt;/code>, but other types are supported. The description in the
docstring will be added to your YAML specification to help describe
what the function does.&lt;/p>
&lt;pre>&lt;code>def add(x: float, y: float) -&amp;gt; float:
&amp;quot;&amp;quot;&amp;quot;
adding float and float.
:param x: x value
:type x: float
:param y: y value
:type y: float
:return: result
:return type: float
&amp;quot;&amp;quot;&amp;quot;
result = x + y
return result
&lt;/code>&lt;/pre>&lt;p>To generate, start, retrieve a result, and stop the service you can use the
following command sequence:&lt;/p>
&lt;pre>&lt;code>cms openapi generate add --filename=./tests/add-float/add.py
cms openapi server start ./tests/add-float/add.yaml
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/add?x=1&amp;amp;y=2&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
# This command returns
&amp;gt; 3.0
cms openapi server stop add
&lt;/code>&lt;/pre>&lt;h3 id="returning-a-json-object">Returning a Json Object&lt;/h3>
&lt;p>Often we like to wrap the return value into a json string object, which can easily be
done by modifying the previous example as showcased next.&lt;/p>
&lt;pre>&lt;code>from flask import jsonify
def add(x: float, y: float) -&amp;gt; str:
&amp;quot;&amp;quot;&amp;quot;
adding float and float.
:param x: x value
:type x: float
:param y: y value
:type y: float
:return: result
:return type: float
&amp;quot;&amp;quot;&amp;quot;
result = {&amp;quot;result&amp;quot;: x + y}
return jsonify(result)
&lt;/code>&lt;/pre>&lt;p>To generate, start, retrieve a result, and stop the service you can use the
following command sequence:&lt;/p>
&lt;pre>&lt;code>cms openapi generate add --filename=./tests/add-json/add.py
cms openapi server start ./tests/add-json/add.yaml
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/add?x=1&amp;amp;y=2&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
# This command returns
&amp;gt; {&amp;quot;result&amp;quot;:3.0}
cms openapi server stop add
&lt;/code>&lt;/pre>&lt;p>As usual in both cases the web browser can be used to inspect the documentation as well as to test running the
example, by filling out the form.&lt;/p>
&lt;h2 id="details-of-the-cms-openapi-command">Details of the &lt;code>cms openapi&lt;/code> command&lt;/h2>
&lt;p>The gaol as stated earlier is to transform a simple python function as a service&lt;/p>
&lt;h3 id="generating-openapi-specification">Generating OpenAPI specification&lt;/h3>
&lt;p>Once you have a Python function you would like to deploy as a web
service, you can generate the OpenAPI specification. Navigate to your
.py file&amp;rsquo;s directory and generate the YAML. This will print
information to your console about the YAML file that was generated.&lt;/p>
&lt;pre>&lt;code>cms openapi generate [function_name] --filename=[filename.py]
&lt;/code>&lt;/pre>&lt;p>If you would like to include more than one function in your web
service, like addition and subtraction, use the &lt;code>--all_functions&lt;/code>
flag. This will ignore functions whose names start with &amp;lsquo;_&amp;rsquo;.&lt;/p>
&lt;pre>&lt;code>cms openapi generate --filename=[filename.py] --all_functions
&lt;/code>&lt;/pre>&lt;p>You can even write a class like Calculator that contains functions for
addition, subtraction, etc. You can generate a specification for an
entire class by using the &lt;code>--import_class&lt;/code> flag.&lt;/p>
&lt;pre>&lt;code>cms openapi generate [ClassName] --filename=[filename.py] --import_class
&lt;/code>&lt;/pre>&lt;h3 id="starting-a-server">Starting a server&lt;/h3>
&lt;p>Once you have generated a specification, you can start the web service
on your localhost by providing the path to the YAML file. This will
print information to your console about the server&lt;/p>
&lt;pre>&lt;code>cms openapi server start ./[filename.yaml]
Starting: [server name]
PID: [PID]
Spec: ./[filename.py]
URL: http://localhost:8080/cloudmesh
Cloudmesh UI: http://localhost:8080/cloudmesh/ui
&lt;/code>&lt;/pre>&lt;h3 id="sending-requests-to-the-server">Sending requests to the server&lt;/h3>
&lt;p>Now you have two options to interact with the web service. The first
is to navigate the the Cloudmesh UI and click on each endpoint to test
the functionality. The second is to use curl commands to submit
requests.&lt;/p>
&lt;p>We have already shown you earlier in our quickstart how to apply this to a
service such as our add service&lt;/p>
&lt;pre>&lt;code>$ curl -X GET &amp;quot;http://localhost:8080/cloudmesh/add?x=1.2&amp;amp;y=1.5&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&amp;gt; 2.7
&lt;/code>&lt;/pre>&lt;h3 id="stopping-the-server">Stopping the server&lt;/h3>
&lt;p>Now you can stop the server using the name of the server. If you
forgot the name, use &lt;code>cms openapi server ps&lt;/code> to get a list of server
processes.&lt;/p>
&lt;pre>&lt;code>$ cms openapi server stop [server name]
&lt;/code>&lt;/pre>&lt;h3 id="basic-auth">Basic Auth&lt;/h3>
&lt;p>To use basic http authentication with a user password for the
generated API, add the following flag at the end of a &lt;code>cms openapi generate&lt;/code> command:&lt;/p>
&lt;pre>&lt;code>--basic_auth=&amp;lt;username&amp;gt;:&amp;lt;password&amp;gt;
&lt;/code>&lt;/pre>&lt;p>We plan on supporting more security features in the future. Example:&lt;/p>
&lt;pre>&lt;code>cms openapi generate get_processor_name \
--filename=./tests/server-cpu/cpu.py \
--basic_auth=admin:secret
&lt;/code>&lt;/pre>&lt;h2 id="manual-page">Manual Page&lt;/h2>
&lt;pre>&lt;code>openapi generate [FUNCTION] --filename=FILENAME
[--serverurl=SERVERURL]
[--yamlfile=YAML]
[--import_class]
[--all_functions]
[--enable_upload]
[--verbose]
[--basic_auth=CREDENTIALS]
openapi server start YAML [NAME]
[--directory=DIRECTORY]
[--port=PORT]
[--server=SERVER]
[--host=HOST]
[--verbose]
[--debug]
[--fg]
[--os]
openapi server stop NAME
openapi server list [NAME] [--output=OUTPUT]
openapi server ps [NAME] [--output=OUTPUT]
openapi register add NAME ENDPOINT
openapi register filename NAME
openapi register delete NAME
openapi register list [NAME] [--output=OUTPUT]
openapi TODO merge [SERVICES...] [--dir=DIR] [--verbose]
openapi TODO doc FILE --format=(txt|md)[--indent=INDENT]
openapi TODO doc [SERVICES...] [--dir=DIR]
openapi sklearn FUNCTION MODELTAG
openapi sklearnreadfile FUNCTION MODELTAG
openapi sklearn upload --filename=FILENAME
Arguments:
FUNCTION The name for the function or class
MODELTAG The arbirtary name choosen by the user to store the Sklearn trained model as Pickle object
FILENAME Path to python file containing the function or class
SERVERURL OpenAPI server URL Default: https://localhost:8080/cloudmesh
YAML Path to yaml file that will contain OpenAPI spec. Default: FILENAME with .py replaced by .yaml
DIR The directory of the specifications
FILE The specification
Options:
--import_class FUNCTION is a required class name instead of an optional function name
--all_functions Generate OpenAPI spec for all functions in FILENAME
--debug Use the server in debug mode
--verbose Specifies to run in debug mode
[default: False]
--port=PORT The port for the server [default: 8080]
--directory=DIRECTORY The directory in which the server is run
--server=SERVER The server [default: flask]
--output=OUTPUT The outputformat, table, csv, yaml, json
[default: table]
--srcdir=SRCDIR The directory of the specifications
--destdir=DESTDIR The directory where the generated code
is placed
Description:
This command does some useful things.
openapi TODO doc FILE --format=(txt|md|rst) [--indent=INDENT]
Sometimes it is useful to generate teh openaopi documentation
in another format. We provide fucntionality to generate the
documentation from the yaml file in a different formt.
openapi TODO doc --format=(txt|md|rst) [SERVICES...]
Creates a short documentation from services registered in the
registry.
openapi TODO merge [SERVICES...] [--dir=DIR] [--verbose]
Merges tow service specifications into a single servoce
TODO: do we have a prototype of this?
openapi sklearn sklearn.linear_model.LogisticRegression
Generates the .py file for the Model given for the generator
openapi sklearnreadfile sklearn.linear_model.LogisticRegression
Generates the .py file for the Model given for the generator which supports reading files
openapi generate [FUNCTION] --filename=FILENAME
[--serverurl=SERVERURL]
[--yamlfile=YAML]
[--import_class]
[--all_functions]
[--enable_upload]
[--verbose]
[--basic_auth=CREDENTIALS]
Generates an OpenAPI specification for FUNCTION in FILENAME and
writes the result to YAML. Use --import_class to import a class
with its associated class methods, or use --all_functions to
import all functions in FILENAME. These options ignore functions
whose names start with '_'. Use --enable_upload to add file
upload functionality to a copy of your python file and the
resulting yaml file.
For optional basic authorization, we support (temporarily) a single user
credential. CREDENTIALS should be formatted as follows:
user:password
Example: --basic_auth=admin:secret
openapi server start YAML [NAME]
[--directory=DIRECTORY]
[--port=PORT]
[--server=SERVER]
[--host=HOST]
[--verbose]
[--debug]
[--fg]
[--os]
starts an openapi web service using YAML as a specification
TODO: directory is hard coded as None, and in server.py it
defaults to the directory where the yaml file lives. Can
we just remove this argument?
openapi server stop NAME
stops the openapi service with the given name
TODO: where does this command has to be started from
openapi server list [NAME] [--output=OUTPUT]
Provides a list of all OpenAPI services in the registry
openapi server ps [NAME] [--output=OUTPUT]
list the running openapi service
openapi register add NAME ENDPOINT
Openapi comes with a service registry in which we can register
openapi services.
openapi register filename NAME
In case you have a yaml file the openapi service can also be
registerd from a yaml file
openapi register delete NAME
Deletes the names service from the registry
openapi register list [NAME] [--output=OUTPUT]
Provides a list of all registerd OpenAPI services
&lt;/code>&lt;/pre>&lt;h2 id="basic-examples">Basic Examples&lt;/h2>
&lt;p>Please follow &lt;a href="tests/README.md">Pytest Information&lt;/a> document for
pytests related information&lt;/p>
&lt;p>We have included a significant number of tests that aso serve as examples&lt;/p>
&lt;h3 id="example-one-function-in-a-python-file">Example: One function in a python file&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Please check &lt;a href="tests/server-cpu/cpu.py">Python file&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Run below command to generate yaml file and start server&lt;/p>
&lt;pre>&lt;code>cms openapi generate get_processor_name --filename=./tests/server-cpu/cpu.py
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;h3 id="example-multiple-functions-in-python-file">Example: Multiple functions in python file&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Please check &lt;a href="tests/generator-calculator/calculator.py">Python file&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Run below command to generate yaml file and start server&lt;/p>
&lt;pre>&lt;code>cms openapi generate --filename=./tests/generator-calculator/calculator.py --all_functions
cms openapi generate server start ./tests/generator-calculator/calculator.py
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;h3 id="example-functions-in-python-class-file">Example: Function(s) in python class file&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Please check &lt;a href="tests/generator-testclass/calculator.py">Python file&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Run below command to generate yaml file and start server&lt;/p>
&lt;pre>&lt;code>cms openapi generate Calculator \
--filename=./tests/generator-testclass/calculator.py \
--import_class&amp;quot;
cms openapi server start ./tests/generator-testclass/calculator.yaml
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/Calculator/multiplyint?x=1&amp;amp;y=5&amp;quot;
cms openapi server stop Calculator
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;h3 id="example-uploading-data">Example: Uploading data&lt;/h3>
&lt;p>Code to handle uploads is located in
&lt;code>cloudmesh-openapi/tests/generator-upload&lt;/code>. The code snippet in
uploadexample.py and the specification in uploadexample.yaml can be
added to existing projects by adding the &lt;code>--enable_upload&lt;/code> flag to the
&lt;code>cms openapi generate&lt;/code> command. The web service will be able to
retrieve the uploaded file from &lt;code>~/.cloudmesh/upload-file/&lt;/code>.&lt;/p>
&lt;h4 id="upload-example">Upload example&lt;/h4>
&lt;p>This example shows how to upload a CSV file and how the web service
can retrieve it.&lt;/p>
&lt;p>First, generate the OpenAPI specification and start the server&lt;/p>
&lt;pre>&lt;code>cms openapi generate print_csv2np \
--filename=./tests/generator-upload/csv_reader.py \
--enable_upload
cms openapi server start ./tests/generator-upload/csv_reader.yaml
&lt;/code>&lt;/pre>&lt;p>Next, navigate to localhost:8080/cloudmesh/ui. Click to open
the /upload endpoint, then click &amp;lsquo;Try it out.&amp;rsquo; Click to choose a file
to upload, then upload &lt;code>tests/generator-upload/np_test.csv&lt;/code>. Click
&amp;lsquo;Execute&amp;rsquo; to complete the upload.&lt;/p>
&lt;p>The uploaded file will be located at
&lt;code>~/.cloudmesh/upload-file/[filename]&lt;/code>. The file
&lt;code>tests/generator-upload/csv_reader.py&lt;/code> contains some example code to
retrieve the array in the uploaded file. To see this in action, click
to open the /print_csv2np endpoint, then click &amp;lsquo;Try it out.&amp;rsquo; Enter
&amp;ldquo;np_test.csv&amp;rdquo; in the field that prompts for a filename, and then click
Execute to view the numpy array defined by the CSV file.&lt;/p>
&lt;h3 id="example-pipeline-anova-svm-example">Example: Pipeline Anova SVM Example&lt;/h3>
&lt;p>This example is based on the sklearn example
&lt;a href="https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection_pipeline.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-pipeline-py">here&lt;/a>&lt;/p>
&lt;p>In this example, we will upload a data set to the server, tell the
server to train the model, and utilize said model for predictions.&lt;/p>
&lt;p>Firstly, ensure we are in the correct directory.&lt;/p>
&lt;pre>&lt;code>$ pwd
~/cm/cloudmesh-openapi
&lt;/code>&lt;/pre>&lt;p>Then install some requirements needed for this example&lt;/p>
&lt;pre>&lt;code>$ pip install sklearn pandas
&lt;/code>&lt;/pre>&lt;p>Let us generate the yaml file from our python file to generate the proper specs for our service.&lt;/p>
&lt;pre>&lt;code>$ cms openapi generate PipelineAnovaSVM \
--filename=./tests/Scikitlearn-experimental/sklearn_svm.py \
--import_class --enable_upload
&lt;/code>&lt;/pre>&lt;p>Now let us start the server&lt;/p>
&lt;pre>&lt;code>$ cms openapi server start ./tests/Scikitlearn-experimental/sklearn_svm.yaml
&lt;/code>&lt;/pre>&lt;p>The server should now be active. Navigate to
&lt;code>http://localhost:8080/cloudmesh/ui&lt;/code>. We now have a nice user inteface
to interact with our newly generated API. Let us upload the data
set. We are going to use the iris data set in this example. We have
provided it for you to use. Simply navigate to the &lt;code>/upload&lt;/code> endpoint
by clicking on it, then click &lt;code>Try it out&lt;/code>.&lt;/p>
&lt;p>We can now upload the file. Click on &lt;code>Choose File&lt;/code> and upload the data
set located at &lt;code>./tests/Scikitlearn-experimental/iris.data&lt;/code>. Simply
hit &lt;code>Execute&lt;/code> after the file is uploaded. We should then get a return
code of 200 (telling us that everything went ok).&lt;/p>
&lt;p>The server now has our dataset. Let us now navigate to the &lt;code>/train&lt;/code>
endpoint by, again, clicking on it. Similarly, click &lt;code>Try it out&lt;/code>.
The parameter being asked for is the filename. The filename we are
interested in is &lt;code>iris.data&lt;/code>. Then click &lt;code>execute&lt;/code>. We should get
another 200 return code with a Classification Report in the Response
Body.&lt;/p>
&lt;pre>&lt;code class="language-CLASSIFICATION_REPORT:" data-lang="CLASSIFICATION_REPORT:">
0 1.00 1.00 1.00 8
1 0.85 1.00 0.92 11
2 1.00 0.89 0.94 19
accuracy 0.95 38
macro avg 0.95 0.96 0.95 38
weighted avg 0.96 0.95 0.95 38
&lt;/code>&lt;/pre>&lt;p>Our model is now trained and stored on the server. Let us make a
prediction now. As we have done, navigate to the &lt;code>/make_prediction&lt;/code>
endpoint. The information we need to provide is the name of the model
we have trained as well as some test data. The name of the model will
be the same as the name of the data-file (ie. iris). So type in &lt;code>iris&lt;/code>
into the &lt;code>model_name&lt;/code> field. Finally for params, let us use the
example &lt;code>5.1, 3.5, 1.4, 0.2&lt;/code> as the model expects 4 values
(attributes). After clicking execute, we should received a response
with the classification the model has made given the parameters.&lt;/p>
&lt;p>The response received should be as follows:&lt;/p>
&lt;pre>&lt;code>&amp;quot;Classification: ['Iris-setosa']&amp;quot;
&lt;/code>&lt;/pre>&lt;p>We can make as many predictions as we like. When finished, we can shut down the server.&lt;/p>
&lt;pre>&lt;code>$ cms openapi server stop sklearn_svm
&lt;/code>&lt;/pre>&lt;h2 id="example-to-run-ai-services-in-the-cloud">Example to Run AI Services in the Cloud&lt;/h2>
&lt;h3 id="google">Google&lt;/h3>
&lt;p>After you create your google cloud account, it is recommended to
download and install Google&amp;rsquo;s &lt;a href="https://cloud.google.com/sdk/docs/quickstarts">Cloud
SDK&lt;/a>. This will
enable CLI. Make sure you enable all the required services.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;pre>&lt;code>gcloud services enable servicemanagement.googleapis.com
gcloud services enable endpoints.googleapis.com
&lt;/code>&lt;/pre>&lt;p>and any other services you might be using for your specific Cloud API
function.&lt;/p>
&lt;p>To begin using the tests for any of the Google Cloud Platform AI
services you must first set up a Google account (set up a free tier
account): &lt;a href="https://cloud.google.com/billing/docs/how-to/manage-billing-account">Google Account
Setup&lt;/a>&lt;/p>
&lt;p>After you create your google cloud account, it is recommended to
download and install Google&amp;rsquo;s &lt;a href="https://cloud.google.com/sdk/docs/quickstarts">Cloud
SDK&lt;/a>. This will
enable CLI. Make sure you enable all the required services.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;pre>&lt;code>gcloud services enable servicemanagement.googleapis.com
gcloud services enable servicecontrol.googleapis.com
gcloud services enable endpoints.googleapis.com
&lt;/code>&lt;/pre>&lt;p>and any other services you might be using for your specific Cloud API
function.&lt;/p>
&lt;p>It is also required to install the cloudmesh-cloud package, if not
already installed:&lt;/p>
&lt;pre>&lt;code>cloudmesh-installer get cloud
cloudmesh-installer install cloud
&lt;/code>&lt;/pre>&lt;p>This will allow you automatically fill out the cloudmesh yaml file
with your credentials once you generate the servcie account JSON file.&lt;/p>
&lt;p>After you have verified your account is created you must then give your account access to the proper APIs and create a
project in the Google Cloud Platform(GCP) console.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Go to the &lt;a href="console.cloud.google.com/projectselector2/home/">project
selector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Follow directions from Google to create a project linked to your
account&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="quickstart-google-python-api">Quickstart Google Python API&lt;/h3>
&lt;pre>&lt;code>pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>For quickstart in using Google API for Python visit &lt;a href="https://developers.google.com/docs/api/quickstart/python">here&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="setting-up-your-google-account">Setting up your Google account&lt;/h3>
&lt;p>Before you generate the service account JSON file for your account you
will want to enable a number of services in the GCP console.&lt;/p>
&lt;ul>
&lt;li>Google Compute&lt;/li>
&lt;li>Billing&lt;/li>
&lt;li>Cloud Natural Language API&lt;/li>
&lt;li>Translate API&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;p>To do this you will need to click the menu icon in the Dashboard
navigation bar. Ensure you are in the correct porject.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once that menu is open hover over the &amp;ldquo;APIs and Services&amp;rdquo; menu item
and click on &amp;ldquo;Dashboard&amp;rdquo; in the submenu.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>At the dashboard click on the &amp;ldquo;+ Enable APIs and Services&amp;rdquo; button
at the top of the dashboard&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Search for &lt;strong>cloud natural language&lt;/strong> to find the API in the search
results and click the result&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once the page opens click &amp;ldquo;Enable&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Do the same for the &lt;strong>translate&lt;/strong> API to enable that as well&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Do the same for the &lt;strong>compute engine API&lt;/strong> to enable that as well&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>You must now properly set up the account roles to ensure you will have
access to the API. Follow the directions from Google to &lt;a href="https://cloud.google.com/natural-language/docs/setup#auth">set up proper
authentication&lt;/a>&lt;/p>
&lt;p>Make you account an owner for each of the APIs in the IAM tool as
directed in the authentication steps for the natural language API.
This makes your service account have proper access to the required
APIs and once the private key is downloaded those will be stored
there.&lt;/p>
&lt;p>It is VERY important that you create a service account and download
the private key as described in the directions from Google. If you do
not the cms google commands will not work properly.&lt;/p>
&lt;p>Once you have properly set up your permissions please make sure you
download your JSON private key for the service account that has
permissions set up for the required API services. These steps to
download are found
&lt;a href="https://cloud.google.com/natural-language/docs/setup#sa-create">here&lt;/a>.
Please take note of where you store the downloaded JSON and copy the
path string to a easily accessible location.&lt;/p>
&lt;p>The client libraries for each API are included in teh requirements.txt file for the openapi proejct and should be isntalled when the
package is installed. If not, follow directions outlined by google install each package:&lt;/p>
&lt;pre>&lt;code>google-cloud-translate
google-cloud-language
&lt;/code>&lt;/pre>&lt;p>To pass the information from your service account private key file ot
the cloudmesh yaml file run the following command:&lt;/p>
&lt;pre>&lt;code>cms register update --kind=google --service=compute --filename=GOOGLE_JSON_FILE
&lt;/code>&lt;/pre>&lt;h4 id="running-the-google-natural-language-and-translate-rest-services">Running the Google Natural Language and Translate REST Services&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>Navigate to the &lt;code>~/.cloudmesh&lt;/code> repo and create a cache directory
for your text examples you would like to analyze.&lt;/p>
&lt;pre>&lt;code>mkdir text-cache
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Add any plain text files your would like to analyze to this
directory with a name that has no special characters or spaces.
You can copy the files at this location,
&lt;code>./cloudmesh-openapi/tests/textanaysis-example-text/reviews/&lt;/code> into
the text-cache if you want to use provided examples.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Navigate to the &lt;code>./cloudmesh-openapi&lt;/code> directory on your machine&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Utilize the generate command to create the OpenAPI spec&lt;/p>
&lt;pre>&lt;code>cms openapi generate TextAnalysis --filename=./tests/generator-natural-lang/natural-lang-analysis.py --all_functions
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Start the server after the yaml file is generated ot the same
directory as the .py file&lt;/p>
&lt;pre>&lt;code>cms openapie start server ./tests/generator-natural-lang/natural-lang-analysis.yaml
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Run a curl command against the newly running server to verify it
returns a result as expected.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Sample text file name is only meant to be the name of the file
not the full path.&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/analyze?filename=SAMPLE_TEXT_FILENAME&amp;amp;cloud=google&amp;quot;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>This is currently only ready to translate a single word through
the API.&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/translate_text?cloud=google&amp;amp;text=WORD_TO_TRANSLATE&amp;amp;lang=LANG_CODE&amp;quot;
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Stop the server&lt;/p>
&lt;pre>&lt;code>cms openapi server stop natural-lang-analysis
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;h3 id="example-using-aws">Example using AWS&lt;/h3>
&lt;p>Sign up for AWS&lt;/p>
&lt;ul>
&lt;li>Go to &lt;a href="https://portal.aws.amazon.com/billing/signup">https://portal.aws.amazon.com/billing/signup&lt;/a>&lt;/li>
&lt;li>Follow online instructions&lt;/li>
&lt;/ul>
&lt;p>Create an IAM User&lt;/p>
&lt;ul>
&lt;li>For instructions, see
&lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html">here&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Set up AWS CLI and AWS SDKs&lt;/p>
&lt;ul>
&lt;li>To download and instructions to install AWS CLI, see
&lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install">here&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Install Boto 3&lt;/p>
&lt;pre>&lt;code>pip install boto3
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>For quickstart, vist &lt;a href="https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html">here&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>As long as you enable all the services you need for using AWS AI APIs you should be able to write your functions for OpenAPI&lt;/p>
&lt;h3 id="example-using-azure">Example using Azure&lt;/h3>
&lt;h4 id="setting-up-azure-sentiment-analysis-and-translation-services">Setting up Azure Sentiment Analysis and Translation Services&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>Create an Azure subscription. If you do not have one, create a
&lt;a href="https://azure.microsoft.com/try/cognitive-services/">free account&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create a &lt;a href="https://portal.azure.com/#create/Microsoft.CognitiveServicesTextAnalytics">Text Analysis resource&lt;/a>&lt;/p>
&lt;ul>
&lt;li>This link will require you to be logged in to the Azure portal&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Create a &lt;a href="https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows">Translation Resource&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The microsoft packages are included in the openapi package
requirements file so they should be installed. If they are not,
install the following:&lt;/p>
&lt;pre>&lt;code>pip install msrest
pip install azure-ai-textanalytics
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Navigate to the &lt;code>~/.cloudmesh&lt;/code> repo and create a cache directory for your text examples you would like to analyze.&lt;/p>
&lt;pre>&lt;code>mkdir text-cache
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Add any plain text files your would like to analyze to this
directory with a name that has no special characters or spaces.
You can copy the files at this location,
&lt;code>./cloudmesh-openapi/tests/textanaysis-example-text/reviews/&lt;/code> into
the text-cache if you want to use provided examples.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Navigate to the &lt;code>./cloudmesh-openapi&lt;/code> directory on your machine&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Utilize the generate command to create the OpenAPI spec&lt;/p>
&lt;pre>&lt;code>cms openapi generate TextAnalysis --filename=./tests/generator-natural-lang/natural-lang-analysis.py --all_functions
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Start the server after the yaml file is generated ot the same
directory as the .py file&lt;/p>
&lt;pre>&lt;code>cms openapie start server ./tests/generator-natural-lang/natural-lang-analysis.yaml
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Run a curl command against the newly running server to verify it
returns a result as expected.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Sample text file name is only meant to be the name of the file not the full path.&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/analyze?filename=&amp;lt;&amp;lt;sample text file name&amp;gt;&amp;gt;&amp;amp;cloud=azure&amp;quot;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>This is currently only ready to translate a single word through the API.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Available language tags are described in the &lt;a href="https://docs.microsoft.com/en-us/azure/cognitive-services/translator/reference/v3-0-languages">Azure docs&lt;/a>&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/translate_text?cloud=azure&amp;amp;text=&amp;lt;&amp;lt;word to translate&amp;gt;&amp;gt;&amp;amp;lang=&amp;lt;&amp;lt;lang code&amp;gt;&amp;gt;&amp;quot;
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Stop the server&lt;/p>
&lt;pre>&lt;code>cms openapi server stop natural-lang-analysis
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;p>The natural langauge analysis API can be improved by allowing for full
phrase translation via the API. If you contribute to this API there is
room for improvement to add custom translation models as well if
preferred to pre-trained APIs.&lt;/p>
&lt;h4 id="setting-up-azure-computervision-ai-services">Setting up Azure ComputerVision AI services&lt;/h4>
&lt;h5 id="prerequisite">Prerequisite&lt;/h5>
&lt;p>Using the Azure Computer Vision AI service, you can describe, analyze
and/ or get tags for a locally stored image or you can read the text
from an image or hand-written file.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Azure subscription. If you do not have one, create a &lt;a href="https://azure.microsoft.com/try/cognitive-services/">free
account&lt;/a> before
you continue further.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create a Computer Vision resource and get the
&lt;code>COMPUTER_VISION_SUBSCRIPTION_KEY&lt;/code> and
&lt;code>COMPUTER_VISION_ENDPOINT&lt;/code>. Follow
&lt;a href="https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=singleservice%2Cunix">instructions&lt;/a>
to get the same.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Install following Python packages in your virtual environment:&lt;/p>
&lt;pre>&lt;code>pip install requests
pip install Pillow
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Install Computer Vision client library&lt;/p>
&lt;pre>&lt;code>pip install --upgrade azure-cognitiveservices-vision-computervision
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;h5 id="steps-to-implement-and-use-azure-ai-image-and-text-rest-services">Steps to implement and use Azure AI image and text &lt;em>REST-services&lt;/em>&lt;/h5>
&lt;ul>
&lt;li>
&lt;p>Go to &lt;code>./cloudmesh-openapi&lt;/code> directory&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Run following command to generate the YAML files&lt;/p>
&lt;pre>&lt;code>cms openapi generate AzureAiImage --filename=./tests/generator-azureai/azure-ai-image-function.py --all_functions --enable_upload
cms openapi generate AzureAiText --filename=./tests/generator-azureai/azure-ai-text-function.py --all_functions --enable_upload
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Verify the &lt;em>YAML&lt;/em> files created in &lt;code>./tests/generator-azureai&lt;/code> directory&lt;/p>
&lt;pre>&lt;code>azure-ai-image-function.yaml
azure-ai-text-function.yaml
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Start the REST service by running following command in &lt;code>./cloudmesh-openapi&lt;/code> directory&lt;/p>
&lt;pre>&lt;code>cms openapi server start ./tests/generator-azureai/azure-ai-image-function.yaml
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>The default port used for starting the service is 8080. In case you
want to start more than one REST service, use a different port in
following command:&lt;/p>
&lt;pre>&lt;code>cms openapi server start ./tests/generator-azureai/azure-ai-text-function.yaml --port=&amp;lt;**Use a different port than 8080**&amp;gt;
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;p>Access the REST service using &lt;a href="http://localhost:8080/cloudmesh/ui/">http://localhost:8080/cloudmesh/ui/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>After you have started the azure-ai-image-function or azure-ai-text-function on default port 8080, run following command to upload the image or text_image file&lt;/p>
&lt;pre>&lt;code>curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; -H &amp;quot;accept: text/plain&amp;quot; -H &amp;quot;Content-Type: multipart/form-data&amp;quot; -F &amp;quot;upload=@tests/generator-azureai/&amp;lt;image_name_with_extension&amp;gt;;type=image/jpeg&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Keep your test image files at &lt;code>./tests/generator-azureai/&lt;/code> directory&lt;/p>
&lt;/li>
&lt;li>
&lt;p>With &lt;em>azure-ai-text-function&lt;/em> started on port=8080, in order to test the azure ai function for text detection in an image, run following command&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-text-function_upload-enabled/get_text_results?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>With &lt;em>azure-ai-image-function&lt;/em> started on port=8080, in order to
test the azure ai function for describing an image, run following
command&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-image-function_upload-enabled/get_image_desc?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>With &lt;em>azure-ai-image-function&lt;/em> started on port=8080, in order to
test the azure ai function for analyzing an image, run following
command&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-image-function_upload-enabled/get_image_analysis?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>With &lt;em>azure-ai-image-function&lt;/em> started on port=8080, in order to
test the azure ai function for identifying tags in an image, run
following command&lt;/p>
&lt;pre>&lt;code>curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-image-function_upload-enabled/get_image_tags?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Check the running REST services using following command:&lt;/p>
&lt;pre>&lt;code>cms openapi server ps
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Stop the REST service using following command(s):&lt;/p>
&lt;pre>&lt;code>cms openapi server stop azure-ai-image-function
cms openapi server stop azure-ai-text-function
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-tests">List of Tests&lt;/h2>
&lt;p>The following table lists the different test we have, we provide additional
information for the tests in the test directory in a README file. Summaries
are provided below the table&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Test&lt;/th>
&lt;th>Short Description&lt;/th>
&lt;th>Link&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Generator-calculator&lt;/td>
&lt;td>Test to check if calculator api is generated correctly. This is to test multiple function in one python file&lt;/td>
&lt;td>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/generator-calculator/test_01_generator.py">test_01_generator.py&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Generator-testclass&lt;/td>
&lt;td>Test to check if calculator api is generated correctly. This is to test multiple function in one python class file&lt;/td>
&lt;td>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/generator-testclass/test_02_generator.py">test_02_generator.py&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Server-cpu&lt;/td>
&lt;td>Test to check if cpu api is generated correctly. This is to test single function in one python file and function name is different than file name&lt;/td>
&lt;td>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/server-cpu/test_03_generator.py">test_03_generator.py&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Server-cms&lt;/td>
&lt;td>Test to check if cms api is generated correctly. This is to test multiple function in one python file.&lt;/td>
&lt;td>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/server-cms/test_04_generator.py">test_04_generator.py&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Registry&lt;/td>
&lt;td>test_001_registry.py - Runs tests for registry. Description is in tests/README.md&lt;/td>
&lt;td>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/README.md">Link&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Image-Analysis&lt;/td>
&lt;td>image_test.py - Runs benchmark for text detection for Google Vision API and AWS Rekognition. Description in image-analysis/README.md&lt;/td>
&lt;td>&lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/image-analysis/README.md">image&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For more information about test cases ,please check &lt;a href="https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/README.md">tests info&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="tests/test_001_registry.py">test_001_registry&lt;/a>&lt;/li>
&lt;li>&lt;a href="tests/test_003_server_manage_cpu.py">test_003_server_manage_cpu&lt;/a>&lt;/li>
&lt;li>&lt;a href="tests/test_010_generator.py">test_010_generator&lt;/a>&lt;/li>
&lt;li>&lt;a href="tests/test_011_generator_cpu.py">test_011_generator_cpu&lt;/a>&lt;/li>
&lt;li>&lt;a href="tests/test_012_generator_calculator.py">test_012_generator_calculator&lt;/a>&lt;/li>
&lt;li>&lt;a href="tests/test_015_generator_azureai.py">test_015_generator_azureai&lt;/a>&lt;/li>
&lt;li>&lt;a href="tests/test_020_server_manage.py">test_020_server_manage&lt;/a>&lt;/li>
&lt;li>&lt;a href="tests/test_server_cms_cpu.py">test_server_cms_cpu&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Note that there a many more tests that you can explore.&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/tests/add-float/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/tests/add-float/readme/</guid><description>
&lt;h1 id="readme">README&lt;/h1>
&lt;p>please see the README in the root dir of this repository&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/tests/add-json/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/tests/add-json/readme/</guid><description>
&lt;h1 id="readme">README&lt;/h1>
&lt;p>please see the README in the root dir of this repository&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/tests/generator-natural-lang/googlecloudvmset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/tests/generator-natural-lang/googlecloudvmset/</guid><description>
&lt;h1 id="steps">Steps:&lt;/h1>
&lt;ol>
&lt;li>Setup a google account with Google Cloud&lt;/li>
&lt;li>Create a project&lt;/li>
&lt;li>Set permission for create on compute engine in the project&lt;/li>
&lt;li>create a service account file and link to json in cloudmesh yaml file
&lt;a href="https://cloud.google.com/docs/authentication/production?hl=en_US">https://cloud.google.com/docs/authentication/production?hl=en_US&lt;/a>&lt;/li>
&lt;li>Create a storage location using google storage
&lt;a href="https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-code_samples">https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-code_samples&lt;/a>&lt;/li>
&lt;li>Install the google cloud sdk
&lt;a href="https://cloud.google.com/compute/docs/tutorials/python-guide">https://cloud.google.com/compute/docs/tutorials/python-guide&lt;/a>&lt;/li>
&lt;li>Install the google cloud api client library
&lt;a href="https://cloud.google.com/apis/docs/client-libraries-explained">https://cloud.google.com/apis/docs/client-libraries-explained&lt;/a>&lt;/li>
&lt;li>Write a startup script for your vm&lt;/li>
&lt;/ol>
&lt;h2 id="azure">Azure&lt;/h2>
&lt;p>&lt;a href="https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal">https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal&lt;/a>
Credentials:
app-name: vm-creation-example
auth url:https://andyvmcreateexample.com/auth&lt;/p>
&lt;p>app (client) ID: 8db85342-7efd-433c-aeba-d175ae4d4404
directory (tenant) id: 398e5475-e850-4239-ba0d-62ddc3e644ff
object ID: 38224a7e-79e0-4642-b765-2bf731d296ad
client-secret: w[f7o=[dKKeSn?VxF3iNoZDW3ctMmd3G
subscription id:4513afc9-4159-49d0-aa1d-0a2a0ab9933c&lt;/p>
&lt;p>when creating a vm in the portal the network interface is set up for you
but if you do it programmatically you have to set it up.&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/tests/gregor/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/tests/gregor/readme/</guid><description>
&lt;h1 id="test-it-yourself">Test it yourself&lt;/h1>
&lt;p>cd to &lt;code>cloudmesh-openapi&lt;/code>&lt;/p>
&lt;p>Start the service&lt;/p>
&lt;pre>&lt;code>cms openapii server start ./tests/server-cpu.yaml
&lt;/code>&lt;/pre>&lt;p>Stop the service&lt;/p>
&lt;pre>&lt;code>cms openapii3 server stop cpu
&lt;/code>&lt;/pre>&lt;p>urls&lt;/p>
&lt;p>cloudmesh/ui&lt;/p>
&lt;p>cloudmesh/cpu&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/tests/image-analysis/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/tests/image-analysis/readme/</guid><description>
&lt;h1 id="test-it-yourself">Test it yourself&lt;/h1>
&lt;h2 id="in-cloudmesh-openapi">In cloudmesh-openapi&lt;/h2>
&lt;p>Start server&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">cms openapi server start ./tests/image-analysis/image.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Get Response Google Vision&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl -sL http://127.0.0.1:8080/cloudmesh/image/detect_text_google
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Get Response AWS Rekognition&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl -sL http://127.0.0.1:8080/cloudmesh/image/detect_text_aws
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Stop server&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">cms openapi server stop image
&lt;/code>&lt;/pre>&lt;/div>&lt;p>urls&lt;/p>
&lt;p>cloudmesh/image/detect_text_google&lt;/p>
&lt;p>cloudmesh/image/detect_text_aws&lt;/p>
&lt;h2 id="image_testpy">image_test.py&lt;/h2>
&lt;p>How to run test&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">pytest -v --capture&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>no tests/image-analysis/image_test.py
&lt;/code>&lt;/pre>&lt;/div>&lt;p>image_test.py has 7 tests&lt;/p>
&lt;ol>
&lt;li>Uses generate command to generate new yaml file&lt;/li>
&lt;li>Check yaml syntax&lt;/li>
&lt;li>Starts server&lt;/li>
&lt;li>Does a curl call for google vision api response&lt;/li>
&lt;li>Does a curl call for aws rekognition api response&lt;/li>
&lt;li>Stops the server&lt;/li>
&lt;li>Prints benchmark&lt;/li>
&lt;/ol></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/tests/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/tests/readme/</guid><description>
&lt;h1 id="how-to-write-and-run-test-case-for-openapi">How to write and run test case for OpenAPI&lt;/h1>
&lt;h2 id="this-document-will-explain-how-to-validate-if-openapi-is-generated-correctly-and-server-start-and-stop-working-correctly">This document will explain how to validate if openapi is generated correctly and server start and stop working correctly&lt;/h2>
&lt;h3 id="we-have-create-a-framework-class-which-has-below-basic-test-case-functions">We have create a framework class which has below basic test case functions&lt;/h3>
&lt;p>Framework file is present under tests/lib named as generator_test.py&lt;/p>
&lt;h4 id="below-test-cases-are-related-to-generator-api">Below test cases are related to generator API&lt;/h4>
&lt;ol>
&lt;li>Create a build folder and copy py file into it. Build sub folder will created where test py file present.&lt;/li>
&lt;li>It will call generator generate function to generate Yaml file inside build folder&lt;/li>
&lt;li>It will check if generated YMAL file syntax is correct or not.&lt;/li>
&lt;li>It will check if number of function generated in YMAL is same as py file.&lt;/li>
&lt;li>Delete the build folder.&lt;/li>
&lt;/ol>
&lt;h4 id="two-test-cases-are-related-to-server-api">Two test cases are related to server API&lt;/h4>
&lt;ol>
&lt;li>It will start server&lt;/li>
&lt;li>It will stop server&lt;/li>
&lt;/ol>
&lt;h3 id="how-to-create-test-case">How to create test case&lt;/h3>
&lt;ol>
&lt;li>If you creating new Open API , then inside tests folder you have to commit your working py and yaml files.&lt;/li>
&lt;li>Create new function for test case where py and yaml located. Example (test_01_generator)&lt;/li>
&lt;li>We have already created test cases function file for generator-calculator name as test_01_generator.py. Please check this file.&lt;/li>
&lt;li>Copy the contains of test_01_generator.py and paste inside your test py file.&lt;/li>
&lt;li>Change startservercommand and filename variables value accordingly to your use case.&lt;/li>
&lt;li>Change some of parameters of constructor of GeneratorBaseTest class.&lt;/li>
&lt;li>if your py file has a class then.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash"> &lt;span style="color:#000">gen&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> GeneratorBaseTest&lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>filename,False,True&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>if your py file has functions then&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash"> &lt;span style="color:#000">gen&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> GeneratorBaseTest&lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>filename,True,False&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>First boolean flag in GeneratorBaseTest for &amp;ndash;all_functions and second flag is for &amp;ndash;import_class&lt;/li>
&lt;li>If you need to write more test cases based on your requirement, check order of test case and write accordingly.&lt;/li>
&lt;/ol>
&lt;h3 id="how-to-run-test-case">How to run test case&lt;/h3>
&lt;p>Below command can use to run your case. Make sure your current directory is cloudmesh-openapi.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ how &lt;span style="color:#204a87;font-weight:bold">do&lt;/span> you call this you can add -x to stop pytest when first &lt;span style="color:#204a87">test&lt;/span> failed
pytest -v --capture&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>no tests/generator-calculator/test_01_generator.py
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="run-test-case-with-csv-command-enabled">Run test case with CSV command enabled&lt;/h3>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ how &lt;span style="color:#204a87;font-weight:bold">do&lt;/span> you call this , you can add -x to stop pytest when first &lt;span style="color:#204a87">test&lt;/span> failed
pytest -v --capture&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>no tests/generator-calculator/test_01_generator.py &lt;span style="color:#000;font-weight:bold">|&lt;/span> fgrep &lt;span style="color:#4e9a06">&amp;#39;# cvs&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="below-are-test-case-files">Below are test case files&lt;/h2>
&lt;p>Generator-calculator and file name is test_01_generator.py&lt;/p>
&lt;pre>&lt;code>pytest -v --capture=no tests/generator-calculator/test_01_generator.py
&lt;/code>&lt;/pre>&lt;p>Generator-testclass and file name is test_02_generator.py&lt;/p>
&lt;pre>&lt;code>pytest -v --capture=no tests/generator-testclass/test_02_generator
&lt;/code>&lt;/pre>&lt;p>Server-cpu and file name is test_03_generator.py&lt;/p>
&lt;pre>&lt;code>pytest -v --capture=no tests/server-cpu/test_03_generator
&lt;/code>&lt;/pre>&lt;p>Server-cms and file name is test_04_generator.py&lt;/p>
&lt;pre>&lt;code>pytest -v --capture=no tests/server-cms/test_04_generator
&lt;/code>&lt;/pre>&lt;p>Generator and file name is test_05_generator.py&lt;/p>
&lt;pre>&lt;code>pytest -v --capture=no tests/generator/test_05_generator
&lt;/code>&lt;/pre>&lt;p>Azure AI Image Function is test_06_generator.py&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">pytest -v --capture&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>no tests/generator_azureai/test_06_generator
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Azure AI Text Function is test_07_generator.py&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">pytest -v --capture&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>no tests/generator_azureai/test_07_generator
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Natural Language Analysis Generator Tests are run from test_generator_natural_language.py&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">pytest -v --capture&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>no ./tests/test_generator_natural_language.py::TestGenerator
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This test will generate an OpenAPI spec for the natural-lang-analysis.py file located in the generator-natural-lang
directory. If the above command is copied and pasted to run in the terminal it will do the following.&lt;/p>
&lt;ol>
&lt;li>Generate a yaml file&lt;/li>
&lt;li>Verify the spec has all the functions that are available in the natural-lang-analysis.py file&lt;/li>
&lt;li>Start a server hosting the openAPI spec&lt;/li>
&lt;li>Run a call against the sentiment analysis and translation endpoint for each available cloud service (Google/Azure) and verify it was successful.&lt;/li>
&lt;li>Stop the service&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Results for Natural Language Tests&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attribute&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>cpu_count&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.active&lt;/td>
&lt;td>2.0 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.available&lt;/td>
&lt;td>2.1 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.free&lt;/td>
&lt;td>148.8 MiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.inactive&lt;/td>
&lt;td>2.0 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.percent&lt;/td>
&lt;td>73.2 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.total&lt;/td>
&lt;td>8.0 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.used&lt;/td>
&lt;td>4.8 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.wired&lt;/td>
&lt;td>2.8 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>platform.version&lt;/td>
&lt;td>10.14.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python&lt;/td>
&lt;td>3.8.1 (v3.8.1:1b293b6006, Dec 18 2019, 14:08:53)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>[Clang 6.0 (clang-600.0.57)]&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.pip&lt;/td>
&lt;td>20.0.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.version&lt;/td>
&lt;td>3.8.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sys.platform&lt;/td>
&lt;td>darwin&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.machine&lt;/td>
&lt;td>x86_64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.node&lt;/td>
&lt;td>Andrews-MacBook-Pro.local&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.processor&lt;/td>
&lt;td>i386&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.release&lt;/td>
&lt;td>18.2.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.system&lt;/td>
&lt;td>Darwin&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.version&lt;/td>
&lt;td>Darwin Kernel Version 18.2.0: Fri Oct 5 19:41:49 PDT 2018; root:xnu-4903.221.2~2/RELEASE_X86_64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>user&lt;/td>
&lt;td>andrewgoldfarb&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Status&lt;/th>
&lt;th>Time&lt;/th>
&lt;th>Start&lt;/th>
&lt;th>tag&lt;/th>
&lt;th>Node&lt;/th>
&lt;th>User&lt;/th>
&lt;th>OS&lt;/th>
&lt;th>Version&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>generator_test/copy_py_file&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.003&lt;/td>
&lt;td>2020-05-09 06:33:47&lt;/td>
&lt;td>openapi&lt;/td>
&lt;td>Andrews-MacBook-Pro.local&lt;/td>
&lt;td>andrewgoldfarb&lt;/td>
&lt;td>Darwin&lt;/td>
&lt;td>10.14.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>generator_test/generate&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>2.601&lt;/td>
&lt;td>2020-05-09 06:33:47&lt;/td>
&lt;td>openapi&lt;/td>
&lt;td>Andrews-MacBook-Pro.local&lt;/td>
&lt;td>andrewgoldfarb&lt;/td>
&lt;td>Darwin&lt;/td>
&lt;td>10.14.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>generator_test/read_spec&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.012&lt;/td>
&lt;td>2020-05-09 06:33:49&lt;/td>
&lt;td>openapi&lt;/td>
&lt;td>Andrews-MacBook-Pro.local&lt;/td>
&lt;td>andrewgoldfarb&lt;/td>
&lt;td>Darwin&lt;/td>
&lt;td>10.14.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>generator_test/start_service&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>1.864&lt;/td>
&lt;td>2020-05-09 06:33:49&lt;/td>
&lt;td>openapi&lt;/td>
&lt;td>Andrews-MacBook-Pro.local&lt;/td>
&lt;td>andrewgoldfarb&lt;/td>
&lt;td>Darwin&lt;/td>
&lt;td>10.14.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>test_generator_natural_language/test_run_analyze_google&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.67&lt;/td>
&lt;td>2020-05-09 06:33:51&lt;/td>
&lt;td>openapi&lt;/td>
&lt;td>Andrews-MacBook-Pro.local&lt;/td>
&lt;td>andrewgoldfarb&lt;/td>
&lt;td>Darwin&lt;/td>
&lt;td>10.14.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>test_generator_natural_language/test_run_analyze_azure&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.58&lt;/td>
&lt;td>2020-05-09 06:33:52&lt;/td>
&lt;td>openapi&lt;/td>
&lt;td>Andrews-MacBook-Pro.local&lt;/td>
&lt;td>andrewgoldfarb&lt;/td>
&lt;td>Darwin&lt;/td>
&lt;td>10.14.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>generator_test/stop_server&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>2.095&lt;/td>
&lt;td>2020-05-09 06:33:52&lt;/td>
&lt;td>openapi&lt;/td>
&lt;td>Andrews-MacBook-Pro.local&lt;/td>
&lt;td>andrewgoldfarb&lt;/td>
&lt;td>Darwin&lt;/td>
&lt;td>10.14.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>generator_test/delete_file&lt;/td>
&lt;td>ok&lt;/td>
&lt;td>0.002&lt;/td>
&lt;td>2020-05-09 06:33:54&lt;/td>
&lt;td>openapi&lt;/td>
&lt;td>Andrews-MacBook-Pro.local&lt;/td>
&lt;td>andrewgoldfarb&lt;/td>
&lt;td>Darwin&lt;/td>
&lt;td>10.14.1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="todo-describe-what-they-do">TODO DESCRIBE WHAT THEY DO&lt;/h2>
&lt;p>cache-scikitlearn
deprecated
examples
generator
generator-azureai
generator-calculator
generator-printerclass
generator-testclass
generator-upload
image-analysis
lib
Scikit-learntestfiles
Scikitlearn_tests
server-cms
server-cms-simple
server-cpu
server-sample
server-sampleFunction
test_mlperf
textanalysis-example-text
&lt;strong>init&lt;/strong>.py
README.md
test_001_registry.py
test_03_generator.py
test_010_generator.py
test_011_generator_cpu.py
test_012_generator_calculator.py
test_015_generator_azureai.py
test_020_server_manage.py
test_server_cms_cpu.py
util.py&lt;/p>
&lt;p>THIS WAS HERE BEFORE&lt;/p>
&lt;h2 id="test_001_registrypy">test_001_registry.py&lt;/h2>
&lt;p>This test has 5 test functions&lt;/p>
&lt;ol>
&lt;li>test_registry_add&lt;/li>
&lt;li>test_registry_list_name&lt;/li>
&lt;li>test_registry_list&lt;/li>
&lt;li>test_registry_delete&lt;/li>
&lt;li>test_benchmark&lt;/li>
&lt;/ol>
&lt;p>Test 1 calls registry and adds to the registry. If successful prints &amp;lsquo;PASSED&amp;rsquo;&lt;/p>
&lt;p>Test 2 calls registry and prints ONLY the server specified in filename.&lt;/p>
&lt;p>Test 3 calls registry and print list for ALL servers in registry.&lt;/p>
&lt;p>Test 4 calls registry and deletes entry for filename.&lt;/p>
&lt;p>Test 5 runs benchmark test on registry.&lt;/p>
&lt;h3 id="how-to-call-this">How to call this&lt;/h3>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">cms &lt;span style="color:#204a87">set&lt;/span> &lt;span style="color:#000">filename&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;./tests/server-cpu/cpu.yaml&amp;#34;&lt;/span>
pytest -v --capture&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>no tests/test_001_registry.py
&lt;/code>&lt;/pre>&lt;/div>&lt;p>deprecated
examples
generator
generator-calculator
generator-printerclass
generator-testclass
server-class
server-cms
server-cms-simple
server-cpu
server-sample
server-sampleFunction
textanalysis-example-text
&lt;strong>init&lt;/strong>.py
README.md
test_001_registry.py Falconi
test_03_generator.py jonthan
test_010_generator.py jonthan
test_011_generator_cpu.py prateek
test_012_generator_calculator.py prateek
test_020_server_manage.py ishan
test_server_cms_cpu.py andrew&amp;ndash;&amp;gt;&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/tests/server-cpu/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/tests/server-cpu/readme/</guid><description>
&lt;h1 id="test-it-yourself">Test it yourself&lt;/h1>
&lt;p>cd to &lt;code>cloudmesh-openapi&lt;/code>&lt;/p>
&lt;p>Start the service&lt;/p>
&lt;pre>&lt;code>cms openapii server start ./tests/server-cpu.yaml
&lt;/code>&lt;/pre>&lt;p>Stop the service&lt;/p>
&lt;pre>&lt;code>cms openapii3 server stop cpu
&lt;/code>&lt;/pre>&lt;p>urls&lt;/p>
&lt;p>cloudmesh/ui&lt;/p>
&lt;p>cloudmesh/cpu&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/tests/test_mlperf/readme-source/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/tests/test_mlperf/readme-source/</guid><description>
&lt;h1 id="mlperf-tests">MLPerf Tests&lt;/h1>
&lt;p>&lt;a href="http://mlperf.org">MLperf&lt;/a> [@www-mlperf] provides &amp;ldquo;fair and useful benchmarks for measuring
training and inference performance of ML hardware, software, and
services&amp;rdquo;&lt;/p>
&lt;p>In this benchmark we will&lt;/p>
&lt;ol>
&lt;li>Deploy MLPerf on the system&lt;/li>
&lt;li>Use functions that run a number of tests as inout to the OpenAPI Gnerator&lt;/li>
&lt;li>From these functions we run our OpenAPI generator to create a service
that allows to run the MLperf examples through a Web service with
http calls&lt;/li>
&lt;li>Test out the created functions by running selected example invocations&lt;/li>
&lt;li>Report the time it takes to run these examples&lt;/li>
&lt;li>Provide a Makefile or python script that allows us to conveniently
cun these tests&lt;/li>
&lt;/ol>
&lt;h2 id="deployment">Deployment&lt;/h2>
&lt;p>Describe how to deploy&lt;/p>
&lt;h3 id="reports-for-running-the-tests-on-machines">Reports for running the tests on Machines&lt;/h3>
&lt;p>Provide summary information about teh runtime
Provide details do checked in results in the &lt;a href="results">results&lt;/a> directory&lt;/p>
&lt;h3 id="local-output">Local Output&lt;/h3>
&lt;p>All output is written into a &lt;code>~/.cloudmesh/dest/benchmark/mlperf&lt;/code> folder
which can be removed after the test is completed. In the results folder
we also find a copy of the OpenAPI YAML file that is generated with the
cenerator. This file can also be used to compare the generated output.&lt;/p>
&lt;h2 id="selected-benchmarks">Selected Benchmarks&lt;/h2>
&lt;p>Describe which benchmarks were selected&lt;/p>
&lt;h2 id="functions">Functions&lt;/h2>
&lt;p>Short description aboutthe functions that have been defined&lt;/p>
&lt;h2 id="opeanapi">OpeanAPI&lt;/h2>
&lt;p>Describe where to find the generated functions
Link th=o wher ethe open api is created in the&lt;/p>
&lt;h2 id="how-to-run-individual-tests">How to run individual tests&lt;/h2>
&lt;p>Describe how to run indific=dual Tests&lt;/p>
&lt;h2 id="benchmarks">Benchmarks&lt;/h2>
&lt;p>Links to benchmarks that are listed in the &lt;a href="results">results&lt;/a> directory&lt;/p>
&lt;h2 id="references">References&lt;/h2></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/tests/test_mlperf/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/tests/test_mlperf/readme/</guid><description>
&lt;h1 id="mlperf-tests">MLPerf Tests&lt;/h1>
&lt;p>According to &lt;a href="https://mlperf.org/">https://mlperf.org/&lt;/a> MLPerf provides &amp;quot; Fair and useful
benchmarks for measuring training and inference performance of ML
hardware, software, and services&amp;quot; [@www-mlperf]&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/tests/test_mlperf/results/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/tests/test_mlperf/results/readme/</guid><description>
&lt;p>put your result files here&lt;/p></description></item><item><title>Report:</title><link>/report/cloudmesh-openapi/tests/timeseries-example/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/cloudmesh-openapi/tests/timeseries-example/readme/</guid><description>
&lt;h1 id="time-series-forecast-using-multi-cloud-ai-services">Time Series Forecast using Multi Cloud AI Services&lt;/h1>
&lt;p>Prafull Porwal, &lt;a href="https://github.com/cloudmesh-community/sp20-516-255/blob/main/Cloudmesh-OpenAPI/Readme.md">sp20-516-255&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cloudmesh-community/sp20-516-255/graphs/contributors">Contributors&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cloudmesh-community/fa19-516-147/pulse">Insights&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cloudmesh-community/sp20-516-255/tree/main/Cloudmesh-OpenAPI/AWSForecast">Project Code&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="objective">Objective&lt;/h2>
&lt;p>Develop Open API for time series forecasting on multiple clouds&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Many cloud providers have introduced machine learning capabilities on their infrastructure. The project aims to provide an open API for timeseries forecasting for AWS using Forecast Services and S3&lt;/p>
&lt;h3 id="aws-ai-service--forecast-open-api-service-features">AWS AI Service : Forecast Open API Service Features&lt;/h3>
&lt;ul>
&lt;li>Upload the data file to ./cloudmesh/upload-file location&lt;/li>
&lt;li>Upload the json schema file to ./cloudmesh/upload-file location&lt;/li>
&lt;li>Validate the data for missing and less than 0 values&lt;/li>
&lt;li>Split the dataset into Train and test by specifying split percentge.&lt;/li>
&lt;li>Provide list of Multi Cloud supported for Timeseries Forecasting&lt;/li>
&lt;li>Initialize the cloud service&lt;/li>
&lt;li>Create a Dataset Group&lt;/li>
&lt;li>Create a Target Time Series Dataset&lt;/li>
&lt;li>Import data into Forecast from AWS Storage S3&lt;/li>
&lt;li>Create a Predictor&lt;/li>
&lt;li>Generate Forecast&lt;/li>
&lt;li>Query the Forecast&lt;/li>
&lt;/ul>
&lt;h3 id="additional-features">Additional Features&lt;/h3>
&lt;ul>
&lt;li>Multiple instance of the process supported&lt;/li>
&lt;li>Data Validation and missing values checks&lt;/li>
&lt;/ul>
&lt;h3 id="environment-configuration">Environment Configuration&lt;/h3>
&lt;ul>
&lt;li>Python 3.8.2 Python or newer.&lt;/li>
&lt;li>Use a venv (see developer install)&lt;/li>
&lt;li>MongoDB installed as regular program not as service&lt;/li>
&lt;li>AWS boto3 library&lt;/li>
&lt;li>Open API package installed&lt;/li>
&lt;/ul>
&lt;p>Make sure that cloudmesh is properly installed on your machine and you have mongodb setup to work with cloudmesh.
More details can be found in the &lt;a href="https://cloudmesh.github.io/cloudmesh-manual/installation/install.html">Cloudmesh Manual&lt;/a>&lt;/p>
&lt;h3 id="openapi-package-installation">OpenAPI package installation&lt;/h3>
&lt;p>Make sure you use a python venv before installing. Users can install the code with&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ pip install cloudmesh-openapi
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="pre-requisites-">Pre Requisites :&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>add below parameter to cloudmesh.yaml for forecast service to work&lt;/p>
&lt;ul>
&lt;li>bucket_name : awsforecastassignnment&lt;/li>
&lt;li>region_name : us-east-1&lt;/li>
&lt;li>forecast_srv : forecast&lt;/li>
&lt;li>forecastquery_srv : forecastquery&lt;/li>
&lt;li>s3_srv : s3&lt;/li>
&lt;li>iam_role_arn: XXXXXX&lt;/li>
&lt;li>algorithmArn: arn:aws:forecast:::algorithm/Deep_AR_Plus&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Data Format : The data should be in csv file format and must have&lt;/p>
&lt;ul>
&lt;li>item_id : reference column for which time series forecast is required&lt;/li>
&lt;li>target_value : the column which need to be predicted, data type integer&lt;/li>
&lt;li>timestamp : timestamp of data samples&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://docs.aws.amazon.com/forecast/latest/dg/API_CreateDataset.html">AWS Time Series Forecast&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Json Schema : Json Schema file with name schema.json&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="quick-forecast-api-reference-commands">Quick Forecast API reference Commands&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Start the open API server for the forecast service&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">cms openapi server start .//forecast.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Check for supported AI services&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl http://localhost:8080/cloudmesh/forecast
&lt;/code>&lt;/pre>&lt;/div>&lt;p>e.g. output:
&amp;ldquo;model&amp;rdquo;: &amp;ldquo;Supported Time Series Forecast Services AWS : Forecast &amp;quot;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Upload file to the server from location (
File path should be the location on server where file is located.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl &lt;span style="color:#4e9a06">&amp;#34;http://localhost:8080/cloudmesh/forecast/upload&amp;#34;&lt;/span> -F &lt;span style="color:#4e9a06">&amp;#34;upload=@&amp;lt;file_path&amp;gt;\countries-aggregated.csv&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>e.g. output:
countries-aggregated.csv uploaded successfully&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Validate data file&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl &lt;span style="color:#4e9a06">&amp;#34;http://localhost:8080/cloudmesh/forecast/validate_data&amp;#34;&lt;/span> -F &lt;span style="color:#4e9a06">&amp;#34;upload=@&amp;lt;file_path&amp;gt;\countries-aggregated.csv&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>e.g. output:
countries-aggregated.csv validated successfully&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Split the data into test and train. Data should be validated first before splitting&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl http://localhost:8080/cloudmesh/forecast/split_data?split_pct&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">20&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>output: &amp;ldquo;Please validate the data first&amp;rdquo;&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl http://localhost:8080/cloudmesh/forecast/split_data?split_pct&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">20&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>output: &amp;ldquo;Data split successfully&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Initialize aws parameters&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl &lt;span style="color:#4e9a06">&amp;#34;http://localhost:8080/cloudmesh/forecast/aws&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>e.g. output:
{&amp;ldquo;model&amp;rdquo;:&amp;ldquo;AWS AI Service initialized successfully&amp;rdquo;}&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create Forecast, this is a multistep process, it cretes datasetgroup, dataset, import job, predictor and forecast&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl http://localhost:8080/cloudmesh/forecast/create_forecast?country&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>Austrailia
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This api expects cloud services to be already initialized if not it will request to initialize
output:&lt;/p>
&lt;p>&amp;ldquo;Please initialize cloud service&amp;rdquo;&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>output: &amp;ldquo;Forecast generated successfully&amp;rdquo;&lt;/p>
&lt;ul>
&lt;li>Lookup a Forecast&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl http://localhost:8080/cloudmesh/forecast/lookupForecast?countryName&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>Austrailia
&lt;/code>&lt;/pre>&lt;/div>&lt;p>output :
shows &lt;a href="https://github.com/cloudmesh-community/sp20-516-255/blob/main/Cloudmesh-OpenAPI/AWSForecast/sampleOutput">ouput&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Delete Data Stack for the current project
This API should be executed at the end of the session to delete all the resources created for the analysis&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl &lt;span style="color:#4e9a06">&amp;#34;http://localhost:8080/cloudmesh/forecast/deletestack&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="algorithm-details">Algorithm details&lt;/h2>
&lt;p>The AWS Forecast service supports following pre-defined algortithms&lt;/p>
&lt;ul>
&lt;li>Autoregressive Integrated Moving Average (ARIMA) Algorithm - arn:aws:forecast:::algorithm/ARIMA&lt;/li>
&lt;li>DeepAR+ Algorithm - arn:aws:forecast:::algorithm/Deep_AR_Plus&lt;/li>
&lt;li>Exponential Smoothing (ETS) - arn:aws:forecast:::algorithm/ETS&lt;/li>
&lt;li>Non-Parametric Time Series (NPTS) Algorithm - arn:aws:forecast:::algorithm/NPTS&lt;/li>
&lt;li>Prophet Algorithm - arn:aws:forecast:::algorithm/Prophet&lt;/li>
&lt;li>Supports hyperparameter optimization (HPO)&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf">AWS Time Series Forecast&lt;/a>&lt;/p>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;ul>
&lt;li>Requires data file with mandatory colums item_id, target_value and timestamp&lt;/li>
&lt;li>Requires a schema file schema.json to be provided by the user&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2>
&lt;p>&lt;a href="https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf">https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/aws-samples/amazon-forecast-samples">https://github.com/aws-samples/amazon-forecast-samples&lt;/a>&lt;/p></description></item><item><title>Report:</title><link>/report/fa20-523-301/assignment6/assignment6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-301/assignment6/assignment6/</guid><description>
&lt;h1 id="assignment-6">Assignment 6&lt;/h1>
&lt;h1 id="health-and-medicine--artificial-intelligence-influence-on-ischemic-stroke-imaging">Health and Medicine â€“ Artificial Intelligence Influence on Ischemic Stroke Imaging&lt;/h1>
&lt;ul>
&lt;li>Gavin Hemmerlein, fa20-523-301&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/blob/master/assignment6/assignment6.md">Edit&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Keywords:&lt;/strong> convolutional neural network, random forest learning, Computer Tomography Scan, CT Scan, stroke, artificial intelligence, deep learning, machine learning, large vessel occlusions&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The Computer Tomography Scan (CT Scan) is a medical procedure that involves multiple x-rays analyzed using computer aided techniques. The CT Scanâ€™s creation was credited to Allan M. Cormack and Godfrey N. Hounsfield for which both individuals were awarded the 1979 Nobel Prize in Physiology or Medicine &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The OECD estimates that there are a total of 42.64 million scanners located in the United States; the fourth most of any country &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This prevalence is extremely important when discussing a diagnosis for stroke victims. In the 1980s, the identification techniques were generally done through a process called computer-aided diagnosis (CAD). â€œCAD usually relies on a combination of interpretation of medical images through computational algorithms and the physiciansâ€™ evaluation of the medical images &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>â€.&lt;/p>
&lt;p>The goal of researchers and medical practitioners is to improve upon detection rates to ensure that more lives are saved by early detection. According to Johns Hopkins Medical Department the faster medical precautions can be given to a victim, the better the prognosis is for the individual &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The brain requires a constant supply of blood and oxygen. When it is starved of these nutrients, the brain tissue begins to die.&lt;/p>
&lt;h2 id="2-assisting-researchers-with-artificial-intelligence">2. Assisting Researchers with Artificial Intelligence&lt;/h2>
&lt;p>According to an article in Radiology Business, automated detection of stroke anomalies is improving. As stated in a review in the article, â€œthe team found convolutional neural networks beat random forest learning (RFL) on sensitivity, 85% to 68% &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.â€ This improvement is an excellent improvement by switching the algorithm that is used to train the model. A convolutional neural network (CNN) is a deep learning technique while a random forest is a modified decision tree. By modifying approaches from a decision tree to a deep learning technique, there is a very high likelihood that more lives could be saved. Strokes account for nearly 140,000 deaths a year and are one of the leading causes of permanent disability in the United States &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>A RFL algorithm is a form of decision tree supervised learning. Decision trees are unique because they can also be used to solve regression and classification problems; which is unique to supervised learning methods. The RFL uses many decision trees that build upon one another. Where the CNN algorithm differs is that it is a form of deep learning that performs unsupervised learning. Each layer in the CNN understands its inputs and outputs while passing the output on to the next layer. A CNN can pass this information forward through a number of layers, but there is also a diminishing return given the amount of processing needed for each layer.&lt;/p>
&lt;p>After reviewing the literature from the Radiology Business article, the most common avenue for early detection appears to be the RFL as stated above. A meta analysis reviewing PubMed articles from January 2014 to February 2019 found that the RFL was the highest performer for predictive measures &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. For large vessel occlusions (LVO), the best approach was to use a CNN. CNNâ€™s use little pre-processing and rely moreso on the filters with the data. This results in a more dynamic approach to the data as opposed to the harder developed structure of a decision tree.&lt;/p>
&lt;h2 id="3-future-work">3. Future Work&lt;/h2>
&lt;p>Upon examining the cited sources, there are some future areas to look research. To improve on current understanding, a standardization of metrics for to evaluate the fidelity of the models &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>, continued development of automative image analysis software &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>, and leveraging emerging techniques to develop even more effective algorithms to detect large vessel occlusion &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. As of 2019, the advantage of CNNâ€™s over conventional detection methods was only 7.6% &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. The percentage may seem marginal, but when expanded out to the 140,000 strokes per year the amount of strokes identified could be as much as 10,000 individuals.&lt;/p>
&lt;p>These areas are only a few of the many improvements that could be made in the world of stroke detection. It is not a far stretch to imagine detecting vessels that are becoming clogged or brittle. If detection of these medical issues could become prevalent, even more lives could be saved by predicting strokes before they even occur.&lt;/p>
&lt;h2 id="4-references">4. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Nobel Prizes &amp;amp; Laureates, &amp;ldquo;The Nobel Prize in Physiology or Medicine 1979,&amp;rdquo; &lt;em>The Nobel Prize,&lt;/em> [Online]. Available:
&lt;a href="https://www.nobelprize.org/prizes/medicine/1979/summary/">https://www.nobelprize.org/prizes/medicine/1979/summary/&lt;/a> [Accessed Oct. 16, 2020]. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>OECD, &amp;ldquo;Computed tomography (CT) scanners,&amp;rdquo; &lt;em>OECD Data,&lt;/em> [Online]. Available:
&lt;a href="https://data.oecd.org/healtheqt/computed-tomography-ct-scanners.htm">https://data.oecd.org/healtheqt/computed-tomography-ct-scanners.htm&lt;/a> [Accessed Oct. 16, 2020]. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Y. Mokli, J. Pfaff, D. Pinto dos Santos, C. Herweh, and S. Nagel &amp;ldquo;Computer-aided imaging analysis in acute ischemic stroke â€“ background and clinical applications&amp;rdquo;, &lt;em>Neurological Research and Practice&lt;/em>, p. 1-13. 2020 [Online serial]. Available: &lt;a href="https://neurolrespract.biomedcentral.com/track/pdf/10.1186/s42466-019-0028-y">https://neurolrespract.biomedcentral.com/track/pdf/10.1186/s42466-019-0028-y&lt;/a> [Accessed Oct. 13, 2020]. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>A. Pruski, â€œStroke Recovery Timeline,â€ &lt;em>John Hopkins Medical,&lt;/em> [Online]. Available: &lt;a href="https://www.hopkinsmedicine.org/health/conditions-and-diseases/stroke/stroke-recovery-timeline">https://www.hopkinsmedicine.org/health/conditions-and-diseases/stroke/stroke-recovery-timeline&lt;/a> [Accessed Oct. 16, 2020]. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>D. Pearson, &amp;ldquo;AI helps bust stroke, identify occlusions,&amp;rdquo; &lt;em>Radiology Business,&lt;/em> [Online]. Available:
&lt;a href="https://www.radiologybusiness.com/topics/artificial-intelligence/ai-helps-bust-stroke-identify-occlusions">https://www.radiologybusiness.com/topics/artificial-intelligence/ai-helps-bust-stroke-identify-occlusions&lt;/a> [Accessed Oct. 13, 2020]. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>The Internet Stroke Center, &amp;ldquo;About Strokes,&amp;rdquo; &lt;em>Stroke Statistics,&lt;/em> [Online]. Available:
&lt;a href="http://www.strokecenter.org/patients/about-stroke/stroke-statistics/#:~:text=More%20than%20140%2C000%20people%20die,and%20185%2C000%20are%20recurrent%20attacks">http://www.strokecenter.org/patients/about-stroke/stroke-statistics/#:~:text=More%20than%20140%2C000%20people%20die,and%20185%2C000%20are%20recurrent%20attacks&lt;/a> [Accessed Oct. 16, 2020]. &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>N. Murray, &amp;ldquo;Artificial intelligence to diagnose ischemic stroke and identify large vessel occlusions: a systematic review,&amp;rdquo; &lt;em>Journal of NeuroInterventional Surgery&lt;/em>, vol. 12, no. 2, p. 156-164. 2020 [Online serial]. Available: &lt;a href="https://jnis.bmj.com/content/12/2/156">https://jnis.bmj.com/content/12/2/156&lt;/a> [Accessed Oct. 13, 2020]. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>M. Stib, J. Vasquez, M. Dong, Y. Kim, S. Subzwari, H. Triedman, A. Wang, H. Wang, A. Yao, M. Jayaraman, J. Boxerman, C. Eickhoff, U. Cetintemel, G. Baird, and R. McTaggart, &amp;ldquo;Detecting Large Vessel Occlusion at Multiphase CT Angiography by Using a Deep Convolutional Neural Network&amp;rdquo;, &lt;em>Original Research Neuroradiology&lt;/em>, Sep 29, 2020. [Online serial]. Available: &lt;a href="https://pubs.rsna.org/doi/full/10.1148/radiol.2020200334">https://pubs.rsna.org/doi/full/10.1148/radiol.2020200334&lt;/a> [Accessed Oct. 13, 2020]. &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>J. Tuan, &amp;ldquo;How AI is able to Predict and Detect a Stroke&amp;rdquo;, &lt;em>Referral MD&lt;/em>. [Online]. Available: &lt;a href="https://getreferralmd.com/2019/10/how-ai-is-able-to-predict-and-detect-a-stroke/">https://getreferralmd.com/2019/10/how-ai-is-able-to-predict-and-detect-a-stroke/&lt;/a> [Accessed Oct. 13, 2020]. &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-301/project/misc_files/blank/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-301/project/misc_files/blank/</guid><description>
&lt;h1 id="blank">Blank&lt;/h1></description></item><item><title>Report:</title><link>/report/fa20-523-301/project/plan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-301/project/plan/</guid><description>
&lt;p>&lt;strong>EE 534: BIG DATA&lt;/strong>&lt;/p>
&lt;p>&lt;strong>CHELSEA GORIUS&lt;/strong>&lt;/p>
&lt;p>&lt;strong>GAVIN HEMMERLEIN&lt;/strong>&lt;/p>
&lt;p>&lt;strong>CLASS 11530&lt;/strong>&lt;/p>
&lt;p>&lt;strong>FALL 2020&lt;/strong>&lt;/p>
&lt;p>&lt;strong>NBA PERFORMANCE AND INJURY&lt;/strong>&lt;/p>
&lt;h2 id="1-team">1. Team&lt;/h2>
&lt;p>Our team will consist of Chelsea Gorius (cgorius - fa20-523-344) and Gavin Hemmerlein (ghemmer - fa20-523-301). Both members are students in the ENGR E534 course. Chelsea and Gavin are also IU Masters students pursuing a degree in Data Science.&lt;/p>
&lt;h2 id="2-topic">2. Topic&lt;/h2>
&lt;p>The topic to be investigated is basketball player performance as it relates to injury. The topic of injury and recovery is a multi-billion dollar industry. The Sports Medicine field is expected to reach $7.2 billion dollars by 2025 &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The scope of this effort is to explore National Basketball Association(NBA) teams, but the additional uses of a topic such as this could expand into other realms such as the National Football League, Major League Baseball, the Olympic Committees, and many other avenues. For leagues with salaries, projecting an expected return on the investment can assist in contract negotiations and cater expectations.&lt;/p>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>To compare performance and injury, a minimum of two datasets will be needed. The first is a dataset of injuries for players &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This dataset will create the samples necessary for review.&lt;/p>
&lt;p>Once the controls for injuries are established, the next requirement will be to establish pre-injury performance parameters and post-injury parameters. These areas will be where the feature engineering will take place. The datasets needed must dive into appropriate basketball performance stats to establish a metric to encompass a playerâ€™s performance. One example that ESPN has tried in the past is the Player Efficiency Rating (PER). To accomplish this, it will be important to review player performance within games such as in the â€œNBA games dataâ€ &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> dataset. There is a potential to pull more data from other datasets such as the â€œNBA Enhanced Box Score and Standings (2012 - 2018)â€ &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. It is important to use the in depth data from the â€œNBA games dataâ€ &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> dataset because of how it will allow us to see how the player was performing throughout the season, and not just their average stats across the year. With in depth information about each game of the season, and not just the teams and players aggregated stats, added to the data provided from the injury dataset &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> we will be able to compose new metrics to understand how these injuries are actually affecting the players performance.&lt;/p>
&lt;p>Along the way we look forward to discovering if there is also a causal relationship to the severity of some of the injuries, based on how the player was performing just before the injury. The term â€œload managementâ€ has become popular in recent years to describe players taking rest periodically throughout the season in order to prevent injury from overplaying. This new practice has received both support for the player safety it provides and also criticism around players taking too much time off. Of course not all injuries are entirely based on the recent strain under the players body, but a better understanding about how that affects the injury as a whole could give better insight into avoiding more injuries. It is important to remember though that any pattern identification would not lead to an elimination of all injuries, any contact sport will continue to have injuries, especially one as high impact as the NBA. There is value to learn from why some players are able to return from certain injuries more quickly and why some return to almost equivalent or better playing performance than before the injury. This comparison of performance will be made by deriving metrics based on varying ranges of games immediately leading up to injury and then immediately after returning from injury. In addition to that we will perform comparisons to the players known peak performance to better understand how the injury affected them. Another factor it will be important to include is the length of time recovering from the injury. Different players take differing amounts of time off, sometimes even with similar injuries. Something will be said about the playerâ€™s dedication to recovery and determination to remain at peak performance, even through injury, when looking at how severe their injury was, how much time was taken for recovery, and how they performed upon returning.&lt;/p>
&lt;p>These datasets were chosen because they allow for a review of individual game performance, for each team, throughout each season in the recent decade. Aggregate statistics such as points per game (ppg) can be deceptive because duration of the metric is such a large period of time. The large sample of 82 games can lead to a perception issue when reviewing the data. These datasets include more variables to help us determine effects to player injury, such as minutes per game (mpg) to understand how strenuous the pre-injury performance or how fatigue may have played a factor in the injury. Understanding more of the variables such as fouls given or drawn can help determine if the player or other team seemed to be the primary aggressor before any injury.&lt;/p>
&lt;h2 id="4-objective">4. Objective&lt;/h2>
&lt;p>The objective of this project is to develop performance indicators for injured players returning to basketball in the NBA. It is unreasonable to expect a player to return to the same level of play post injury immediately upon starting back up after recovery. It often takes a player months if not years to return to the same level of play as pre-injury, especially considering the severity of the injuries. In order to successfully analyse this information from the datasets, a predictive model will need to be created using a large set of the data to train.&lt;/p>
&lt;p>From this point, a test run will be used to gauge the validity and accuracy of the model compared to some of the data set aside. The model created will be able to provide feature importance to give a better understanding of which specific features are the most crucial when it comes to determining how bad the effects of an injury may or may not be on player performance. Feature engineering will be performed prior to training the model in order to improve the chances of higher accuracy from the predictions. This model could be used to keep an eye out for how a player&amp;rsquo;s performance intensity and the engineered features could affect how long a player takes to recover from injury, if there are any warning signs prior to an injury, and even how well they perform when returning.&lt;/p>
&lt;h2 id="sources">Sources&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>A. Mehra, &lt;em>Sports Medicine Market worth $7.2 billion by 2025&lt;/em>, Markets and Markets.
&lt;a href="https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp">https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>R. Hopkins, &lt;em>NBA Injuries from 2010-2020&lt;/em>, Kaggle. &lt;a href="https://www.kaggle.com/ghopkins/nba-injuries-2010-2018">https://www.kaggle.com/ghopkins/nba-injuries-2010-2018&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>N. Lauga, &lt;em>NBA games data&lt;/em>, Kaggle. &lt;a href="https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv">https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>P. Rossotti, &lt;em>NBA Enhanced Box Score and Standings (2012 - 2018)&lt;/em>, Kaggle. &lt;a href="https://www.kaggle.com/pablote/nba-enhanced-stats">https://www.kaggle.com/pablote/nba-enhanced-stats&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-301/project/project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-301/project/project/</guid><description>
&lt;h1 id="nba-performance-and-injury">NBA Performance and Injury&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;ul>
&lt;li>Gavin Hemmerlein, fa20-523-301&lt;/li>
&lt;li>Chelsea Gorius, fa20-523-344&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/blob/main/project/project.md">Edit&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Sports Medicine will be a $7.2 billion dollar industry by 2025. The NBA has a vested interest in predicting performance of players as they return from injury. The authors evaluated datasets available to the public within the 2010 decade to build machine and deep learning models to expect results. The team utilized Gradient Based Regressor, Light GBM, and Keras Deep Learning models. The results showed that the coefficient of determination for the deep learning model was approximately 98.5%. The team recommends future work to predicting individual player performance utilizing the Keras model.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-dataset">3. Dataset&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-data-transformations-and-calculations">3.1 Data Transformations and Calculations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-development-of-models">4.1 Development of Models&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#411-evaluation-metrics">4.1.1 Evaluation Metrics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#412-gradient-boost-regression">4.1.2 Gradient Boost Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#412-lightgbm-regression">4.1.2 LightGBM Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#413-keras-deep-learning-models">4.1.3 Keras Deep Learning Models&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-inference">5. Inference&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#61-limitations">6.1 Limitations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#71-work-breakdown">7.1 Work Breakdown&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> basketball, NBA, injury, performance, salary, rehabilitation, artificial intelligence, convolutional neural network, lightGBM, deep learning, gradient based regressor.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The topic to be investigated is basketball player performance as it relates to injury. The topic of injury and recovery is a multi-billion dollar industry. The Sports Medicine field is expected to reach $7.2 billion dollars by 2025 &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The scope of this effort is to explore National Basketball Association(NBA) teams, but the additional uses of a topic such as this could expand into other realms such as the National Football League, Major League Baseball, the Olympic Committees, and many other avenues. For leagues with salaries, projecting an expected return on the investment can assist in contract negotiations and cater expectations. Competing at such a high level of intensity puts these players at a greater risk to injury than the average athlete because of the intense and constant strain on their bodies. The overall valuation of the NBA in recent years is over 2 billion dollars, meaning each team is spending millions of dollars in the pursuit of a championship every season. Injuries to players can cost teams not only wins but also significant profits. Ticket sales alone for a single NBA finals game have reported greater than 10 million dollars in profit for the home team, if a team&amp;rsquo;s star player gets injured just before the playoffs and the team does not succeed, that is a lot of money lost. These injuries can have an effect no matter the time of year, regular season ticket sales have been known to fluctuate with injuries from the team&amp;rsquo;s top performers. Besides ticket sales these injuries can also influence viewership, TV or streaming, and potentially lead to a greater loss in profits. With the health of the players and so much money at stake NBA team organizations as a whole do their best to take care of their players and keep them injury free.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>The assumptions were made based on current literature as well. The injury return and limitations upon return of Anterior Cruciate Ligament (ACL) rupture (ACLR) are well documented and known. Interesting enough, forty percent of the players in the study occurred during the fourth quarter &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This leads some credence to the idea that fatigue is a major factor in the occurrence of these injuries.&lt;/p>
&lt;p>The current literature also shows that a second or third injury can occur more frequently due to minor injuries. &lt;em>&amp;ldquo;When an athlete is recovering from an injury or surgery, tissue is already compromised and thus requires far more attention despite the recovery of joint motion and strength. Moreover, injuries and surgical procedures can create detraining issues that increase the likelihood of further injury&amp;rdquo;&lt;/em> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>To compare performance and injury, a minimum of two datasets will be needed. The first is a dataset of injuries for players &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This dataset created the samples necessary for review.&lt;/p>
&lt;p>Once the controls for injuries were established, the next requirement was to establish pre-injury performance parameters and post-injury parameters. These areas were where the feature engineering took place. The datasets needed had to include appropriate basketball performance stats to establish a metric to encompass a player&amp;rsquo;s performance. One example that ESPN has tried in the past is the Player Efficiency Rating (PER). To accomplish this, it was important to review player performance within games such as in the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> dataset because of how it allowed the team to evaluate the player performance throughout the season, and not just the average stats across the year. In addition to that the data from the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> dataset was valuable in order to compare the calculated performance metrics just before an injury or after recovery to the player&amp;rsquo;s overall performance that season or in seasons prior. That comparison provided a solid baseline to understand how injuries can effect a player&amp;rsquo;s performance. With in depth information about each game of the season, and not just the teams and players aggregated stats, added to the data provided from the injury dataset &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> the team was be able to compose new metrics to understand how these injuries are actually affecting the players performance.&lt;/p>
&lt;p>Along the way attempted to discover if there is also a causal relationship to the severity of some of the injuries, based on how the player was performing just before the injury. The term &lt;em>load management&lt;/em> has become popular in recent years to describe players taking rest periodically throughout the season in order to prevent injury from overplaying. This new practice has received both support for the player safety it provides and also criticism around players taking too much time off. Of course not all injuries are entirely based on the recent strain under the players body, but a better understanding about how that affects the injury as a whole could give better insight into avoiding more injuries. It is important to remember though that any pattern identification would not lead to an elimination of all injuries, any contact sport will continue to have injuries, especially one as high impact as the NBA. There is value to learn from why some players are able to return from certain injuries more quickly and why some return to almost equivalent or better playing performance than before the injury. This comparison of performance was attempted by deriving metrics based on varying ranges of games immediately leading up to injury and then immediately after returning from injury. In addition to that performed comparisons to the players known peak performance to better understand how the injury affected them. Another factor that was important to include is the length of time recovering from the injury. Different players take differing amounts of time off, sometimes even with similar injuries. Something will be said about the playerâ€™s dedication to recovery and determination to remain at peak performance, even through injury, when looking at how severe their injury was, how much time was taken for recovery, and how they performed upon returning.&lt;/p>
&lt;p>These datasets were chosen because they allow for a review of individual game performance, for each team, throughout each season in the recent decade. Aggregate statistics such as points per game (ppg) can be deceptive because duration of the metric is such a large period of time. The large sample of 82 games can lead to a perception issue when reviewing the data. These datasets include more variables to help the team determine effects to player injury, such as minutes per game (mpg) to understand how strenuous the pre-injury performance or how fatigue may have played a factor in the injury. Understanding more of the variables such as fouls given or drawn can help determine if the player or other team seemed to be the primary aggressor before any injury.&lt;/p>
&lt;h3 id="31-data-transformations-and-calculations">3.1 Data Transformations and Calculations&lt;/h3>
&lt;p>Using the Kaggle package the datasets were downloaded direct from the website and unzipped to a directory accessible by the â€˜project_dateEngineering.ipynbâ€™ notebook. The 7 unzipped datasets are then loaded into the notebook as pandas data frames using the â€˜.read_csv()â€™ function. The data engineering performed in the notebook includes removal of excess data and data type transformations across almost all the data frames loaded. This data transformation includes transforming the games details column â€˜MINâ€™, meaning minutes played, from a timestamp format to a numerical format that could have calculations like summation or average performed on it. This was a crucial transformation since minutes played have a direct correlation to player fatigue, which can increase a playerâ€™s chance of injury.&lt;/p>
&lt;p>One of the more difficult tasks was transforming the Injury dataset into something that would provide more information through machine learning and analysis. The dataset is loaded as one data set where 2 columns â€˜Relinquishedâ€™ and â€˜Acquiredâ€™ defined if the row in questions was a player leaving the roster due to injury or returning from injury, respectively. In this case for each for one of those two columns contained a players name and the other was blank. Besides that the data frame contained information like the date, notes, and the team name. In order to appropriately understand each injury as whole the data frame needs to be transformed into one where each row contains the player, the start date of the injury, and the end date of the injury. In order to do this first the original Injury dataset was separated into rows marking the start of an injury and those marking the end of an injury. Data frames from the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> data set were used to join TeamID and PlayerID columns to the Injury datasets. An â€˜iterrows():â€™ loop was then used on the data frame marking the start of an injury to specifically locate the corresponding row in the Injury End data frame with the same PlayerID and where the return date was the closest date after the injury date. As this new data frame was being transformed, it was noted that sometimes a Player would have multiple rows with the same Injury ending date but different injury start dates, this can happen if an injury worsens or the player did not play due to last minute decision. In order to solve this the table was grouped by the PlayerID and InjuryEnd Date while keeping the oldest Injury Start date, since the model will want to see the full length of the injury. From there it was simple to calculate the difference in days for each row between the Injury start and end dates. This data frame is called â€˜df_Injury_lengthâ€™ in the notebook and is much easier to use for improved understanding of NBA injuries than the original format of the Injury data set.&lt;/p>
&lt;p>Once created, the â€˜df_Injury_lengthâ€™ data frame was copied and built upon. Using â€˜iterrows():â€™ loop again to filter down the games details data frame rows with the same PlayerId, over 60 calculated columns are created to produce the â€˜df_Injury_statsâ€™ data frame. The data frame includes performance statistics specifically from the game the player was injured and the game the player returned from that injury. In addition to this aggregate performance metrics were calculated based on the 5 games prior to the injury and the 5 games post returning from injury. At this time the season of when the injury occurred and when the player returned is also stored in the dataframe. This will allow comparisons between the â€˜df_Injury_statsâ€™ data frame and the â€˜df_Season_statsâ€™ data frame which contains the players average performance metrics for entire seasons.&lt;/p>
&lt;p>A few interesting figures were generated within the Exploratory Data Analysis (EDA) stage. &lt;strong>Figure 1&lt;/strong> gave a view of the load of the player returning from injury. The load to the player will show how recovered the player is upon completion of rehab. Many teams decide to slowly work a returning player in. Additionally, the amount of time for an injury can be seen on this graph. The longer the injury, the more unlikely the player will return to action.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/avg_min_played_post5.png" alt="Average Minutes Played in First Five Games Upon Return over Injury Length in Days">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Average Minutes Played in First Five Games Upon Return over Injury Length in Days*&lt;/p>
&lt;p>&lt;strong>Figure 2&lt;/strong> shows the frequency in which a player is injured. The idea behind this graph is to see a relationship between the time leading up to the injury. Interesting enough, there is no key indication of where injury is more likely to occur. It can be assumed that there is a rarity of players who see playing time greater than 30 minutes. The histogram only shows a near flat relationship; which was surprising.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/frequencies_by_average_minutes.png" alt="Frequency of Injuries by Average Minutes Played in Prior Five Games">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Frequency of Injuries by Average Minutes Played in Prior Five Games*&lt;/p>
&lt;p>&lt;strong>Figure 3&lt;/strong> shows the length of injury over number of injuries. By reviewing this data, it can be seen that most injuries occur fewer rather than more often. A player that is deemed injury prone will be a lot more likely to be cut from the team. This data makes sense.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injury_length.png" alt="Injury Length in Days over Number of Injuries">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Injury Length in Days over Number of Injuries&lt;/p>
&lt;p>&lt;strong>Figure 4&lt;/strong> shows the injury length over average minutes played in the five games before injury. This graph attempts to show all of the previous games and the impacts to the players injury. The data looks evenly distributed, but the majority of plaers do not play close to 40 minutes per game. By looking at this data, it shows that minutes played does likely contribute to the injury severity.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injury_length_over_avg_min.png" alt="Injury Length in Days over Avg Minutes Played in Prior 5 Games">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Injury Length in Days over Avg Minutes Played in Prior 5 Games&lt;/p>
&lt;p>&lt;strong>Figure 5&lt;/strong> shows that in general the number of games played does not have a significant relationship to the length of the injury. There is a darker cluster between 500-1000 days injured that exists over the 40-82 games played, this could suggest that as more games are played there is likeliness for more severe injury.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_gamesplayed.png" alt="Injury Length in Days over Player Games Played that Season">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Injury Length in Days over Player Games Played that Season&lt;/p>
&lt;p>&lt;strong>Figures 6&lt;/strong>, &lt;strong>Figure 7&lt;/strong>, and &lt;strong>Figure 8&lt;/strong> attempt to demonstrate if any relationship exists visually between a player&amp;rsquo;s injury length and their age, weight, or height. For the most part &lt;strong>Figure 6&lt;/strong> shows most severe injuries occurring to younger players, which could make sense considering they can perform more difficult moves or have more stamina than older players. Some severe injuries still exist among the older players, this also makes sense considering their bodies have been under stress for many years and are more prone to injury. It should be noted that there are more players in the league that fall into the younger age bucket than the older ages. It is difficult to identify any pattern on &lt;strong>Figure 7&lt;/strong>. If anything the graph is somewhat normally shaped similar to the heights of players across the league. Suprisingly the injuries on &lt;strong>Figure 8&lt;/strong> are clustered a bit towards the left, being the lighter players. This could be explained through the fact that the lighter players are often more athletic and perform more strenuous moves than heavier players. It is also somewhat surprising since the argument that heavier players are putting more strain on their bodies could be used as a reason why heavier players would have worse injuries. One possible explanation could be the musculature adding more of the dense body mass could add protection to weakened joints. More investigation would be needed to identify an exact reason.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerage.png" alt="Injury Length in Days over Player Age that Season">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Injury Length in Days over Player Age that Season&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerHeight.png" alt="Injury Length in Days over Player Height in Inches">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Injury Length in Days over Player Height in Inches&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerWeight.png" alt="Injury Length in Days over Player Weight in Kilograms">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Injury Length in Days over Player Weight in Kilograms&lt;/p>
&lt;p>Finally, the team decided to use the z-score to normalize all of the data. By using the Z-score from the individual data in a column of df_Injury_stats, the team was able to limit variability of multiple metrics across the dataframe. A player&amp;rsquo;s blocks and steals should be a miniscule amount compared to minutes or points of some players. The same can be said of assists, technical fouls, or any other statistic in the course of an NBA game. The Z-score, by nature of the metric from the mean, allows for much less variability across the columns.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;p>The objective of this project was to develop performance indicators for injured players returning to basketball in the NBA. It is unreasonable to expect a player to return to the same level of play post injury immediately upon starting back up after recovery. It often takes a player months if not years to return to the same level of play as pre-injury, especially considering the severity of the injuries. In order to successfully analyze this information from the datasets, a predictive model will need to be created using a large set of the data to train.&lt;/p>
&lt;p>From this point, a test run was used to gauge the validity and accuracy of the model compared to some of the data set aside. The model created was able to provide feature importance to give a better understanding of which specific features are the most crucial when it comes to determining how bad the effects of an injury may or may not be on player performance. Feature engineering was performed prior to training the model in order to improve the chances of higher accuracy from the predictions. This model could be used to keep an eye out for how a player&amp;rsquo;s performance intensity and the engineered features could affect how long a player takes to recover from injury, if there are any warning signs prior to an injury, and even how well they perform when returning.&lt;/p>
&lt;h3 id="41-development-of-models">4.1 Development of Models&lt;/h3>
&lt;p>To help with review of the data, conditioned data was used to save resources on Google Colab. By conditioning the data and saving the files as a .CSV, the team was able to create a streamlined process. Additionally, the team found benefit by uploading these files to Google Drive to quickly import data near real time. After operating in this fashion for some time, the team was able to load the datasets into Github and utilize that feature. By loading the datasets up to Github, a url could be used to link the files directly to the files saved on Github without using a token like with Kaggle or Google Drive. The files saved were the following:&lt;/p>
&lt;p>&lt;strong>Table 1:&lt;/strong> Datasets Imported&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Dataframe&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Title&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">1.&lt;/td>
&lt;td style="text-align:center">df_Injury_stats&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">2.&lt;/td>
&lt;td style="text-align:center">df_Injury_length&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">3.&lt;/td>
&lt;td style="text-align:center">df_Season_stats&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">4.&lt;/td>
&lt;td style="text-align:center">games&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">5.&lt;/td>
&lt;td style="text-align:center">df_Games_gamesDetails&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">6.&lt;/td>
&lt;td style="text-align:center">injuries_2010-2018&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">7.&lt;/td>
&lt;td style="text-align:center">players&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">8.&lt;/td>
&lt;td style="text-align:center">ranking&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">9.&lt;/td>
&lt;td style="text-align:center">teams&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Every time Google Colab loads data, it takes time and resources. The team was able to utilize the cross platform connectivity of the Google utilities. The team could then focus on building models as opposed to conditioning data every time the code was ran.&lt;/p>
&lt;h4 id="411-evaluation-metrics">4.1.1 Evaluation Metrics&lt;/h4>
&lt;p>The metrics chosen were designed to give results on Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and the Explained Variance (EV) Score. MAE is a measure of errors between paired observations experiencing the same expression. RMSE is the standard deviation of the prediction errors for our dataset. EV is the relationship between the train data and the test data. By using these metrics, the team is capable of reviewing the data in a statistical manner.&lt;/p>
&lt;h4 id="412-gradient-boost-regression">4.1.2 Gradient Boost Regression&lt;/h4>
&lt;p>The initial model that was used was a Gradient Boosting Regressor (GBR) model. This model produced the results shown in Table 2. The GBR model builds in a stage-wise fashion; similarly to other boosting methods. GBR also generalizes the data and attempts to optimize the results utilizing a loss function. An example of the algorithm can be seen in &lt;strong>Figure 5&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/gbr.png" alt="Gradient Boosting Regressor">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Gradient Boosting Regressor &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The team saw a relationship given the data. &lt;strong>Table 2&lt;/strong> shows the results of that model. The results were promising given the speed and utility of a GBR model. The team reviewed the data multiple times after multiple stages of conditioning the data.&lt;/p>
&lt;p>&lt;strong>Table 2:&lt;/strong> GBR Results&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Category&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Value&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MAE Mean&lt;/td>
&lt;td style="text-align:center">-10.787&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MAE STD&lt;/td>
&lt;td style="text-align:center">0.687&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE Mean&lt;/td>
&lt;td style="text-align:center">-115.929&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE STD&lt;/td>
&lt;td style="text-align:center">96.64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV Mean&lt;/td>
&lt;td style="text-align:center">1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV STD&lt;/td>
&lt;td style="text-align:center">0.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>After running a GBR model, the decision was made to try multiple models to see what gives the best results. The team settled on LightGBM and a Deep Learning model utilizing Keras built on the TensorFlow platform. These results will be seen in &lt;em>4.1.2&lt;/em> and &lt;em>4.1.3&lt;/em>.&lt;/p>
&lt;h4 id="412-lightgbm-regression">4.1.2 LightGBM Regression&lt;/h4>
&lt;p>Another algorithm chosen was a Light Gradient Boost Machine (LightGBM) model. LightGBM is known for its lightweight and resource sparse abilities. The model is built from decision tree algorithms and used for ranking, classification, and other machine learning tasks. By choosing LightGBM data scientists are able to analyze larger data a faster approach. LightGBM can often over fit a model if the data is too small, but fortunately for the purpose of this assignment the data available for NBA injuries and stats is extremely large. Availability of data allowed for smooth operation of the LightGBM model. Mandot explains the model really well in The Medium. Mandot said, &lt;em>&amp;ldquo;Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development&amp;rdquo;&lt;/em> &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. There are a lot of benefits available to this algorithm.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/lightGBM_regressor.png" alt="LightGBM Algorithm: Leafwise searching">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> LightGBM Algorithm: Leafwise searching &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>&lt;/p>
&lt;p>When running the model &lt;strong>Table 3&lt;/strong> was generated. This table uses the same metrics as the GBR Results Table (&lt;strong>Table 2&lt;/strong>). After reviewing the results, the GBR model still appeared to be a viable avenue. The Keras model will be evaluated next to see most optimal model to use for repeatable fresults.&lt;/p>
&lt;p>&lt;strong>Table 3:&lt;/strong> LightGBM Results&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Category&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Value&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MAE Mean&lt;/td>
&lt;td style="text-align:center">-0.011&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MAE STD&lt;/td>
&lt;td style="text-align:center">0.001&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE Mean&lt;/td>
&lt;td style="text-align:center">-0.128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE STD&lt;/td>
&lt;td style="text-align:center">0.046&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV Mean&lt;/td>
&lt;td style="text-align:center">0.982&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV STD&lt;/td>
&lt;td style="text-align:center">0.013&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="413-keras-deep-learning-models">4.1.3 Keras Deep Learning Models&lt;/h4>
&lt;p>The final model attempted was a Deep Learning model. A few runs of different layers and epochs were chosen. They can be seen in &lt;strong>Table 4&lt;/strong> (shown later). The model was sequentially ran through the test layers to refine the model. When this is done, each predecessor layer acts as an input to the next layer&amp;rsquo;s input for the model. The results can produce accurate results while using unsupervised learning. The visualization for this model can be seen in the following figure:&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/simple_neural_network_vs_deep_learning.jpg" alt="Neural Network">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Neural Network &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>&lt;/p>
&lt;p>When the team ran the Neural Networks, the data went through three layers. Each layer was built upon the previous similarly to the figure. This allowed for the team to capture information from the processing. &lt;strong>Table 4&lt;/strong> shows the results for the deep learning model.&lt;/p>
&lt;p>&lt;strong>Table 4:&lt;/strong> Epochs and Batch Sizes Chosen&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Number&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Regressor Epoch&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Regressor Batch Sizes&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>KFolds&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Model Epochs&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>R2&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">&lt;em>1.&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>25&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>25&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>10&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>10&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>0.985&lt;/em>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">2.&lt;/td>
&lt;td style="text-align:center">40&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.894&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">3.&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.966&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">4.&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.707&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">5.&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">0.611&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">6.&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">0.982&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The team has decided that the results for the Deep Learning are the most desirable. This model would be the one that the team would recommend based on the results from the metrics available. The parameters the team recommends are italicized in &lt;em>Line 1&lt;/em> of &lt;strong>Table 4&lt;/strong>.&lt;/p>
&lt;h2 id="5-inference">5. Inference&lt;/h2>
&lt;p>With the data available, some conclusions can be made. Not all injuries are of the same severity. By treating an ACL tear in the same manner as a bruise, the team doctors would take terrible approaches to rehab. The severity of the injury is a part of the approach to therapy. This detail is nearly impossible to capture in the model.&lt;/p>
&lt;p>Another aspect to come to a conclusion is that not every player recovers in the same timetable as another. Genetics, diet, effort, and mental health can all harm or reinforce the efforts from the medical staff. These areas are hard to capture in the data and cannot be appropriately reviewed with this model.&lt;/p>
&lt;p>It is also difficult to indicate where a previous injury may have contributed to a current injury. The kinetic chain is a structure of the musculoskeletal system that moves the body using the muscles and bones. If one portion of the chain is compromised, the entire chain will need to be modified to continue movement. This modification can result in more injuries. The data cannot provide this information. It is important to remember these possible confounding variables when interpreting the results of the model.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>After reviewing the results, the team created a robust model to predict the performance of a player after an injury. The coefficient of determination for the deep learning model shows a strong relationship between the training and test sets. After conditioning the data, the results can be seen in &lt;strong>Table 2&lt;/strong>, &lt;strong>Table 3&lt;/strong>, and &lt;strong>Table 5&lt;/strong>. The team had an objective to find this correlation and build it to the point where injury and performance can be modeled. The team was able to accomplish this goal.&lt;/p>
&lt;p>Additionally, these results are consistent with the current scientific literature &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The biological community has been able to record these results for decades. By leveraging this effort, the scientific community could move to a more proactive approach as opposed to reactive with respect to injury controls. This data will also allow for proper contract negotiations to take place in the NBA, considering potential decisions to avoid injury may include less playing time. The negotiations are pivotal to ensuring that expectations are met in the future seasons; especially when injury occurs in the final year of a player&amp;rsquo;s contract. Teams with an improved understanding of how players can or will return from injury have an opportunity to make the best of scenarios where other teams may be hesitant to sign an injured player. These different opportunities for a team&amp;rsquo;s front office could be the difference between a championship ring and missing the playoffs entirely.&lt;/p>
&lt;h2 id="61-limitations">6.1 Limitations&lt;/h2>
&lt;p>With respect to the current work, the models could be continued to be refined. Currently the results are to the original intentions of the team, but improvements can be made. Feature Engineering is always an area where the models can improve. Some valuable features to be created in the future are the calculations for the player&amp;rsquo;s efficiency overall, as well as offensinve and defensive efficiencies in each game. The team would also like to develop a model to use the stats of a player in pre-injury and apply that to the post-injury set of metrics. Also, the team would like to move to where the same could be applied given the length of the injury to the player while considering the severity of the injury. Longer and more severe injury will lead to different future results than say a long not severe injury, or a short injury that was somewhat severe. The number of varaibles that could provide more valuable information to the model are endless.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The authors would like to thank Dr. Gregor von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article. In addition to that the community of students from the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course also deserve a thanks from the author for the support, continued engagement, and valuable discussions through Piazza.&lt;/p>
&lt;h3 id="71-work-breakdown">7.1 Work Breakdown&lt;/h3>
&lt;p>For the effort developed, the team split tasks between each other to cover more ground. The requirements for the investigation required a more extensive effort for the teams in the ENGR-E 534 class. To accomplish the requirements, the task was expanded by addressing multiple datasets within the semester and building in multiple models to display the results. The team members were responsible for committing in Github multiple times throughout the semester. The tasks were divided as follows:&lt;/p>
&lt;ol>
&lt;li>Chelsea Gorius
&lt;ul>
&lt;li>Exploratory Data Analysis&lt;/li>
&lt;li>Feature Engineering&lt;/li>
&lt;li>Keras Deep Learning Model&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Gavin Hemmerlein
&lt;ul>
&lt;li>Organization of Items&lt;/li>
&lt;li>Model Development&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Both
&lt;ul>
&lt;li>Report&lt;/li>
&lt;li>All Outstanding Items&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>A. Mehra, &lt;em>Sports Medicine Market worth $7.2 billion by 2025&lt;/em>, [online] Markets and Markets.
&lt;a href="https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp">https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp&lt;/a> [Accessed Oct. 15, 2020]. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>J. Harris, B. Erickson, B. Bach Jr, G. Abrams, G. Cvetanovich, B. Forsythe, F. McCormick, A. Gupta, B. Cole,
&lt;em>Return-to-Sport and Performance After Anterior Cruciate Ligament Reconstruction in National Basketball Association Players&lt;/em>, Sports Health. 2013 Nov;5(6):562-8. doi: 10.1177/1941738113495788. [Online serial]. Available: &lt;a href="https://pubmed.ncbi.nlm.nih.gov/24427434">https://pubmed.ncbi.nlm.nih.gov/24427434&lt;/a> [Accessed Oct. 24, 2020]. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>W. Kraemer, C. Denegar, and S. Flanagan, &lt;em>Recovery From Injury in Sport: Considerations in the Transition From Medical Care to Performance Care&lt;/em>, Sports Health.
2009 Sep; 1(5): 392â€“395.[Online serial]. Available: &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445177">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445177&lt;/a> [Accessed Oct. 24, 2020]. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>R. Hopkins, &lt;em>NBA Injuries from 2010-2020&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/ghopkins/nba-injuries-2010-2018">https://www.kaggle.com/ghopkins/nba-injuries-2010-2018&lt;/a> [Accessed Oct. 9, 2020]. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>N. Lauga, &lt;em>NBA games data&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv">https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv&lt;/a> [Accessed Oct. 9, 2020]. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>J. Cirtautas, &lt;em>NBA Players&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/justinas/nba-players-data">https://www.kaggle.com/justinas/nba-players-data&lt;/a> [Accessed Oct. 9, 2020]. &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>V. Aliyev, &lt;em>A hands-on explanation of Gradient Boosting Regression&lt;/em>, [online] Medium. &lt;a href="https://medium.com/@vagifaliyev/a-hands-on-explanation-of-gradient-boosting-regression-4cfe7cfdf9e">https://medium.com/@vagifaliyev/a-hands-on-explanation-of-gradient-boosting-regression-4cfe7cfdf9e&lt;/a> [Accessed Nov., 9 2020]. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>P. Mandon, &lt;em>What is LightGBM, How to implement it? How to fine tune the parameters?&lt;/em>, [online] Medium. &lt;a href="https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc">https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc&lt;/a> [Accessed Nov., 9 2020]. &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>The Data Scientist, &lt;em>What deep learning is and isnâ€™t&lt;/em>, [online] The Data Scientist. &lt;a href="https://thedatascientist.com/what-deep-learning-is-and-isnt">https://thedatascientist.com/what-deep-learning-is-and-isnt&lt;/a> [Accessed Nov., 9 2020]. &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-301/test/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-301/test/</guid><description>
&lt;h2 id="gavin-hemmerlein">Gavin Hemmerlein&lt;/h2>
&lt;h2 id="ghemmer">ghemmer&lt;/h2>
&lt;h2 id="engr-e-534">ENGR-E 534&lt;/h2>
&lt;p>This is a test MarkDown file to ensure I have write privileges.&lt;/p>
&lt;h1 id="test-typing">Test Typing&lt;/h1>
&lt;p>This appears to be &lt;em>working.&lt;/em>&lt;/p>
&lt;h1 id="table">Table&lt;/h1>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Col1&lt;/th>
&lt;th>Col2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Row 1&lt;/td>
&lt;td>11&lt;/td>
&lt;td>12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Row 2&lt;/td>
&lt;td>21&lt;/td>
&lt;td>22&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="images">Images&lt;/h1>
&lt;p>&lt;img src="https://assets.iu.edu/brand/3.2.x/trident-large.png" alt="Image of IU Logo">&lt;/p></description></item><item><title>Report:</title><link>/report/fa20-523-302/assignment6/wearables_and_ai/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-302/assignment6/wearables_and_ai/</guid><description>
&lt;p>Wearables and Personalized Medicine&lt;br>
Adam Martin&lt;/p>
&lt;p>Wearables have been on the market for years now, gradually improving and providing increasingly insightful data on user health metrics. Most wearables contain an array of sensors allowing the user to track aspects of their physical health. This includes heart rate, motion, calories burned, and some devices now support ECG and BMI measurements. This vast trove of data is valuable to consumers, as it allows for the measurement and gamification of key health metrics. But can this data also be useful for health professionals in determining a patientâ€™s activity levels and tracing important events in their health history?&lt;/p>
&lt;p>Many wearable devices, predominantly smartwatches, provide high-granularity data to the various apps that consume it. The Apple Watch Core Motion API provides accelerometer, gyroscope, pedometer, magnetometer, altitude, and other measurements at a rate of 50hz. This is in addition to the heart rate data that is sampled throughout the day. Apple also provides a Movement Disorder Manager interface for the analysis of Parkinsonâ€™s disease symptoms. FitBit and Pebble devices provide similar tracking capabilities. Beyond existing consumer smartwatches, there is hope for smart tattoos, VR displays, footwear, and fabrics. These wearables could measure a userâ€™s electrolyte and metabolite levels in their perspiration. They could measure abnormal gaits or detect bacteria (Yetisen, 2018).&lt;/p>
&lt;p>This high-fidelity data describing a wide variety of user activities could be invaluable to a healthcare professional hoping to find some insight in a patientâ€™s condition. However, the process for extraction, transformation, and transfer of this data is unclear. With different device protocols and APIs providing information of varying quality and quantity, there is a need for a centralized, structured database for collection and analysis. Along with this, there is a potential for the application of AI on the analysis of wearable data. Raw sensor values will likely be incomprehensible to most analysts, so clustering of movement types and fuzzy logic on various parameters can allow a healthcare professional to better understand the meaning behind the data. Furthermore, this data can be used to feed into a system of â€œpredictive preventative diagnosisâ€. Patients suffering from a variety of psychological or physical ailments can provide valuable data that highlights periods of symptom expression and also predicts prognosis (Piwek, 2016). When something is measured, it is easier to begin to act towards fixing it.&lt;/p>
&lt;p>The artificial intelligence algorithms employed in the processing of collected data can be as diverse and complex as the systems they attempt to understand. Time series analysis for oscillating signals involving Fourier transforms. Feature extraction analysis through PCA. Noise reduction and motion clustering. These applications ignore the extra layer of abstraction, which involves the diagnosis and prediction aspects of wearable data. The field of wearables devices is growing, along with the promise of better digital representations, or â€˜digital twinsâ€™, of patients. While there are still are matters to consider, including patient well-being and data privacy, the prognosis of wearables changing the healthcare industry looks good.&lt;/p>
&lt;p>â€ƒ
Works Cited&lt;br>
Piwek, L. (2016). The Rise of Consumer Health Wearables: Promises and Barriers. PLOS MEDICINE.&lt;br>
Yetisen, A. K. (2018). Wearables in Medicine. Wiley Online Library.&lt;/p></description></item><item><title>Report:</title><link>/report/fa20-523-302/project/plan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-302/project/plan/</guid><description>
&lt;p>Wearables and Personalized Medicine
Adam Martin&lt;/p>
&lt;p>Wearables have been on the market for years now, gradually improving and providing increasingly insightful data on user health metrics. Most wearables contain an array of sensors allowing the user to track aspects of their physical health. This includes heart rate, motion, calories burned, and some devices now support ECG and BMI measurements. This vast trove of data is valuable to consumers, as it allows for the measurement and gamification of key health metrics. But can this data also be useful for health professionals in determining a patientâ€™s activity levels and tracing important events in their health history?&lt;/p>
&lt;p>Many wearable devices, predominantly smartwatches, provide high-granularity data to the various apps that consume it. The Apple Watch Core Motion API provides accelerometer, gyroscope, pedometer, magnetometer, altitude, and other measurements at a rate of 50hz. This is in addition to the heart rate data that is sampled throughout the day. Apple also provides a Movement Disorder Manager interface for the analysis of Parkinsonâ€™s disease symptoms. FitBit and Pebble devices provide similar tracking capabilities. Beyond existing consumer smartwatches, there is hope for smart tattoos, VR displays, footwear, and fabrics. These wearables could measure a userâ€™s electrolyte and metabolite levels in their perspiration. They could measure abnormal gaits or detect bacteria (Yetisen, 2018).&lt;/p>
&lt;p>This high-fidelity data describing a wide variety of user activities could be invaluable to a healthcare professional hoping to find some insight in a patientâ€™s condition. However, the process for extraction, transformation, and transfer of this data is unclear. With different device protocols and APIs providing information of varying quality and quantity, there is a need for a centralized, structured database for collection and analysis. Along with this, there is a potential for the application of AI on the analysis of wearable data. Raw sensor values will likely be incomprehensible to most analysts, so clustering of movement types and fuzzy logic on various parameters can allow a healthcare professional to better understand the meaning behind the data. Furthermore, this data can be used to feed into a system of â€œpredictive preventative diagnosisâ€. Patients suffering from a variety of psychological or physical ailments can provide valuable data that highlights periods of symptom expression and also predicts prognosis (Piwek, 2016). When something is measured, it is easier to begin to act towards fixing it.&lt;/p>
&lt;p>The artificial intelligence algorithms employed in the processing of collected data can be as diverse and complex as the systems they attempt to understand. Time series analysis for oscillating signals involving Fourier transforms. Feature extraction analysis through PCA. Noise reduction and motion clustering. These applications ignore the extra layer of abstraction, which involves the diagnosis and prediction aspects of wearable data. The field of wearables devices is growing, along with the promise of better digital representations, or â€˜digital twinsâ€™, of patients. While there are still are matters to consider, including patient well-being and data privacy, the prognosis of wearables changing the healthcare industry looks good.&lt;/p>
&lt;p>â€ƒ Works Cited
Piwek, L. (2016). The Rise of Consumer Health Wearables: Promises and Barriers. PLOS MEDICINE.
Yetisen, A. K. (2018). Wearables in Medicine. Wiley Online Library.&lt;/p></description></item><item><title>Report:</title><link>/report/fa20-523-302/project/project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-302/project/project/</guid><description>
&lt;h1 id="review-of-the-use-of-wearables-in-personalized-medicine">Review of the Use of Wearables in Personalized Medicine&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-302/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-302/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-302/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-302/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Adam Martin, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-302">fa20-523-302&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-302/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Wearable devices offer an abundant source of data on wearer activity and health metrics. Smartphones and smartwatches have become increasingly
ubiquitous, and provide high-quality motion sensor data. This research attempts to classify movement types, including running, walking, sitting, standing,
and going up and down stairs, to establish the practicality of sharing this raw data with healthcare workers. It also addresses the existing research regarding
the use of wearable data in clinical settings and discusses shortcomings in making this data available.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-existing-devices">2.1 Existing Devices&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-need-for-wearable-data-in-healthcare">2.2 Need for Wearable Data in Healthcare&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-choice-of-dataset">3. Choice of Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology-and-code">4. Methodology and Code&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-discussion">5. Discussion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-descriptive-analysis">5.1 Descriptive Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-results">5.2 Results&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-results">6.1 Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#62-limitations">6.2 Limitations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#63-impact">6.3 Impact&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Wearables, Classification, Descriptive Analysis, Healthcare, Movement Tracking, Precision Health, LSTM&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Wearables have been on the market for years now, gradually improving and providing increasingly insightful data on user health metrics. Most wearables contain an array of sensors allowing the user to track aspects of their physical health.
This includes heart rate, motion, calories burned, and some devices now support ECG and BMI measurements. This vast trove of data is valuable to consumers, as it allows for the measurement and gamification of key
health metrics. But can this data also be useful for health professionals in determining a patientâ€™s activity levels and tracing important events in their health history?&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>Previous work exists on the use of sensors and wearables in assisted living environments. Consumer wearables are commonplace and have been used primarily for tracking individual activity metrics.
This research attempts to establish the efficacy of these devices in providing useful data for user activity, and how this information could be useful for healthcare workers.
This paper examines the roadblocks in making this information available to healthcare professionals and examines what wearable information is currently being used in healthcare.&lt;/p>
&lt;h3 id="21-existing-devices">2.1 Existing Devices&lt;/h3>
&lt;p>Existing research focuses on a wide variety of inputs &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Sensors including electrodes, chemical probes, microphones, optical detectors, and blood glucose sensors are referenced as devices used for gathering healthcare information. This research will focus on data that can be gathered with a modern smartphone or smartwatch. Most of the sensors described are not as ubiquitous as consumer items like FitBits or Apple Watches.
Furthermore, many users report diminished enthusiasm towards wearables due to complex sensors and pairing processes &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Focusing on devices that are already successful in the consumer market
ensures that the impact of this study will not be confined to specific users and use cases. Apple has released a suite of tools for interfacing with device sensors, and recently launched
ResearchKit and CareKit, providing a framework for researchers and healthcare workers to collect and analyze user data &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. There are several apps available that utilize these tools, including
Johns Hopkins' CorrieHealth app, which helps users manage their heart health care and shares data with their doctors. This is an encouraging step towards streamlining the sharing of
wearable data between patients and healthcare professionals, as Apple provides standards for privacy, consent, and data quality.&lt;/p>
&lt;h3 id="22-need-for-wearable-data-in-healthcare">2.2 Need for Wearable Data in Healthcare&lt;/h3>
&lt;p>Previous studies have indicated the significance of precision health and the need for patient-specific data from wearables to be integrated into a patient&amp;rsquo;s care strategy &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.
Wearable data outlining a patient&amp;rsquo;s sleep, motion habits, heart rate, and other metrics can be invaluable in diagnosing or predicting conditions. Increased sedentary activity could indicate
depression, and could predict future heart problems. A patient&amp;rsquo;s health could be graphed and historical trends could be useful in determining factors that contribute to the patient&amp;rsquo;s condition.
It is often asserted that a person&amp;rsquo;s environmental factors are better predictors for their health than their genetic makeup &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Linking behavioral and social determinants with biomedical data
would allow professionals to better target certain conditions.&lt;/p>
&lt;h2 id="3-choice-of-dataset">3. Choice of Dataset&lt;/h2>
&lt;p>The dataset used for this project contains labeled movement data from wearable devices. The goal is to establish the potential for wearable devices to provide high-quality data to users and healthcare professionals.&lt;/p>
&lt;p>A dataset gathered from 24 individuals with Apple devices measuring attitude, gravity, and acceleration was used to determine user states. The dataset is labeled with six states (walking downstairs,
walking upstairs, sitting, standing, walking and jogging) and each sensor has several attributes describing its motion. The attitude, gravity, and acceleration readings each have three components corresponding to each
axis of freedom. Many smartphones and wearables already offer comprehensive sleep tracking features, so sleep motion
data will not be considered for this study. The CrowdSense iOS application was used to record user movements. Each sensor was configured to sample at 50hz, and each user was instructed to start the recording,
and begin their assigned activity.&lt;/p>
&lt;h2 id="4-methodology-and-code">4. Methodology and Code&lt;/h2>
&lt;p>The IPython notebook used for this analysis is available on the &lt;a href="https://github.com/cybertraining-dsc/fa20-523-302/blob/main/project/code/Wearables.ipynb">GitHub repository&lt;/a>.&lt;/p>
&lt;p>The analysis of relevant wearable data is undertaken to determine the accuracy of activity information. This analysis will consist of a brief descriptive analysis of the motion tracking data,
and will proceed with attempts to classify the labeled data.&lt;/p>
&lt;p>First, the data has to be downloaded from the MotionSense project on GitHub. A basic descriptive analysis will be performed, visualizing the sensor values for each movement class over time.
During the data acquisition, the sensors are sampled at a 50hz rate. Since the dataset is a timeseries, classification methods that take advantage of historical datapoints will be the most effective.
The Keras Long Short Term Memory classifier implementation is used for this task. The dataset is first split into its various classes of motion using the one-hot-encoded matrix to filter out each
class. Each class is then subdivided into one-second &amp;lsquo;windows&amp;rsquo;, each with 50 entries. Each window is offset by 10 entries from the previous window. The use of windows allows the model to remain small
in size, while still gathering enough information to make accurate classifications. The hyperparameters that can be tuned include:&lt;/p>
&lt;ul>
&lt;li>Window size (50)&lt;/li>
&lt;li>Window offset (10)&lt;/li>
&lt;li>LSTM size (50)&lt;/li>
&lt;li>Dense layer size (50)&lt;/li>
&lt;li>Batch size (64)&lt;/li>
&lt;li>Epochs (15)&lt;/li>
&lt;li>Dropout ratio (0.5)&lt;/li>
&lt;/ul>
&lt;p>The resulting data structure is a 3-dimensional array of shape (107434, 12, 50) for the training set and (32439, 12, 50) for the testing set. The dimensions correspond to the number of windows, the number of movement
features, and the number of samples per window, respectively. These windows are then paired with their corresponding movement classifications and fed into a Keras LSTM workflow. This workflow is executed on a standard (non-gpu)
Google Colab instance and benchmarked. The workflow consists of the following:&lt;/p>
&lt;ul>
&lt;li>A Long Short Term Memory layer with the cell count matching the size of the input window (50)&lt;/li>
&lt;li>A Dropout layer to minimize overfitting&lt;/li>
&lt;li>A fully connected layer with relu activation to help learn the weights of the LSTM output&lt;/li>
&lt;li>A fully connected output layer with a softmax activation to return the final classifications&lt;/li>
&lt;/ul>
&lt;p>The model is trained in 15 epochs, and uses a batch size of 64 for each backpropagation.&lt;/p>
&lt;p>If a classification strategy of sufficient accuracy is possible, it will be determined that wearable data can potentially serve as a useful supplementary source of information to aid in establishing a patient&amp;rsquo;s
medical history.&lt;/p>
&lt;p>Reviewing relevant literature is important to determine the current state of wearables research regarding usefulness to healthcare workers and user well-being.
Much of this research will be focused on determining the state of wearables in the healthcare industry and determining if there is a need for streamlined data transfer to healthcare professionals.&lt;/p>
&lt;h2 id="5-discussion">5. Discussion&lt;/h2>
&lt;p>The dataset is comprised of six discrete classes of movement. There are 12 parameters describing the readouts of the sensors over time.&lt;/p>
&lt;h3 id="51-descriptive-analysis">5.1 Descriptive Analysis&lt;/h3>
&lt;p>There is an imbalance in the number of datapoints for each class, which could lead to classification errors.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/occurence.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Data distribution per movement class.&lt;/p>
&lt;p>Only roll, pitch, and yaw are shown for clarity and to illustrate the quality of the readings obtained by the sensors. Figures 2-7 illustrate sensor readouts over time for each class of movement.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_run.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> 10 second sensor readout of a jogging male.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_downstairs.png" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> 20 second sensor readout of a female going downstairs.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_upstairs.png" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> 20 second sensor readout of a male going upstairs.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_walk.png" alt="Figure 5">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> 10 second sensor readout of a female walking.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_sit.png" alt="Figure 6">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> 10 second sensor readout of a male sitting.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_stand.png" alt="Figure 7">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> 10 second sensor readout of a female standing.&lt;/p>
&lt;p>Interestingly, initial classification attempts involving random forests and knn methods performed fairly well despite their inherent lack of awareness of historical data.&lt;/p>
&lt;h3 id="52-results">5.2 Results&lt;/h3>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/LSTM_benchmark.png" alt="Figure 8">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Cloudmesh benchmark for LSTM train and test.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/lstm_curves.png" alt="Figure 9">&lt;/p>
&lt;p>&lt;strong>Figure 9&lt;/strong> LSTM training and loss curves.&lt;/p>
&lt;p>The final accuracy measurement for the LSTM was &lt;strong>%95.42&lt;/strong>. This proves that discrete movement classes can be determined through the analysis of basic sensor data regarding device movement.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;h3 id="61-results">6.1 Results&lt;/h3>
&lt;p>Using relatively basic machine learning methods, it is possible to determine with a high level of accuracy the type of movement being performed at a given moment. Viewing the benchmarks,
the inference time is rapid, taking only 3 seconds to validate results for the entire testing dataset. This model could be distilled for a production environment, and the rapid inference speed
would allow for faster analyses for end users.&lt;/p>
&lt;h3 id="62-limitations">6.2 Limitations&lt;/h3>
&lt;p>The classes of movement considered for this study were limited. For more precise movements, or movement combinations, more data and a more complex model would be required. For example;
classifying the type of activity being done while a user is seated, if they are typing or eating. Future research could involve a wider review of timeseries classifiers, including transformers
convolutional neural networks, and recurrent neural networks, in order to establish what classification strategy would be best suited for this data. Privacy is also important to consider; raw sensor data could provide malicious actors with
information regarding a users daily habits, their gender, their location, and other sensitive data.&lt;/p>
&lt;p>Existing research highlights some of the issues with the adoption of wearable devices in healthcare. Inconsistent reporting, usage, and data quality are the most common concerns &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. Addressing
these issues through an analysis of data quality and device usage could contribute towards the robustness of this study.&lt;/p>
&lt;h3 id="63-impact">6.3 Impact&lt;/h3>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/service_prop.png" alt="Figure 10">&lt;/p>
&lt;p>&lt;strong>Figure 10&lt;/strong> Proposal for integration of wearables data with other data sources and healthcare portals.&lt;/p>
&lt;p>Frameworks like Apple CareKit and Google Fit are emerging to address the increasing demand for health tracking applications. There is a need for a more effective pipeline for sharing this information
securely with doctors and researchers, and these frameworks are a step in the right direction. Furthermore, this research can be applied towards finding correlations between a patient&amp;rsquo;s condition and
their activity history, or helping a patient reach certain goals towards their overall well-being. Comprehensive movement history can be combined with device usage patterns, eating habit data, self-reported
well-being data, and other relevant sources to establish a more holistic perspective of a patient&amp;rsquo;s health.
Giving users and healthcare workers access to and insights on the data that they generate every day can promote healthier habits, increase physician efficacy, and promote overall well-being. The author proposes
the idea of a centralized system for user data tracking. This could support cross-platform devices, and tie into other fitness and well-being apps to provide a centralized and holistic view of a user&amp;rsquo;s health.
A system of this nature could also tie in information from patient portals, including test results, checkup info, and prescription information.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor von Laszewski for his invaluable feedback on this paper, and Dr. Geoffrey Fox for sharing his expertise in Big Data applications throughout this course.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Yetisen, Ali K. (2018, August 16). I Retrieved November 15, 2020 from &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6541866/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6541866/&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Piwek L, Ellis DA, Andrews S, Joinson A. The Rise of Consumer Health Wearables: Promises and Barriers (2016, February 02). I Retrieved November 11, 2020 from &lt;a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001953">https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001953&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Loncar-Turukalo, Tatjana. Literature on Wearable Technology for Connected Health: Scoping Review of Research Trends, Advances, and Barriers (2019, September 21). I Retrieved December 1st from &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6818529/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6818529/&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Glasgow, Russell E. Realizing the full potential of precision health: The need to include patient-reported health behavior, mental health, social determinants, and patient preferences data (2018, September 13). I Retrieved November 15, 2020 from &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6202010/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6202010/&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Malekzadeh, Mohammad. Mobile Sensor Data Anonymization (2018). I Retrieved September 18, 2020 from &lt;a href="http://doi.acm.org/10.1145/3302505.3310068">http://doi.acm.org/10.1145/3302505.3310068&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-304/project/project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-304/project/project/</guid><description>
&lt;p>#How Big Data Can Eliminate Racial Bias and Structural Discrimination&lt;/p>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-304/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-304/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-304/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-304/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;ul>
&lt;li>Robert Neubauer, fa20-523-304&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-304/blob/main/report/report.md">Edit&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Healthcare is utilizing Big Data to to assist in creating systems that can be used to detect health risks, implement preventative care, and provide an overall better experience for patients. However, there are fundmental issues that exist in the creation and implementation of these systems. Medical algorithms and efforts in precision medicine often neglect the structural inequalities that already exist for minorities accessing healthcare and therefore perpetuate bias in the healthcare industry. The author examines current applications of these concepts, how they are affecting minority communities in the United States, and discusses improvements in order to achieve more equitable care in the industry.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-bias-in-medical-algorithms">2. Bias in Medical Algorithms&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-disparities-found-with-data-dashboards">3. Disparities Found with Data Dashboards&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-effect-of-precision-medicine-and-predictive-care">4. Effect of Precision Medicine and Predictive Care&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-precision-public-health">4.1 Precision Public Health&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-telehealth-and-telemedicine-applications">5. Telehealth and Telemedicine Applications&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-limitations-of-teleheath-and-telemedicine">5.1 Limitations of Teleheath and Telemedicine&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> healthcare, machine learning, data science, racial bias, precision medicine, coronavirus, big data, telehealth, telemedicine, public health.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Big Data is helping to reshape healthcare through major advancements in telehealth and precision medicine. Due to the swift increase in telehealth services due to the COVID-19 pandemic, researchers at the University of California San Francisco have found that black and hispanic patients use these services less frequently than white patients. Prior to the pandemic, research showed that racial and ethnic minorities were disadvantaged by the digital divide &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. These differences were attributed to disparities in access to technology and digital literacy &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Studies like these highlight how racial bias in healthcare is getting detected more frequently; However, there are few attempts to eradicate it through the use of similar technology. This has implications in various areas of healthcare including major healthcare algorithms, telehealth, precision medicine, and overall care provision.&lt;/p>
&lt;p>From the 1985 &lt;em>Report of the Secretaryâ€™s Task Force on Black and Minority Health&lt;/em>, &amp;lsquo;Blacks, Hispanics, Native Americans and those of Asian/Pacific Islander heritage have not benefited fully or equitably from the fruits of science or from those systems responsible for translating and using health sciences technology&amp;rsquo; &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The utilization of big data in industries largely acts to automate a process that was carried out by a human. This makes the process quicker to accomplish and the outcomes more precise since human error can now be eliminated. However, whenever people create the algorithms that are implemented, it is common that these algorithms will align with the biases of the human, or system, that created it. An area where this is happening that is especially alarming is the healthcare industry. Structural discrimination has long caused discrepencies in healthcare between white patients and minority patients and, with the introduction of big data to determine who should receive certain kinds of care, the issue has not been resolved but automated. Studies have shown that minority groups that are often at higher risk than white patients receive less preventative care while spending almost equal amounts on healthcare &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. National data also indicates that racial and ethnic minorities also have poorer health outcomes from preventable and treatable diseases such as cardiovascular disease, cancer, asthma, and HIV/AIDS than those in the majority &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="2-bias-in-medical-algorithms">2. Bias in Medical Algorithms&lt;/h2>
&lt;p>In a research article published to &lt;em>Science&lt;/em> in October of 2019, the researchers uncovered that one of the most used algorithms in healthcare, widely adopted by non- and for-profit medical centers and government agencies, less frequently identified black patients for preventative care than white patients. This algorithm is estimated to be applied to around 200 million people in the United States every year in order to target patients for high-risk care management. These programs seek to improve the care of patients with complex health needs by providing additional resources. The dataset used in the study contained the algorithms predictions, the underlying ingredients that formed the algorithm, and rich data outcomes which allowed for the ability to quantify racial disparities and isolate the mechanisms by which they arise. The sample consisted of 6,079 self-identified black patients and 43,539 self-identified white patients where 71.2% of all patients were enrolled in commercial insurance and 28.8% were on Medicare. On average, the patient age was 50.9 years old and 63% of patients were female. The patients enrolled in the study were classified among risk percentiles, where patients with scores at or above the 97th percentile were auto-enrolled and patients with scores over the 55th percentile were encouraged to enroll &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In order to measure health outcomes, they linked predictions to a wide range of outcomes in electronic health records, which included all diagnoses, and key quantitative laboratory studies and vital signs that captured the severity of chronic illnesses. When focusing on a point in the very-high-risk group, which would be patients in the 97th percentile, they were able to quantify the differences between white and black patients, where black patients had 26.3% more chronic illnesses than white patients&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. To get a corrected health outcome measurement among white and black patients, the researchers set a specific risk threshold for health outcomes among all patients, and repeated the procedure to replace healthier white patients with sicker black patients. So, for a white patient with a health risk score above the threshold, their data was replaced with a black patient whose score fell below the threshold and this continued until the health risk scores for black and white patients were equal and the predictive gap between patients would be eliminated. The health scores were based on the number of chronic medical conditions. The researchers then compared the data from their corrected algorithm and the original and found that the fraction of black patients at all risk thresholds above the 50th percentile increased when using the corrected algorithm. At the 97th percentile, the fraction of black patients increased to 46.5% from the original 17.7% &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Black patients are likely to have more severe hypertension, diabetes, renal failure, and anemia, and higher cholesterol. Using data from clinical trials and longitudinal studies, the researchers found that for mortality rates with hypertension and diabetes black patients had a 7.6% and 30% increase, respectively&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In the original and corrected algorithms, black and white patients spent roughly the same amount on healthcare. However, black patients spent more on emergency care and dialysis while white patients spent more on inpatient surgery and outpatient specialist care&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. In a study that tracked black patients with a black versus a white primary care provider, it found the occurrence of a black primary care provider recommending preventative care was significantly higher than recommendations from a white primary care provider. This conclusion sheds additional light on the disparities black patients face in the healthcare system and further adds to the lack of trust black people have in the healthcare system that has been heavily documented since the Tuskegee study &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. The change recommended by the researchers that would correct the gap in the predictive care model was rather simple, shifting from predictions from purely future cost to an index that combined future cost prediction with health prediction. The researchers were able to work with the distributor of the original algorithm in order to make a more equitable algorithm. Since the original and corrected models from the study were both equal in cost but varied significantly in health predictions, they reworked the cost prediction based on health predictions, conditional on the risk factor percentiles. Both of the models excluded race from the predictions, but the algorithm created with the researchers saw an 84% reduction in bias among black patients, reducing the number of excess active chronic conditions in black patients to 7,758.&lt;/p>
&lt;h2 id="3-disparities-found-with-data-dashboards">3. Disparities Found with Data Dashboards&lt;/h2>
&lt;p>To relate this to a present health issue that is affecting everyone, more black patients are dying from the novel coronavirus than white patients. In the United States, in counties where more than 86% of residents are black, the COVID-19 death rates were 10 times higher than the national average &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Considering how medical algorithms allocate resources to black patients, similar trends are expected for minorities, people who speak languages other than english, low-income residents, and people without insurance. At Brigham Health, a member of the not-for-profit Mass General Brigham health system, Karthik Sivashanker, Tam Duong, Shauna Ford, Cheryl Clark, and Sunil Eappen created data dashboards in order to assist staff and those in positions of leadership. The dashboards included rates of those who tested positive for COVID-19 sorted into different subgroups based on race, ethnicity, language, sex, insurance status, geographic location, health-care worker status, inpatient and ICU census, deaths, and discharges &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Through the use of these dashboards, the COVID-19 equity committee were able to identify emerging risks to incident command leaders, including the discovery that non-English speaking Hispanic patients had higher mortality rates when compared to English speaking Hispanic patients. This led to quality-improvement efforts to increase patient access to language interpreters. While attempting to implement these changes, it was discovered that efforts to reduce clinicians entering patient rooms to maintain social distancing guidelines was impacting the ability for interpreters to join at a patient&amp;rsquo;s bedside during clinician rounding. The incident command leadership expanded their virtual translation services by purchasing additional iPads to allow interpreters and patients to communicate through online software. The use of the geographic filter, when combined with a visual map of infection-rates by neighborhood, showed that people who lived in historically segregated and red-lined neighborhoods were tested less frequently but tested positive more frequently than those from affluent white neighborhoods &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. In a study conducted with survey data from the Pew Research Center on U.S. adults with internet access, black people were significantly more likely to report using telehealth services. In the same study, black and latino respondents had higher odds of using telehealth to report symptoms &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>However, COVID-19 is not the only disease that researchers have found to be higher in historically segregated communities. In 1999, Laumann and Youm found that disparities segregation in social and sexual networks explained racial disparities in STDs which, they suggested, could also explain the disparities black people face in the spread of other diseases &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Prior to 1999 researchers believed that some unexplained characteristic of black people described the spread of such diseases, which shows the pervasiveness of racism in healthcare and academia. Residential segregation may influence health by concentrating poverty, environmental pollutants, infectious agents, and other adverse conditions. In 2006, Morello-Frosch and Jesdale found that segregation increased the risk of cancer related to air pollution &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Big Data can assess national and local public health for disease prevention. An example is how the National Health Interview Survey is being used to estimate insurance coverage in different areas of the U.S. population and clinical data is being used to measure access and quality-related outcomes. Community-level data can be linked with health care system data using visualization and network analysis techniques which would enable public health officials and clinicians to effectively allocate resources and assess whether all patients are getting the medical services they need &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. This would drastically improve the health of historically segregated and red-lined communities who are already seeing disparities during the COVID-19 pandemic.&lt;/p>
&lt;h2 id="4-effect-of-precision-medicine-and-predictive-care">4. Effect of Precision Medicine and Predictive Care&lt;/h2>
&lt;p>Public health experts established that the most important determinant of health throughout a personâ€™s course of life is the environment where they live, learn, work, and play. There exists a discrepancy between electronic health record systems in well-resourced clinical practices and smaller clinical sites, leading to disparities in how they are able to support population health management. For Big Data technology, if patient, family, and community focus were implemented equally in both settings, it has shown that the social determinants of health information would both improve public health among minority communities and minimize the disparities that would arise. Geographic information systems are one way to locate social determinants of health. These help focus public health interventions on populations at greater risk of health disparities. Duke University used this type of system to visualize the distribution of individuals with diabetes across Durham County, NC in order to explore the gaps in access to care and self-management resources. This allowed them to identify areas of need and understand where to direct resources. A novel approach to identify place-based disparities in chronic diseases was used by Young, Rivers, and Lewis where they analyzed over 500 million tweets and found a significant association between the geographic location of HIV-related tweets and HIV prevalence, a disease which is known to predominantly affect the black community &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>One of the ways researchers call for strengthening the health of the nation is through community-level engagement. This is often ignored when it comes to precision medicine, which is one of the latest ways that big data is influencing healthcare. It has the potential to benefit racial and ethnic minority populations since there is a lack of clinical trial data with adequate numbers of minority populations. It is because of this lack of clinical data that predictions in precision medicine are often made off risks associated with the majority which give preferential treatment to those in the majority while ignoring the risks of minority groups, further widening the gap in the allocation of preventative health resources. These predictive algorithms are rooted in cost/benefit tradeoffs, which were proven to limit resources to black patients from the science magazine article on medical algorithms &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. For the 13th Annual Texas Conference on Health Disparities, the overall theme was &amp;ldquo;Diversity in the Era of Precision Medicine.&amp;rdquo; Researchers at the event said diversity should be kept at the forefront when designing and implementing the study in order to increase participation by minority groups &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Building a trusting relationship with the community is also necessary for increased participation, therefore the institution responsible for recruitment needs to be perceived as trustworthy by the community. Some barriers for participation shared among minority groups are hidden cost of participation, concern about misuse of research data, lack of understanding the consent form and research materials, language barrier, low perceived risk of disease, and fear of discrimination &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. As discussed previously, overall lack of distrust in the research process is rooted in the fact that research involving minority groups often overwhelmingly benefits the majority by comparison. Due to the lack of representation of minority communities, big clinical data can be generated for the means of conducting pragmatic trials with underserved populations and distribute the lack of benefits &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="41-precision-public-health">4.1 Precision Public Health&lt;/h3>
&lt;p>The benefit of the majority highlights the issue that one prevention strategy does not account for everyone. This is the motivation behind combining precision medicine and public health to create precision public health. The goal of this is to target populations that would benefit most from an intervention as well as identify which populations the intervention would not be suitable for. Machine learning applied to clinical data has been used to predict acute care use and cost of treatment for asthmatic patients and diagnose diabetes, both of which are known to affect black people at greater rates than white patients &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. This takes into account the aforementioned factors that contribute to a personâ€™s health and combines it with genomic data. Useful information about diseases at the population level are attributed to advancements in genetic epidemiology, through increased genetic and genomic testing. Integration of genomic technologies with public health initiatives have already shown success in preventing diabetes and cancers for certain groups, both of which affect black patients at greater rates than white patients. Specifically, black men have the highest incidence and mortality rates of prostate cancer. The presence of Kaiso, a transcriptional repressors present in human genes, is abundant in those with prostate cancer and, in black populations, it has been shown to increase cancer aggressive and reduce survival rates &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. The greatest challenge affecting advancements made to precision public health is the involvement of all subpopulations required to get effective results. This demonstrates another area where thereâ€™s a need for the healthcare industry to prioritize building a stronger relationship with minority communities in order to assist in advancing healthcare.&lt;/p>
&lt;p>Building a stronger relationship with patients begins with having an understanding of the patientâ€™s needs and their backgrounds, requiring multicultural understanding on the physicians side. This can be facilitated by the technological advances in healthcare. Researchers from Johns Hopkins University lay out three strategic approaches to improve multicultural communications. The first is providing direct services to minimize the gap in language barriers through the use of interpreters and increased linguistic competency in health education materials. The second is the incorporation of cultural homophily in care through staff who share a cultural background, inclusion of holistic medical suggestions, and the use of community health workers. Lastly, they highlight the need for more institutional accommodation such as increasing the ability of professionals to interact effectively within the culture of the patient population, more flexible hours of operation, and clinic locations &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. These strategic approaches are much easier to incorporate into practice when used in telehealth monitoring, providing more equitable care to minority patients who are able to use these services. There are three main sections of telehealth monitoring which include synchronous, asynchronous, and remote monitoring. Synchronous would be any real-time interaction, whether it be over the telephone or through audio/visual communication via a tablet or smartphone. This could occur when the patient is at their home or they are present with a healthcare professional while consulting with a medical provider virtually. Asynchronous communication occurs when patients communicate with their provider through a secure messaging platform in their patient portal. Remote patient monitoring is the direct transmission of a patientâ€™s clinical measurements to their healthcare provider. Remote access to healthcare would be the most beneficial to those who are medically and socially vulnerable or those without ready access to providers and could also help preserve the patient-provider relationship &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Connecting a patient to a provider that is from a similar cultural or ethnic background becomes easier through a virtual consultation, a form of synchronous telehealth monitoring. A virtual consultation would also help eliminate the need for transportation and open up the flexibility of meeting times for both the patient and the provider. From this, a way to increase minority patient satisfaction in regards to healthcare during the shift to telehealth services due to COVID-19 restrictions would be a push to increase technology access to these groups by providing them with low-cost technology with remote-monitoring capabilities.&lt;/p>
&lt;h2 id="5-telehealth-and-telemedicine-applications">5. Telehealth and Telemedicine Applications&lt;/h2>
&lt;p>Telehealth monitoring is evolving the patient-provider relationship by extending care beyond the in-person clinical visit. This provides an excellent opportunity to build a more trusting and personal relationship with the patient, which would be critical for minority patients as it would likely increase their trust in the healthcare system. Also, with an increase in transparency and involvement with their healthcare, the patient will be more engaged in the management of their healthcare which will likely have more satisfactory outcomes. Implementing these types of services will create large amounts of new data for patients, requiring big data applications in order to manage it. Similar to the issue of inequality in the common medical algorithm for determination of preventative care, if the data collected from minority groups using this method is not accounted for properly, then the issue of structural discrimination will continue. The data used in healthcare decision-making often comes from a patientâ€™s electronic health record. An issue that presents itself when considering the use of a patientâ€™s electronic health record in the process of using big data to assist with the patientâ€™s healthcare is missing data. In the scope of telehealth monitoring, since the visit and most of the patient monitoring would be done virtually, the electronic health record would need to be updated virtually as well &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>For telehealth to be viable, the tools that accommodate it need to work seamlessly and be supported by the data streams that are integrated into the electronic health record. Most electronic health record systems are unable to be populated with remote self-monitoring patient-generated data &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. However, the American Telemedicine Association is advocating for remotely-monitored patient-generated data to be incorporated into electronic health records. The SMART Health IT platform is an approach that would allow clinical apps to run across health systems and integrate with electronic health records through the use of a standards-based open-source application programming interface (API) Fast Healthcare Interoperability Resources (FHIR). There are also advancements being made in technology that is capable of integrating data from electronic health records with claims, laboratory, imaging, and pharmacy data &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. There is also a push to include social determinants of health disparities including genomics and socioeconomic status in order to further research underlying causes of health disparities &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="51-limitations-of-teleheath-and-telemedicine">5.1 Limitations of Teleheath and Telemedicine&lt;/h3>
&lt;p>The issue of lack of access to the internet and devices that would be necessary for virtual health visits would limit the participation of those from lower socioeconomic backgrounds. From this arises the issue of representativeness in remotely-monitored studies where the participant must have access to a smartphone or tablet. However, much like the Brigham Health group providing iPads in order to assist with language interpretation, there should be an incentive to provide access to these devices for patients in high risk groups in order to boost trust and representation in this type of care. From the article that discussed the survey results that found black and latino patients to be more responsive to using telehealth, the researchers contrasted the findings with another study where 52,000 Mount Sinai patients were monitored between March and May of 2020 that found black patients were less likely to use telehealth than white patients &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. One reason for the discrepancy the researchers introduce is that the Pew survey, while including data from across the country, only focused on adults that had internet access. This brings up the need for expanding broadband access, which is backed by many telehealth experts &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The process of providing internet access and devices with internet capabilities to those without them should be similar to that from the science magazine study where patients whose risk scores are above a certain threshold should automatically qualify for technological assistance. Programs such as the Telehealth Network Grant Program would be beneficial for researchers conducting studies with a similar focus, as the grant emphasizes advancements in tele-behavioral health and tele-emergency medical services and providing access to these services to those who live in rural areas. Patients from rural areas are less likely to have access to technology that would enable them to participate in a study requiring remote monitoring. The grant proposal defines tele-emergency as an electronic, two-way, audio/visual communication service between a central emergency healthcare center, the tele-emergency hub, and a remote hospital emergency department designed to provide real-time emergency care consultation &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. This is especially important when considering that major medical algorithms show that black patients often spend more on emergency medical care.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>Big Data is changing many areas of healthcare and all of the areas that itâ€™s affecting can benefit from making structural changes in order to allow minorities to get equitable healthcare. This includes how the applications are put into place, since Big Data has the ability to demonstrate bias and reinforce structural discrimination in care. It should be commonplace to consider race or ethnicity, socioeconomic status, and other relevant social determinants of health in order to account for this. Several studies have displayed the need for different allocations of resources based on race and ethnicity. From the findings that black patients were often given more equitable treatment when matched with a primary care provider that was black and that COVID-19 has limited in-person resources, such as a bedside interpreter for non-English speaking patients, there should be a development of a resource that allows people to be matched with a primary care provider that aligns with their identity and to connect with them virtually. When considering the lack of trust black people and other minority populations have in the healthcare system, there are a variety of services that would help boost trust in the process of getting proper care. Given the circumstances surrounding COVID-19 pandemic, there is already an emphasis on making improvements within telehealth monitoring as barriers to telehealth have been significantly reduced. Several machine-learning based studies have highlighted the importance of geographic locationâ€™s impact on aspects of the social determinants of health, including the effects in segregated communities. Recent work has shown that black and other ethnic minority patients report having less involvement in medical decisions and lower levels of satisfaction of care. This should motivate researchers who are focused on improving big data applications in the healthcare sector to focus on these communities in order to eliminate disparities in care and increase the amount of minority healthcare workers in order to have accurate representation. From the survey data showing that minority populations were more likely to use telehealth services, there needs to be an effort to highlight these communities in future work surrounding telehealth and telemedicine. Several studies have prepared a foundation for what needs to be improved and have already paved the way for additional research. With the progress that these studies have made and continued reports of inadequacies in care, it is only a matter of time before substantial change is implemented and equitable care is available.&lt;/p>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/kW1Y">E. Weber, S. J. Miller, V. Astha, T. Janevic, and E. Benn, &amp;ldquo;Characteristics of telehealth users in NYC for COVID-related care during the coronavirus pandemic,&amp;rdquo; J. Am. Med. Inform. Assoc., Nov. 2020, doi: 10.1093/jamia/ocaa216.&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/Wsa3">K. Senz, &amp;ldquo;Racial disparities in telemedicine: A research roundup,&amp;rdquo; Nov. 30, 2020. &lt;a href="https://journalistsresource.org/studies/government/health-care/racial-disparities-telemedicine/">https://journalistsresource.org/studies/government/health-care/racial-disparities-telemedicine/&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/VuXu">C. L. F. Gilbert C. Gee, &amp;ldquo;STRUCTURAL RACISM AND HEALTH INEQUITIES: Old Issues, New Directions1,&amp;rdquo; Du Bois Rev., vol. 8, no. 1, p. 115, Apr. 2011, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/NRPs">Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan, &amp;ldquo;Dissecting racial bias in an algorithm used to manage the health of populations,&amp;rdquo; Science, vol. 366, no. 6464, pp. 447â€“453, Oct. 2019, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/mDYj">J. N. G. Chazeman S. Jackson, &amp;ldquo;Addressing Health and Health-Care Disparities: The Role of a Diverse Workforce and the Social Determinants of Health,&amp;rdquo; Public Health Rep., vol. 129, no. Suppl 2, p. 57, 2014, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/IZ1k">A. Mamun et al., &amp;ldquo;Diversity in the Era of Precision Medicine - From Bench to Bedside Implementation,&amp;rdquo; Ethn. Dis., vol. 29, no. 3, p. 517, 2019, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/i6o0">&amp;ldquo;A Data-Driven Approach to Addressing Racial Disparities in Health Care Outcomes,&amp;rdquo; Jul. 21, 2020. &lt;a href="https://hbr.org/2020/07/a-data-driven-approach-to-addressing-racial-disparities-in-health-care-outcomes">https://hbr.org/2020/07/a-data-driven-approach-to-addressing-racial-disparities-in-health-care-outcomes&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/euqs">&amp;ldquo;Study: Black patients more likely than white patients to use telehealth because of pandemic,&amp;rdquo; Sep. 08, 2020. &lt;a href="https://www.healthcareitnews.com/news/study-black-patients-more-likely-white-patients-use-telehealth-because-pandemic">https://www.healthcareitnews.com/news/study-black-patients-more-likely-white-patients-use-telehealth-because-pandemic&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/E4t2">X. Zhang et al., &amp;ldquo;Big Data Science: Opportunities and Challenges to Address Minority Health and Health Disparities in the 21st Century,&amp;rdquo; Ethn. Dis., vol. 27, no. 2, p. 95, 2017, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/0HLR">S. A. Ibrahim, M. E. Charlson, and D. B. Neill, &amp;ldquo;Big Data Analytics and the Struggle for Equity in Health Care: The Promise and Perils,&amp;rdquo; Health Equity, vol. 4, no. 1, p. 99, 2020, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/A4tr">Institute of Medicine (US) Committee on Understanding and Eliminating Racial and Ethnic Disparities, B. D. Smedley, A. Y. Stith, and A. R. Nelson, &amp;ldquo;PATIENT-PROVIDER COMMUNICATION: THE EFFECT OF RACE AND ETHNICITY ON PROCESS AND OUTCOMES OF HEALTHCARE,&amp;rdquo; in Unequal Treatment: Confronting Racial and Ethnic Disparities in Health Care, National Academies Press (US), 2003.&lt;/a> &lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/0RYU">CDC, &amp;ldquo;Using Telehealth to Expand Access to Essential Health Services during the COVID-19 Pandemic,&amp;rdquo; Sep. 10, 2020. &lt;a href="https://www.cdc.gov/coronavirus/2019-ncov/hcp/telehealth.html">https://www.cdc.gov/coronavirus/2019-ncov/hcp/telehealth.html&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/lyDn">&amp;quot;[No title].&amp;quot; &lt;a href="https://www.nejm.org/doi/full/10.1056/NEJMsr1503323">https://www.nejm.org/doi/full/10.1056/NEJMsr1503323&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/FjdO">&amp;ldquo;Telehealth Network Grant Program,&amp;rdquo; Feb. 12, 2020. &lt;a href="https://www.hrsa.gov/grants/find-funding/hrsa-20-036">https://www.hrsa.gov/grants/find-funding/hrsa-20-036&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-304/report/report/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-304/report/report/</guid><description>
&lt;h1 id="how-big-data-can-eliminate-racial-bias-and-structural-discrimination">How Big Data Can Eliminate Racial Bias and Structural Discrimination&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-304/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-304/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-304/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-304/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;ul>
&lt;li>Robert Neubauer, fa20-523-304&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-304/blob/main/report/report.md">Edit&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Healthcare is utilizing Big Data to to assist in creating systems that can be used to detect health risks, implement preventative care, and provide an overall better experience for patients. However, there are fundmental issues that exist in the creation and implementation of these systems. Medical algorithms and efforts in precision medicine often neglect the structural inequalities that already exist for minorities accessing healthcare and therefore perpetuate bias in the healthcare industry. The author examines current applications of these concepts, how they are affecting minority communities in the United States, and discusses improvements in order to achieve more equitable care in the industry.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-bias-in-medical-algorithms">2. Bias in Medical Algorithms&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-disparities-found-with-data-dashboards">3. Disparities Found with Data Dashboards&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-effect-of-precision-medicine-and-predictive-care">4. Effect of Precision Medicine and Predictive Care&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-precision-public-health">4.1 Precision Public Health&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-telehealth-and-telemedicine-applications">5. Telehealth and Telemedicine Applications&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-limitations-of-teleheath-and-telemedicine">5.1 Limitations of Teleheath and Telemedicine&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> healthcare, machine learning, data science, racial bias, precision medicine, coronavirus, big data, telehealth, telemedicine, public health.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Big Data is helping to reshape healthcare through major advancements in telehealth and precision medicine. Due to the swift increase in telehealth services due to the COVID-19 pandemic, researchers at the University of California San Francisco have found that black and hispanic patients use these services less frequently than white patients. Prior to the pandemic, research showed that racial and ethnic minorities were disadvantaged by the digital divide &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. These differences were attributed to disparities in access to technology and digital literacy &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Studies like these highlight how racial bias in healthcare is getting detected more frequently; However, there are few attempts to eradicate it through the use of similar technology. This has implications in various areas of healthcare including major healthcare algorithms, telehealth, precision medicine, and overall care provision.&lt;/p>
&lt;p>From the 1985 &lt;em>Report of the Secretaryâ€™s Task Force on Black and Minority Health&lt;/em>, &amp;lsquo;Blacks, Hispanics, Native Americans and those of Asian/Pacific Islander heritage have not benefited fully or equitably from the fruits of science or from those systems responsible for translating and using health sciences technology&amp;rsquo; &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The utilization of big data in industries largely acts to automate a process that was carried out by a human. This makes the process quicker to accomplish and the outcomes more precise since human error can now be eliminated. However, whenever people create the algorithms that are implemented, it is common that these algorithms will align with the biases of the human, or system, that created it. An area where this is happening that is especially alarming is the healthcare industry. Structural discrimination has long caused discrepencies in healthcare between white patients and minority patients and, with the introduction of big data to determine who should receive certain kinds of care, the issue has not been resolved but automated. Studies have shown that minority groups that are often at higher risk than white patients receive less preventative care while spending almost equal amounts on healthcare &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. National data also indicates that racial and ethnic minorities also have poorer health outcomes from preventable and treatable diseases such as cardiovascular disease, cancer, asthma, and HIV/AIDS than those in the majority &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="2-bias-in-medical-algorithms">2. Bias in Medical Algorithms&lt;/h2>
&lt;p>In a research article published to &lt;em>Science&lt;/em> in October of 2019, the researchers uncovered that one of the most used algorithms in healthcare, widely adopted by non- and for-profit medical centers and government agencies, less frequently identified black patients for preventative care than white patients. This algorithm is estimated to be applied to around 200 million people in the United States every year in order to target patients for high-risk care management. These programs seek to improve the care of patients with complex health needs by providing additional resources. The dataset used in the study contained the algorithms predictions, the underlying ingredients that formed the algorithm, and rich data outcomes which allowed for the ability to quantify racial disparities and isolate the mechanisms by which they arise. The sample consisted of 6,079 self-identified black patients and 43,539 self-identified white patients where 71.2% of all patients were enrolled in commercial insurance and 28.8% were on Medicare. On average, the patient age was 50.9 years old and 63% of patients were female. The patients enrolled in the study were classified among risk percentiles, where patients with scores at or above the 97th percentile were auto-enrolled and patients with scores over the 55th percentile were encouraged to enroll &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In order to measure health outcomes, they linked predictions to a wide range of outcomes in electronic health records, which included all diagnoses, and key quantitative laboratory studies and vital signs that captured the severity of chronic illnesses. When focusing on a point in the very-high-risk group, which would be patients in the 97th percentile, they were able to quantify the differences between white and black patients, where black patients had 26.3% more chronic illnesses than white patients&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. To get a corrected health outcome measurement among white and black patients, the researchers set a specific risk threshold for health outcomes among all patients, and repeated the procedure to replace healthier white patients with sicker black patients. So, for a white patient with a health risk score above the threshold, their data was replaced with a black patient whose score fell below the threshold and this continued until the health risk scores for black and white patients were equal and the predictive gap between patients would be eliminated. The health scores were based on the number of chronic medical conditions. The researchers then compared the data from their corrected algorithm and the original and found that the fraction of black patients at all risk thresholds above the 50th percentile increased when using the corrected algorithm. At the 97th percentile, the fraction of black patients increased to 46.5% from the original 17.7% &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Black patients are likely to have more severe hypertension, diabetes, renal failure, and anemia, and higher cholesterol. Using data from clinical trials and longitudinal studies, the researchers found that for mortality rates with hypertension and diabetes black patients had a 7.6% and 30% increase, respectively&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In the original and corrected algorithms, black and white patients spent roughly the same amount on healthcare. However, black patients spent more on emergency care and dialysis while white patients spent more on inpatient surgery and outpatient specialist care&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. In a study that tracked black patients with a black versus a white primary care provider, it found the occurrence of a black primary care provider recommending preventative care was significantly higher than recommendations from a white primary care provider. This conclusion sheds additional light on the disparities black patients face in the healthcare system and further adds to the lack of trust black people have in the healthcare system that has been heavily documented since the Tuskegee study &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. The change recommended by the researchers that would correct the gap in the predictive care model was rather simple, shifting from predictions from purely future cost to an index that combined future cost prediction with health prediction. The researchers were able to work with the distributor of the original algorithm in order to make a more equitable algorithm. Since the original and corrected models from the study were both equal in cost but varied significantly in health predictions, they reworked the cost prediction based on health predictions, conditional on the risk factor percentiles. Both of the models excluded race from the predictions, but the algorithm created with the researchers saw an 84% reduction in bias among black patients, reducing the number of excess active chronic conditions in black patients to 7,758.&lt;/p>
&lt;h2 id="3-disparities-found-with-data-dashboards">3. Disparities Found with Data Dashboards&lt;/h2>
&lt;p>To relate this to a present health issue that is affecting everyone, more black patients are dying from the novel coronavirus than white patients. In the United States, in counties where more than 86% of residents are black, the COVID-19 death rates were 10 times higher than the national average &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Considering how medical algorithms allocate resources to black patients, similar trends are expected for minorities, people who speak languages other than english, low-income residents, and people without insurance. At Brigham Health, a member of the not-for-profit Mass General Brigham health system, Karthik Sivashanker, Tam Duong, Shauna Ford, Cheryl Clark, and Sunil Eappen created data dashboards in order to assist staff and those in positions of leadership. The dashboards included rates of those who tested positive for COVID-19 sorted into different subgroups based on race, ethnicity, language, sex, insurance status, geographic location, health-care worker status, inpatient and ICU census, deaths, and discharges &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Through the use of these dashboards, the COVID-19 equity committee were able to identify emerging risks to incident command leaders, including the discovery that non-English speaking Hispanic patients had higher mortality rates when compared to English speaking Hispanic patients. This led to quality-improvement efforts to increase patient access to language interpreters. While attempting to implement these changes, it was discovered that efforts to reduce clinicians entering patient rooms to maintain social distancing guidelines was impacting the ability for interpreters to join at a patient&amp;rsquo;s bedside during clinician rounding. The incident command leadership expanded their virtual translation services by purchasing additional iPads to allow interpreters and patients to communicate through online software. The use of the geographic filter, when combined with a visual map of infection-rates by neighborhood, showed that people who lived in historically segregated and red-lined neighborhoods were tested less frequently but tested positive more frequently than those from affluent white neighborhoods &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. In a study conducted with survey data from the Pew Research Center on U.S. adults with internet access, black people were significantly more likely to report using telehealth services. In the same study, black and latino respondents had higher odds of using telehealth to report symptoms &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>However, COVID-19 is not the only disease that researchers have found to be higher in historically segregated communities. In 1999, Laumann and Youm found that disparities segregation in social and sexual networks explained racial disparities in STDs which, they suggested, could also explain the disparities black people face in the spread of other diseases &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Prior to 1999 researchers believed that some unexplained characteristic of black people described the spread of such diseases, which shows the pervasiveness of racism in healthcare and academia. Residential segregation may influence health by concentrating poverty, environmental pollutants, infectious agents, and other adverse conditions. In 2006, Morello-Frosch and Jesdale found that segregation increased the risk of cancer related to air pollution &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Big Data can assess national and local public health for disease prevention. An example is how the National Health Interview Survey is being used to estimate insurance coverage in different areas of the U.S. population and clinical data is being used to measure access and quality-related outcomes. Community-level data can be linked with health care system data using visualization and network analysis techniques which would enable public health officials and clinicians to effectively allocate resources and assess whether all patients are getting the medical services they need &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. This would drastically improve the health of historically segregated and red-lined communities who are already seeing disparities during the COVID-19 pandemic.&lt;/p>
&lt;h2 id="4-effect-of-precision-medicine-and-predictive-care">4. Effect of Precision Medicine and Predictive Care&lt;/h2>
&lt;p>Public health experts established that the most important determinant of health throughout a personâ€™s course of life is the environment where they live, learn, work, and play. There exists a discrepancy between electronic health record systems in well-resourced clinical practices and smaller clinical sites, leading to disparities in how they are able to support population health management. For Big Data technology, if patient, family, and community focus were implemented equally in both settings, it has shown that the social determinants of health information would both improve public health among minority communities and minimize the disparities that would arise. Geographic information systems are one way to locate social determinants of health. These help focus public health interventions on populations at greater risk of health disparities. Duke University used this type of system to visualize the distribution of individuals with diabetes across Durham County, NC in order to explore the gaps in access to care and self-management resources. This allowed them to identify areas of need and understand where to direct resources. A novel approach to identify place-based disparities in chronic diseases was used by Young, Rivers, and Lewis where they analyzed over 500 million tweets and found a significant association between the geographic location of HIV-related tweets and HIV prevalence, a disease which is known to predominantly affect the black community &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>One of the ways researchers call for strengthening the health of the nation is through community-level engagement. This is often ignored when it comes to precision medicine, which is one of the latest ways that big data is influencing healthcare. It has the potential to benefit racial and ethnic minority populations since there is a lack of clinical trial data with adequate numbers of minority populations. It is because of this lack of clinical data that predictions in precision medicine are often made off risks associated with the majority which give preferential treatment to those in the majority while ignoring the risks of minority groups, further widening the gap in the allocation of preventative health resources. These predictive algorithms are rooted in cost/benefit tradeoffs, which were proven to limit resources to black patients from the science magazine article on medical algorithms &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. For the 13th Annual Texas Conference on Health Disparities, the overall theme was &amp;ldquo;Diversity in the Era of Precision Medicine.&amp;rdquo; Researchers at the event said diversity should be kept at the forefront when designing and implementing the study in order to increase participation by minority groups &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Building a trusting relationship with the community is also necessary for increased participation, therefore the institution responsible for recruitment needs to be perceived as trustworthy by the community. Some barriers for participation shared among minority groups are hidden cost of participation, concern about misuse of research data, lack of understanding the consent form and research materials, language barrier, low perceived risk of disease, and fear of discrimination &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. As discussed previously, overall lack of distrust in the research process is rooted in the fact that research involving minority groups often overwhelmingly benefits the majority by comparison. Due to the lack of representation of minority communities, big clinical data can be generated for the means of conducting pragmatic trials with underserved populations and distribute the lack of benefits &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="41-precision-public-health">4.1 Precision Public Health&lt;/h3>
&lt;p>The benefit of the majority highlights the issue that one prevention strategy does not account for everyone. This is the motivation behind combining precision medicine and public health to create precision public health. The goal of this is to target populations that would benefit most from an intervention as well as identify which populations the intervention would not be suitable for. Machine learning applied to clinical data has been used to predict acute care use and cost of treatment for asthmatic patients and diagnose diabetes, both of which are known to affect black people at greater rates than white patients &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. This takes into account the aforementioned factors that contribute to a personâ€™s health and combines it with genomic data. Useful information about diseases at the population level are attributed to advancements in genetic epidemiology, through increased genetic and genomic testing. Integration of genomic technologies with public health initiatives have already shown success in preventing diabetes and cancers for certain groups, both of which affect black patients at greater rates than white patients. Specifically, black men have the highest incidence and mortality rates of prostate cancer. The presence of Kaiso, a transcriptional repressors present in human genes, is abundant in those with prostate cancer and, in black populations, it has been shown to increase cancer aggressive and reduce survival rates &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. The greatest challenge affecting advancements made to precision public health is the involvement of all subpopulations required to get effective results. This demonstrates another area where thereâ€™s a need for the healthcare industry to prioritize building a stronger relationship with minority communities in order to assist in advancing healthcare.&lt;/p>
&lt;p>Building a stronger relationship with patients begins with having an understanding of the patientâ€™s needs and their backgrounds, requiring multicultural understanding on the physicians side. This can be facilitated by the technological advances in healthcare. Researchers from Johns Hopkins University lay out three strategic approaches to improve multicultural communications. The first is providing direct services to minimize the gap in language barriers through the use of interpreters and increased linguistic competency in health education materials. The second is the incorporation of cultural homophily in care through staff who share a cultural background, inclusion of holistic medical suggestions, and the use of community health workers. Lastly, they highlight the need for more institutional accommodation such as increasing the ability of professionals to interact effectively within the culture of the patient population, more flexible hours of operation, and clinic locations &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. These strategic approaches are much easier to incorporate into practice when used in telehealth monitoring, providing more equitable care to minority patients who are able to use these services. There are three main sections of telehealth monitoring which include synchronous, asynchronous, and remote monitoring. Synchronous would be any real-time interaction, whether it be over the telephone or through audio/visual communication via a tablet or smartphone. This could occur when the patient is at their home or they are present with a healthcare professional while consulting with a medical provider virtually. Asynchronous communication occurs when patients communicate with their provider through a secure messaging platform in their patient portal. Remote patient monitoring is the direct transmission of a patientâ€™s clinical measurements to their healthcare provider. Remote access to healthcare would be the most beneficial to those who are medically and socially vulnerable or those without ready access to providers and could also help preserve the patient-provider relationship &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Connecting a patient to a provider that is from a similar cultural or ethnic background becomes easier through a virtual consultation, a form of synchronous telehealth monitoring. A virtual consultation would also help eliminate the need for transportation and open up the flexibility of meeting times for both the patient and the provider. From this, a way to increase minority patient satisfaction in regards to healthcare during the shift to telehealth services due to COVID-19 restrictions would be a push to increase technology access to these groups by providing them with low-cost technology with remote-monitoring capabilities.&lt;/p>
&lt;h2 id="5-telehealth-and-telemedicine-applications">5. Telehealth and Telemedicine Applications&lt;/h2>
&lt;p>Telehealth monitoring is evolving the patient-provider relationship by extending care beyond the in-person clinical visit. This provides an excellent opportunity to build a more trusting and personal relationship with the patient, which would be critical for minority patients as it would likely increase their trust in the healthcare system. Also, with an increase in transparency and involvement with their healthcare, the patient will be more engaged in the management of their healthcare which will likely have more satisfactory outcomes. Implementing these types of services will create large amounts of new data for patients, requiring big data applications in order to manage it. Similar to the issue of inequality in the common medical algorithm for determination of preventative care, if the data collected from minority groups using this method is not accounted for properly, then the issue of structural discrimination will continue. The data used in healthcare decision-making often comes from a patientâ€™s electronic health record. An issue that presents itself when considering the use of a patientâ€™s electronic health record in the process of using big data to assist with the patientâ€™s healthcare is missing data. In the scope of telehealth monitoring, since the visit and most of the patient monitoring would be done virtually, the electronic health record would need to be updated virtually as well &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>For telehealth to be viable, the tools that accommodate it need to work seamlessly and be supported by the data streams that are integrated into the electronic health record. Most electronic health record systems are unable to be populated with remote self-monitoring patient-generated data &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. However, the American Telemedicine Association is advocating for remotely-monitored patient-generated data to be incorporated into electronic health records. The SMART Health IT platform is an approach that would allow clinical apps to run across health systems and integrate with electronic health records through the use of a standards-based open-source application programming interface (API) Fast Healthcare Interoperability Resources (FHIR). There are also advancements being made in technology that is capable of integrating data from electronic health records with claims, laboratory, imaging, and pharmacy data &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. There is also a push to include social determinants of health disparities including genomics and socioeconomic status in order to further research underlying causes of health disparities &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="51-limitations-of-teleheath-and-telemedicine">5.1 Limitations of Teleheath and Telemedicine&lt;/h3>
&lt;p>The issue of lack of access to the internet and devices that would be necessary for virtual health visits would limit the participation of those from lower socioeconomic backgrounds. From this arises the issue of representativeness in remotely-monitored studies where the participant must have access to a smartphone or tablet. However, much like the Brigham Health group providing iPads in order to assist with language interpretation, there should be an incentive to provide access to these devices for patients in high risk groups in order to boost trust and representation in this type of care. From the article that discussed the survey results that found black and latino patients to be more responsive to using telehealth, the researchers contrasted the findings with another study where 52,000 Mount Sinai patients were monitored between March and May of 2020 that found black patients were less likely to use telehealth than white patients &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. One reason for the discrepancy the researchers introduce is that the Pew survey, while including data from across the country, only focused on adults that had internet access. This brings up the need for expanding broadband access, which is backed by many telehealth experts &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The process of providing internet access and devices with internet capabilities to those without them should be similar to that from the science magazine study where patients whose risk scores are above a certain threshold should automatically qualify for technological assistance. Programs such as the Telehealth Network Grant Program would be beneficial for researchers conducting studies with a similar focus, as the grant emphasizes advancements in tele-behavioral health and tele-emergency medical services and providing access to these services to those who live in rural areas. Patients from rural areas are less likely to have access to technology that would enable them to participate in a study requiring remote monitoring. The grant proposal defines tele-emergency as an electronic, two-way, audio/visual communication service between a central emergency healthcare center, the tele-emergency hub, and a remote hospital emergency department designed to provide real-time emergency care consultation &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. This is especially important when considering that major medical algorithms show that black patients often spend more on emergency medical care.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>Big Data is changing many areas of healthcare and all of the areas that itâ€™s affecting can benefit from making structural changes in order to allow minorities to get equitable healthcare. This includes how the applications are put into place, since Big Data has the ability to demonstrate bias and reinforce structural discrimination in care. It should be commonplace to consider race or ethnicity, socioeconomic status, and other relevant social determinants of health in order to account for this. Several studies have displayed the need for different allocations of resources based on race and ethnicity. From the findings that black patients were often given more equitable treatment when matched with a primary care provider that was black and that COVID-19 has limited in-person resources, such as a bedside interpreter for non-English speaking patients, there should be a development of a resource that allows people to be matched with a primary care provider that aligns with their identity and to connect with them virtually. When considering the lack of trust black people and other minority populations have in the healthcare system, there are a variety of services that would help boost trust in the process of getting proper care. Given the circumstances surrounding COVID-19 pandemic, there is already an emphasis on making improvements within telehealth monitoring as barriers to telehealth have been significantly reduced. Several machine-learning based studies have highlighted the importance of geographic locationâ€™s impact on aspects of the social determinants of health, including the effects in segregated communities. Recent work has shown that black and other ethnic minority patients report having less involvement in medical decisions and lower levels of satisfaction of care. This should motivate researchers who are focused on improving big data applications in the healthcare sector to focus on these communities in order to eliminate disparities in care and increase the amount of minority healthcare workers in order to have accurate representation. From the survey data showing that minority populations were more likely to use telehealth services, there needs to be an effort to highlight these communities in future work surrounding telehealth and telemedicine. Several studies have prepared a foundation for what needs to be improved and have already paved the way for additional research. With the progress that these studies have made and continued reports of inadequacies in care, it is only a matter of time before substantial change is implemented and equitable care is available.&lt;/p>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/kW1Y">E. Weber, S. J. Miller, V. Astha, T. Janevic, and E. Benn, &amp;ldquo;Characteristics of telehealth users in NYC for COVID-related care during the coronavirus pandemic,&amp;rdquo; J. Am. Med. Inform. Assoc., Nov. 2020, doi: 10.1093/jamia/ocaa216.&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/Wsa3">K. Senz, &amp;ldquo;Racial disparities in telemedicine: A research roundup,&amp;rdquo; Nov. 30, 2020. &lt;a href="https://journalistsresource.org/studies/government/health-care/racial-disparities-telemedicine/">https://journalistsresource.org/studies/government/health-care/racial-disparities-telemedicine/&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/VuXu">C. L. F. Gilbert C. Gee, &amp;ldquo;STRUCTURAL RACISM AND HEALTH INEQUITIES: Old Issues, New Directions1,&amp;rdquo; Du Bois Rev., vol. 8, no. 1, p. 115, Apr. 2011, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/NRPs">Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan, &amp;ldquo;Dissecting racial bias in an algorithm used to manage the health of populations,&amp;rdquo; Science, vol. 366, no. 6464, pp. 447â€“453, Oct. 2019, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/mDYj">J. N. G. Chazeman S. Jackson, &amp;ldquo;Addressing Health and Health-Care Disparities: The Role of a Diverse Workforce and the Social Determinants of Health,&amp;rdquo; Public Health Rep., vol. 129, no. Suppl 2, p. 57, 2014, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/IZ1k">A. Mamun et al., &amp;ldquo;Diversity in the Era of Precision Medicine - From Bench to Bedside Implementation,&amp;rdquo; Ethn. Dis., vol. 29, no. 3, p. 517, 2019, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/i6o0">&amp;ldquo;A Data-Driven Approach to Addressing Racial Disparities in Health Care Outcomes,&amp;rdquo; Jul. 21, 2020. &lt;a href="https://hbr.org/2020/07/a-data-driven-approach-to-addressing-racial-disparities-in-health-care-outcomes">https://hbr.org/2020/07/a-data-driven-approach-to-addressing-racial-disparities-in-health-care-outcomes&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/euqs">&amp;ldquo;Study: Black patients more likely than white patients to use telehealth because of pandemic,&amp;rdquo; Sep. 08, 2020. &lt;a href="https://www.healthcareitnews.com/news/study-black-patients-more-likely-white-patients-use-telehealth-because-pandemic">https://www.healthcareitnews.com/news/study-black-patients-more-likely-white-patients-use-telehealth-because-pandemic&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/E4t2">X. Zhang et al., &amp;ldquo;Big Data Science: Opportunities and Challenges to Address Minority Health and Health Disparities in the 21st Century,&amp;rdquo; Ethn. Dis., vol. 27, no. 2, p. 95, 2017, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/0HLR">S. A. Ibrahim, M. E. Charlson, and D. B. Neill, &amp;ldquo;Big Data Analytics and the Struggle for Equity in Health Care: The Promise and Perils,&amp;rdquo; Health Equity, vol. 4, no. 1, p. 99, 2020, Accessed: Dec. 07, 2020. [Online].&lt;/a> &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/A4tr">Institute of Medicine (US) Committee on Understanding and Eliminating Racial and Ethnic Disparities, B. D. Smedley, A. Y. Stith, and A. R. Nelson, &amp;ldquo;PATIENT-PROVIDER COMMUNICATION: THE EFFECT OF RACE AND ETHNICITY ON PROCESS AND OUTCOMES OF HEALTHCARE,&amp;rdquo; in Unequal Treatment: Confronting Racial and Ethnic Disparities in Health Care, National Academies Press (US), 2003.&lt;/a> &lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/0RYU">CDC, &amp;ldquo;Using Telehealth to Expand Access to Essential Health Services during the COVID-19 Pandemic,&amp;rdquo; Sep. 10, 2020. &lt;a href="https://www.cdc.gov/coronavirus/2019-ncov/hcp/telehealth.html">https://www.cdc.gov/coronavirus/2019-ncov/hcp/telehealth.html&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/lyDn">&amp;quot;[No title].&amp;quot; &lt;a href="https://www.nejm.org/doi/full/10.1056/NEJMsr1503323">https://www.nejm.org/doi/full/10.1056/NEJMsr1503323&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>&lt;a href="http://paperpile.com/b/9IXs7U/FjdO">&amp;ldquo;Telehealth Network Grant Program,&amp;rdquo; Feb. 12, 2020. &lt;a href="https://www.hrsa.gov/grants/find-funding/hrsa-20-036">https://www.hrsa.gov/grants/find-funding/hrsa-20-036&lt;/a> (accessed Dec. 07, 2020).&lt;/a> &lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-304/test/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-304/test/</guid><description>
&lt;h1 id="header">Header&lt;/h1>
&lt;h2 id="sub-header-with">Sub Header with&lt;/h2>
&lt;ul>
&lt;li>Bulleted&lt;/li>
&lt;li>lists&lt;/li>
&lt;/ul>
&lt;h2 id="sub-header-with-1">Sub Header with&lt;/h2>
&lt;ol>
&lt;li>Numbered&lt;/li>
&lt;li>Lists&lt;/li>
&lt;/ol></description></item><item><title>Report:</title><link>/report/fa20-523-305/homework3/cody_harris_hw3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-305/homework3/cody_harris_hw3/</guid><description>
&lt;h1 id="square-kilometer-array-ska-use-case">Square Kilometer Array (SKA) Use Case&lt;/h1>
&lt;p>The SKA is an unprecedented, international, engineering endeavor to create the largest radio telescope in the world. Completion of this project requires the use of state-of-the-art technologies to facilitate the massive amount of data that will be captured [1]. Once this data is captured, it will require advanced high-performance computing centers to make sense of the data and gain valuable insight. While there are many innovative ideas involved with the SKA, this use case will only examine the technologies and processes involved with the solutions directly related to the SKAâ€™s big data needs.&lt;/p>
&lt;h1 id="what-is-a-radio-telescope">What is a radio telescope?&lt;/h1>
&lt;p>Before understanding the data needs of the SKA, it is important to understand what a radio telescope is. Many people are familiar with a regular telescope that uses a series of lenses to amplify light waves from distant places to create an image. A radio telescope is similar in the fact that it collects weak electromagnetic radiation from far distances, and then amplifies it so that it can be analyzed. Another application could be to send radio waves towards a direction and then record the reflection off celestial bodies. In any case, the signalâ€™s that astronomers are interested in are extremely weak. Many earthly sources of electro-magnetic radiation are many times greater in strength. There are multiple ways to combat this noise from earth-based radiation, and some of it could be done using hardware, or software, but there are also other ways to combat this that the SKA is utilizing.
Modern radio telescopes accept a wide range of radio frequencies, and then computationally split the frequencies into up to many thousands of channels. To further complicate things, while increasing the efficacy of the radio telescopes, generally more than one telescope is used. This allows multiple positions on the ground to receive the same radio signal, but at slightly different times and slightly different phases of the waveform. This variation allows for more complex analysis of the radio signal. Obviously, this adds another step in the computational work, but having a large array of radio telescopes is imperative to accomplish most modern astronomical research goals [2].&lt;/p>
&lt;h1 id="science-goals">Science Goals&lt;/h1>
&lt;p>The vast size of the SKA project allows the exploration of a variety of burning questions that not only intrigue astrophysicists, but nearly everyone on the planet. One overreaching design goal of the SKA is to have a design flexible enough that it can be used as a â€œdiscovery machineâ€ for the â€œexploration of the unknownâ€. With that said, there are five broad research goals of the SKA [3].&lt;/p>
&lt;h2 id="galaxy-evolution-and-dark-energy">Galaxy Evolution and Dark Energy&lt;/h2>
&lt;p>As a central goal of the SKA, this is quite a broad question that requires a great deal of study to fully understand. With the data gathered, researchers how to understand fundamental questions about how galaxies change over the course of their lifetimes. One problem with studying this, is that most galaxies nearest to us are so far along in their evolution that it is hard to know what happens in the early years of the galaxy. We can overcome this challenge with SKA, due to its â€œsensitivity and resolutionâ€. The SKA will be able to focus on younger galaxies that are much earlier in their evolution to study what our galaxy was like shortly after the big bang.
To gain an understanding of the creation and evolution of galaxies, a study of dark energy must be done. While this mysterious energy has made headlines in the past decade, it is still the subject of a lot of speculation. As gravity is a main driving factor in the evolution of cosmic objects, understanding dark energy is needed to gain a full picture of what is happening in galactical evolution. Currently our fundamental physical theories, derived by Einstein, suggest that universal expansion should be slowing, but it is not. This is where dark energy plays a part in the formation of our universe [4].&lt;/p>
&lt;h2 id="was-einsteins-theory-of-relativity-correct">Was Einsteinâ€™s theory of relativity, correct?&lt;/h2>
&lt;p>It is a tall order to question the most influential physicist in history. Technology is catching up with our theoretical understanding of physics so that we can test fundamental theories that we have held true for many years. The SKA hopes to use its incredible sensitivity to investigate gravitational waves from extremely powerful sources of gravity such as black holes. While Einsteinâ€™s theories are very likely to be mostly true, they might not be fully complete and that is what SKA hopes to find out [1].&lt;/p>
&lt;h2 id="what-are-the-sources-of-large-magnetic-fields-in-space">What are the sources of large magnetic fields in space?&lt;/h2>
&lt;p>We know that our earth creates a magnetic field that is imperative for life to exist. For the most part we understand that this is due to the composition and actions of the core of the planet. When it comes to the origin of magnetic fields in space, we are not completely sure what creates all the fields. The study of these magnetic fields will allow further study of the evolution of galaxies and our universe [5].&lt;/p>
&lt;h2 id="what-are-the-origins-of-our-universe">What are the origins of our universe?&lt;/h2>
&lt;p>This is a burning question that we have some theories about, but still have a great deal of exploration to do on the topic. The prevailing theory relies on the big bang, but the SKA hopes to further study the eras shortly after the big bang to gain insight into the origins of our universe. The SKA hopes to do this by once again using its sensitivity to give the most accurate measurements of the initial light sources in our universe [6]. As long this question remains unsolved, humans will always want to understand where we all came from.&lt;/p>
&lt;h2 id="as-living-beings-are-we-alone-in-the-universe">As living beings, are we alone in the universe?&lt;/h2>
&lt;p>Using Drakeâ€™s equation, and new exoplanet information, scientists are extremely optimistic that life exists somewhere in our universe. In some estimates, what has happened on our planet, could have happened about â€œ10 billion other times over in cosmic history!â€ [7]. One way that SKA can look for extraterrestrial life is by searching for radio signals sent out by advanced civilizations such as ours. Another way that SKA could look for extraterrestrial life is by looking for signs of the building blocks of life. One of these building blocks are amino acids, which can be identified by the SKA.&lt;/p>
&lt;h1 id="current-progress">Current Progress&lt;/h1>
&lt;p>The SKA telescopes reside in two separate locations. One location is in Western Australia and will be focused on low frequencies. The second location is in South Africa and will have two arrays, one for mid frequencies, and one for mid to high frequency [8].&lt;/p>
&lt;h2 id="south-africa">South Africa&lt;/h2>
&lt;p>Design and preparations for the final SKA implementation are still on-going. Currently there are two arrays named KAT7 and MeerKAT that are installed and functioning and will be the precursor to the SKA arrays in South Africa.&lt;/p>
&lt;h2 id="australia">Australia&lt;/h2>
&lt;p>This site also has a precursor to SKA already operating named ASKAP. It is currently located in the same location that the SKAâ€™s major components will eventually occupy, so this will give insights into the performance of this location for radio telescopes. Also, in Australia, as recent as in the past year, prototype antennas are being setup in smaller arrays to capture data and run tests before the design is used in the final array [10].&lt;/p>
&lt;h1 id="big-data-challenges-and-solutions">Big Data Challenges and Solutions&lt;/h1>
&lt;p>The SKA presents many big data challenges, from preprocessing to long-term storage of data. The estimated output of all the telescopes is around 700 PB per year [12].&lt;/p>
&lt;h2 id="raw-data-and-preprocessing">Raw Data and Preprocessing&lt;/h2>
&lt;p>The data comes in the form of an analog radio signals that are collected over a vast geographical area. At some point, to do analytics on the data, the data needs to be converted from analog to digital. While this is usually done via hardware, and is not on computational machines, this is still a data processing step that must be done at scale.
There is also some preprocessing of the data, that must happen constantly as data is collected. While this could be done once reaching the supercomputer, it is a repetitive task that could be done using FPGAs. The benefit of using a FGPA is that it can parallel process in many more threads and do repetitive algorithms faster and with less power as normal CPUs [12].&lt;/p>
&lt;h2 id="storage-and-access">Storage and Access&lt;/h2>
&lt;p>As mentioned previously, the estimated data output of the telescope at peak is 700 PB. The initiative also hopes to save all data for the lifetime of the project which is around 50 years. This ends up being in the realm of needing to eventually store 35 EB of data. For more immediate storage, the SKA team plans to use a buffer system. The way this works is by having a large array of fast read and write storage devices such as SSDs and NVMe (a specialized SSD). This buffer will immediately take in the data as it is coming in at rates that require write speeds that are not as prevalent with traditional spinning disks. After being written to this buffer, they will slowly move the data onto more affordable solutions, that have slower read/write speeds.
While the team could use SSDs for the entire storage, the cost would be enormous. It is much more cost effective to have most of the data stored on hard disk. When it comes to long-term storage of data, even cheaper sources of data such as tape drives could be utilized. After a certain time from data collection, the data will be opened up to the public, this means that the data will likely not end up in a cold storage system [12].&lt;/p>
&lt;h2 id="processing-of-data">Processing of data&lt;/h2>
&lt;p>Currently, the processing of data will be done at a large network of sites that will be made up of a variety of technologies. Mostly, no new high-performance computing centers will be created. Existing infrastructures, including public clouds will be used for the processing of data. Along with using FPGAs for pre-processing and possibly more processing afterwards, the SKA team plans to use GPU accelerators to allow for efficient processing.
Each team of researchers will have various goals that they will want from the data. This means that they will have a variety of processing needs, which will be carried out in SKA Regional Centers (SRCs). This might mean machine learning programs to get insights from the data, all the way to other mathematical operations to make the data ready for study. In any case, it is the expectation that this additional data is preserved as well, leading to even more data needing to be managed [12].&lt;/p>
&lt;h2 id="other-challenges">Other Challenges&lt;/h2>
&lt;p>While this data is not the most sensitive data on the planet, it is important that security is considered. The SKA team is planning on creating a sort of firewall between users and the actual HPC centers by using an AAAI (authorization, access, authentication, and identification) system. Security of proprietary data will be a concern that will have to be addressed. As there is a large team working on the project, as well as many external actors, security becomes extremely complex, especially the more access points there are to the data [12].
A project this large and versatile requires the use of many software tools. These software tools generally need some level or automatic communication if they are used together in a project. With a large number of tools, there becomes a complex IT infrastructure that needs to be managed, and constantly monitored. It is possible for one tool to receive a critical update, and then cause issues with integration of other software systems.&lt;/p>
&lt;h1 id="references">References&lt;/h1>
&lt;p>[1] &amp;ldquo;Square Kilometre Array - ICRAR&amp;rdquo;, ICRAR, 2020. [Online]. Available: &lt;a href="https://www.icrar.org/our-research/ska/">https://www.icrar.org/our-research/ska/&lt;/a>. [Accessed: 23- Sep- 2020].&lt;br>
[2] &amp;ldquo;What are Radio Telescopes? - National Radio Astronomy Observatory&amp;rdquo;, National Radio Astronomy Observatory, 2020. [Online]. Available: &lt;a href="https://public.nrao.edu/telescopes/radio-telescopes/">https://public.nrao.edu/telescopes/radio-telescopes/&lt;/a>. [Accessed: 23- Sep- 2020].&lt;br>
[3] &amp;ldquo;SKA Science - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href="https://www.skatelescope.org/science/">https://www.skatelescope.org/science/&lt;/a>. [Accessed: 24- Sep- 2020].&lt;br>
[4] &amp;ldquo;Galaxy Evolution, Cosmology and Dark Energy - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href="https://www.skatelescope.org/galaxyevolution/">https://www.skatelescope.org/galaxyevolution/&lt;/a>. [Accessed: 24- Sep- 2020].&lt;br>
[5] &amp;ldquo;Cosmic Magnetism - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href="https://www.skatelescope.org/magnetism/">https://www.skatelescope.org/magnetism/&lt;/a>. [Accessed: 24- Sep- 2020].&lt;br>
[6] &amp;ldquo;Probing the Cosmic Dawn - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href="https://www.skatelescope.org/cosmicdawn/">https://www.skatelescope.org/cosmicdawn/&lt;/a>. [Accessed: 24- Sep- 2020].&lt;br>
[7] L. Sierra, &amp;ldquo;Are we alone in the universe? Revisiting the Drake equation&amp;rdquo;, Exoplanet Exploration: Planets Beyond our Solar System, 2020. [Online]. Available: &lt;a href="https://exoplanets.nasa.gov/news/1350/are-we-alone-in-the-universe-revisiting-the-drake-equation/">https://exoplanets.nasa.gov/news/1350/are-we-alone-in-the-universe-revisiting-the-drake-equation/&lt;/a>. [Accessed: 24- Sep- 2020].&lt;br>
[8] &amp;ldquo;Design - ICRAR&amp;rdquo;, ICRAR, 2020. [Online]. Available: &lt;a href="https://www.icrar.org/our-research/ska/design/">https://www.icrar.org/our-research/ska/design/&lt;/a>. [Accessed: 24- Sep- 2020].&lt;br>
[9] &amp;ldquo;Africa - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href="https://www.skatelescope.org/africa/">https://www.skatelescope.org/africa/&lt;/a>. [Accessed: 24- Sep- 2020].&lt;br>
[10] Square Kilometre Array, Building a giant telescope in the outback - part 2. 2020.&lt;br>
[11] &amp;ldquo;Australia - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href="https://www.skatelescope.org/australia/">https://www.skatelescope.org/australia/&lt;/a>. [Accessed: 24- Sep- 2020].&lt;br>
[12] Filled in Use Case Survey for SKA&lt;/p></description></item><item><title>Report:</title><link>/report/fa20-523-305/homework6/cody_harris_hw6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-305/homework6/cody_harris_hw6/</guid><description>
&lt;h1 id="applying-computer-vision-to-medical-imaging">Applying Computer Vision to Medical Imaging&lt;/h1>
&lt;p>Computer vision technology has made great strides in the past decade. The most obvious proof of this statement comes from looking at early consumer image classification programs. Early programs from the early 2010s struggled to find the difference between a cat and a person. Now, consumer image classification programs can accurately tell the difference between two cats. With these improvements, there are great applications for computer vision to aid with radiology.&lt;/p>
&lt;h2 id="specific-application-areas">Specific Application Areas&lt;/h2>
&lt;p>The five most common modalities of medical imaging are: X-Rays, CT scans, MRI, ultrasound, and PET scan [1]. Within these major modalities, there are various special types of each imaging technique such as an fMRI, which is called a functional MRI. Then there are the more niche imaging techniques that are used. For example, Diffusion Tensor Imaging (DTI) is a technique that allows visualization of the white matter in the brain. One application of this imaging technique is coming up with a way to diagnose certain mental illness from imaging [2]. Seeing as mental health issues are typically tougher to diagnose, this would be a major breakthrough.&lt;/p>
&lt;p>Oncology seems to be a major area of study for computer vision in medical imaging. Logically this makes sense as cancers seems to create anomalies that can be seen in medical imaging. Thoracic imaging focuses on looking at the lungs, and computer vision could aid with finding anomalies that could lead to the early detection of cancer which in turn creates a better prognosis. An application that is only based on analyzing normal images or video, is analyzing a colonoscopy. Certain structures in the colon can create colorectal cancer if not correctly identified and classified as benign or malignant. Another imaging technique that can be rather difficult to analyze correctly are mammograms, and correctly identifying the various anomalies that are present as either malignant or benign [3].&lt;/p>
&lt;h2 id="how-computer-vision-can-be-used-in-medical-imaging">How Computer Vision can be used in Medical Imaging&lt;/h2>
&lt;p>In a perfect world, a sufficiently advanced AI could be the only entity to ever examine a certain medical image before providing a prognosis. In reality our technology is far from achieving this lofty goal. In the meantime, AI can still be used to improve radiologist workflows. In some studies, it was found that a radiologist would have to look at one image every three to four seconds to stay caught up with their workload in an 8-hour day. It is obvious why this could cause issues with accuracy. Now think about having the same time to look at an image, but instead of a raw image, the image comes with suggestions of diagnosis, and points to specific areas for the radiologist to focus on. This would improve the effectiveness of radiologists without relying completely on the AI model to be 100% accurate [3].&lt;/p>
&lt;h2 id="modeling-techniques">Modeling Techniques&lt;/h2>
&lt;p>There are two main techniques that are currently being employed to work with computer vision and medical imaging. The first technique is extra certain features from the image based on qualifications that are input to the system. For example, the user of the system might put in to extract the texture and shape of anomalies in the lower left lobe of the lungs as one of the features. Once all of these features are collected, they are fed into an expert system that selects the most promising features that could help with diagnosis. These selected features are fed into a machine learning classifier system that then sends its insights along with the image to the radiologist [3]. This system has itâ€™s draw backs that are typical of expert systems. First off, setting the system up and giving it the parameters for the expert system is extremely complicated, and incorrect parameters in the system could heavily affect the output.&lt;/p>
&lt;p>The second technique employs deep learning. Over the years, deep learning has become a widely used method to gain insights from data, and computer vision is no exception. The deep learning models have the benefit of not requiring any setup or expert systems. Really the biggest challenge is getting a good enough training data such that the deep learning model accurately predicts in the same way that a radiologist would. Some studies have been done on testing the accuracy of such methods and they found that â€œdeep learning technologies are on par with radiologistsâ€™ performance for both detection and segmentation tasks in ultrasonography and MRI, respectivelyâ€ [3].&lt;/p>
&lt;h2 id="special-considerations">Special Considerations&lt;/h2>
&lt;p>While deep learning seems to be the best method to create these systems, experts still need to be involved with the creation of these systems. One example of this is having expert radiologists evaluate training data. Just because there might be 30 years worth of data, that doesnâ€™t mean it all can be used. The medical field is constantly evolving and making sure that the data you train your model is relevant is an important part of creating any model. Also using radiologists to shape the software that is used by radiologists would always improve the end product [4]. Too often software is built by software engineers and data scientists and doesnâ€™t use enough advice from experts in the field, and this almost always is a detriment to the software.&lt;/p>
&lt;p>Radiologists using these deep learning tools, will require a great deal of training with these tools. They know that the AI model is not always going to be correct, and it is important that the radiologists understand how the software works so that they can make a determination of whether their opinion should be trusted over the output from the software, especially early on with newer technology [4].&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;p>[1] &amp;ldquo;Different Imaging Tests Explained | UVA Radiology&amp;rdquo;, UVA Radiology and Medical Imaging Blog for Patients, 2019. [Online]. Available: &lt;a href="https://blog.radiology.virginia.edu/different-imaging-tests-explained/">https://blog.radiology.virginia.edu/different-imaging-tests-explained/&lt;/a>. [Accessed: 20- Oct- 2020].&lt;/p>
&lt;p>[2] &amp;ldquo;Diffusion Tensor Imaging (DTI) | Psychiatry Neuroimaging Laboratory&amp;rdquo;, Pnl.bwh.harvard.edu, 2020. [Online]. Available: &lt;a href="http://pnl.bwh.harvard.edu/portfolio-item/diffusion-tensor-imaging-dti/">http://pnl.bwh.harvard.edu/portfolio-item/diffusion-tensor-imaging-dti/&lt;/a>. [Accessed: 20- Oct- 2020].&lt;/p>
&lt;p>[3] A. Hosny, C. Parmar, J. Quackenbush, L. Schwartz and H. Aerts, &amp;ldquo;Artificial intelligence in radiology&amp;rdquo;, Nature Reviews Cancer, vol. 18, no. 8, pp. 500-510, 2018. Available: 10.1038/s41568-018-0016-5 [Accessed 20 October 2020].&lt;/p>
&lt;p>[4] &amp;ldquo;AI and the Future of Radiology&amp;rdquo;, Diagnostic Imaging, 2020. [Online]. Available: &lt;a href="https://www.diagnosticimaging.com/view/ai-and-future-radiology">https://www.diagnosticimaging.com/view/ai-and-future-radiology&lt;/a>. [Accessed: 20- Oct- 2020].&lt;/p></description></item><item><title>Report:</title><link>/report/fa20-523-305/project/project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-305/project/project/</guid><description>
&lt;h1 id="estimating-soil-moisture-content-using-weather-data">Estimating Soil Moisture Content Using Weather Data&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-305/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-305/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-305/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-305/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Cody Harris, &lt;a href="mailto:harrcody@iu.edu">harrcody@iu.edu&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-305">fa20-523-305&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-305/edit/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>As the world is gripped with finding solutions to problems such as food and water shortages, the study of agriculture could improve where we stand with both of these problems. By integrating weather and sensor data, a model could be created to estimate soil moisture based on weather data that is easily accessible. While some farmers could afford to have many moisture sensors and monitor them, many would not have the funds or resources to keep track of the soil moisture long term. A solution would be to allow farmers to contract out a limited study of their land using sensors and then this model would be able to predict soil moistures from weather data. This collection of data, and predictions could be used on their own or as a part of a larger agricultural solution.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background">2. Background&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-datasets">3. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-data-cleaning-and-aggregation">4. Data Cleaning and Aggregation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-pipeline-for-preprocessing">5. Pipeline for Preprocessing&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-loading-and-joining-data">5.1 Loading and Joining Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-feature-engineering">5.2 Feature Engineering&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53-generic-pipeline">5.3 Generic Pipeline&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-multiple-models-for-multiple-soil-depths">6. Multiple Models for Multiple Soil Depths&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-splitting-data-into-train-and-test">7. Splitting Data into Train and Test&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-preliminary-analysis-and-eda">8. Preliminary Analysis and EDA&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-initial-model-testing-regressor">9. Initial Model Testing (Regressor)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#10-classifier-vs-regressor">10. Classifier vs. Regressor&lt;/a>&lt;/li>
&lt;li>&lt;a href="#11-various-other-linear-regression-model-experiments">11. Various Other Linear Regression Model Experiments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#12-other-models">12. Other Models&lt;/a>&lt;/li>
&lt;li>&lt;a href="#13-conclusion">13. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#14-acknowledgements">14. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#15-references">15. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> agriculture, soil moisture, IoT, machine learning, regression, sklearn&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Maintaining correct soil moisture throughout the plant growing process can result in better yields, and less overall problems with the crop. Water deficiencies or surplus at various stages of growth have different effects, or even negligible effects &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. It is important to have an idea of how your land consumes and stores water, which could be very different based on the plants being used, and variation of elevation and geography.&lt;/p>
&lt;p>For hundreds of years, farmers have done something similar to this model. The difference is the precision that we can gain by using real data. For the past few hundred years, farmers had to rely on mostly experience and touch to know the moisture of their soil. While many farmers were successful, in the sense that they produced crops, there were ways they could have better optimized their crops to produce better. The water available to the plants is not the only variable that effects yields, but this project seeks to create an accessible model to which farmers can have predicted values of soil moisture without needing to buy and deploy expensive sensors.&lt;/p>
&lt;p>The model created could be used in various ways. The first main use is to be able to monitor what is currently happening in the soil so that changes can be made to correct the issue if there is one. Secondly, a farmer could evaluate historical data and compare it to yields or other results of the harvest and use this analytical information to inform future decisions. For example, a corn farmer might only care about the predicted conditions to make sure that they are within reasonable ranges. A grape farmer in a wine vineyard might use this data, along with other data, to predict the quality of wine or even the recipe of wine that would best used grapes farmed under these conditions. Again, this model is just the starting point of a theoretical complex agricultural data analysis suite.&lt;/p>
&lt;p>This project specifically seeks to see the effect of weather on a particular piece of land in Washington state. This process could be done all over the world to obtain benchmarks. These benchmarks could be a cheap option for a farmer that does not have the funds to support a full study of water usage on their land to use as training data. Instead, they could look for a model that has land that has similar soil and or geographical features, and then use their own weather data to estimate their soil moisture content. A major goal of this project is to create the best tool that is cheap enough for widespread adoption.&lt;/p>
&lt;h2 id="2-background">2. Background&lt;/h2>
&lt;p>Understanding how weather impacts soil moisture is something that has been studied in various ways, all because it is a driving factor in crop success. Multiple studies have sought to apply a deterministic approach to calculating soil moisture based on observational weather data.&lt;/p>
&lt;p>One such study, was motivated by trying to predict dust storms in China, in which soil moisture plays a large role in. This prediction used multiple-linear regression, and focused on predictions that dealt with the top 10 cm of soil. Two key takeaways can be derived from this work that are beneficial for carrying out this project.&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;The influence of precipitation on surface soil moisture content does not last more than 16 days.&amp;rdquo;&lt;/li>
&lt;li>&amp;ldquo;The compound effect of the ratio of precipitation to evaporation, which is nonlinearly summed, can be used to calculate the surface soil moisture content in China&amp;rdquo; &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/li>
&lt;/ul>
&lt;p>Moving forward, this project will assume that precipitation from the prior 16 days is relevant. In the case that for the specific data being fit, less days are relevant, then their coefficients in the model will likely become small enough to not affect the model. Secondly, soil moisture is influenced by a ratio or precipitation to evaporation. While this project might not seek to evaluate this relationship directly, it will seek to include data that would influence these ratios such as temperature, time of year, and wind speeds.&lt;/p>
&lt;p>Multiple publications have sought to come up with complete hydrological models to determine soil moisture from a variety of factors. These models are generally stochastic in nature and are reliable predictors when many parameters of the model are available. One such cited model requires a minimum or 19 variables or measured coefficients &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The authors of another study note the aforementioned study, as well as other similar studies, and make a point that these methods might not be the best models when it comes to practical applications. Their solution was to create a generalize model that relied mostly on soil moisture as &amp;ldquo;a function of the time-weighted average of previous cumulative rainfall over a period&amp;rdquo; &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Such a model is closer in terms to simplicity and generalization to what is hoped to be accomplished in this project.&lt;/p>
&lt;p>The relationship between soil moisture and weather patterns is one with a rich history of study. Both of these measures affect each other in various ways. Most studies that sought to quantify this relationship were conducted at a time in which large scale sensor arrays could not have been implemented in the field. With the prevalence of IoT and improved sensing technologies, it seems as though there might not be a need to use predictive models for soil moisture, but instead just use sensor data. While this could be true in some applications, a wide array of challenges occur when trying to maintain these sensor arrays. Problems such as charging or replacing batteries, sensor and relay equipment not working if completely buried, but are in the way of farming if mounted above ground, sensors failing, etc. These were real challenges faced by the farm in which the soil moisture data was collected &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. The objective of this project is to create predictive models based on limited training data so that farmers would not need to deal with sensor arrays indefinitely.&lt;/p>
&lt;h2 id="3-datasets">3. Datasets&lt;/h2>
&lt;p>The first data set comes from NOAA and contains daily summary data in regards to various measurements such as temperature, precipitation, wind speed, etc. For this project, only data that came from the closest station to the field will be used &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. In this case, that is the Pullman station at the Pullman-Moscow airport. Below is an image showing the weather data collection location, and the red pin is at the longitude and latitude of one of the sensors in the field. This data is in csv format (see Figure 1).&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/main/project/images/distance_map.png" alt="Figure 1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Estimated distance from weather reports to the crop fields. Distance is calculated using Google Maps&lt;/p>
&lt;p>The second dataset comes from the USDA. This dataset consists of &amp;ldquo;hourly and daily measurements of volumetric water content, soil temperature, and bulk electrical conductivity, collected at 42 monitoring locations and 5 depths (30, 60, 90, 120, and 150 cm)&amp;rdquo; at a farm in Washington state &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Mainly, the daily temperature and water content are the measurements of interest. There are multiple files that have data that corresponds to what plants are being grown in specific places, and the makeup of the soil at each sensor cite. This auxilary information could be used in later models once the base model has been completed. This data is in tab delimited files.&lt;/p>
&lt;p>Within the data, there are GIS file types that can be imported into Google Maps desktop to visualize the locations of the sensors and other geographical information. Below is an example of the sensor locations plotted on the satellite image (see Figure 2).&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/main/project/images/sensor_locations.png" alt="Figure 2">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Location of sensors within the test field&lt;/p>
&lt;h2 id="4-data-cleaning-and-aggregation">4. Data Cleaning and Aggregation&lt;/h2>
&lt;p>The first step is to get the soil moisture data into a combined format, currently it is in one file per sensor, and there are 42 sensors. See the &lt;a href="https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/code/ml_pipeline.ipynb">ml_pipeline.ipynb&lt;/a> file to see how this was done, specifically the section titled &amp;ldquo;Data Processing&amp;rdquo;. After aggregation, some basic information can be checked about the data. For instance, there is quite a bit of NAs in the data. These NAs are just instances where there was no measurement on that day. There is about 45% NAs in the measurement columns. To further clean the data, any row that has only NAs for the measurements will be removed.&lt;/p>
&lt;p>Next, the weather data needs some small adjustments. This is mostly in the form of removing columns that either are empty or have redundant data such as elevation, which is the same for every row.&lt;/p>
&lt;p>Once the data is sufficiently clean, some choices have to be made on joining the data. The simplest route would be to join the weather measurements directly with the same day the soil measurement, however, the previous days weather is likely to also have an impact on the moisture. As evaluated in section 2 above, it is believed that the prior 16 days weather data is what is needed for a good prediction.&lt;/p>
&lt;h2 id="5-pipeline-for-preprocessing">5. Pipeline for Preprocessing&lt;/h2>
&lt;p>Before feeding the data through a machine learning algorithm, the data needs to be manipulated in such a way that it is ready to be directly fed into an algorithm. This includes joining the two data sets, feature engineering, and other tasks that prepare the data. This will need to be done every time a new dataset is being used, so this must be built in a repeatable way. The machine learning library scikit-learn incorporates something called &amp;ldquo;pipelines&amp;rdquo; that can allow processed to be sequentially done to a dataframe. For purposes of this project two pipelines will be built, one will be used for feature engineering and joining the data, the other will be used to handle preparation of numerical, categorical, and date data. See sections: &amp;ldquo;Data Processing Pipeline&amp;rdquo; in &lt;a href="https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/code/ml_pipeline.ipynb">ml_pipeline.ipynb&lt;/a>.&lt;/p>
&lt;h3 id="51-loading-and-joining-data">5.1 Loading and Joining Data&lt;/h3>
&lt;p>This is the first step of the entire pipeline. This is where both the weather, and the soil moisture data are read in from csv files in their raw format. The soil moisture data is found in many different files, and these all need to be combined. After combining the files, any lines that are full of NAs for the measurements are dropped. Next the weather data is loaded in. Both files have a date field which is the field they will be joined on. To make things consistent, both of these fields need to set be date format.&lt;/p>
&lt;p>When it comes to joining the data, each row should include the moisture content at various depths, as well as the weather information from the past ten days. While this creates a great deal of redundant data, the data is small enough that this is not an issue. Experiments will be done to evaluate just how many days of prior weather data are needed to form accurate results, while trying to minimize the number of the days.&lt;/p>
&lt;h3 id="52-feature-engineering">5.2 Feature Engineering&lt;/h3>
&lt;p>Currently only two features are added, the first is a boolean flag that says whether it rained or not on a certain day. The thought behind this is, that for some days prior to the current measurement, the amount of rain might be needed, but for other days, such as 10 days prior, it might be more important to just know if there was rain or not. This feature is engineered within the pipeline.&lt;/p>
&lt;p>The next feature is a categorical feature that is the month of the year. It isn&amp;rsquo;t very import to know the exact date of a measurement, but the month might be helpful in a model. This simplifies the model by not using date as a predictor, while still being able to capture this potentially important feature.&lt;/p>
&lt;p>An excerpt of the code used to create these two features, this comes from &lt;a href="https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/code/ml_pipeline.ipynb">ml_pipeline.ipynb&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#000">soil&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;Month&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">pd&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">DatetimeIndex&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">soil&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;Date&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">])&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">month&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">for&lt;/span> &lt;span style="color:#000">i&lt;/span> &lt;span style="color:#204a87;font-weight:bold">in&lt;/span> &lt;span style="color:#204a87">range&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">17&lt;/span>&lt;span style="color:#000;font-weight:bold">):&lt;/span>
&lt;span style="color:#000">col_name&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;PRCP_&amp;#39;&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">+&lt;/span> &lt;span style="color:#204a87">str&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">i&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">rain_y_n_name&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;RAIN_Y_N_&amp;#39;&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">+&lt;/span> &lt;span style="color:#204a87">str&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">i&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">X&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#000">rain_y_n_name&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">np&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">nan&lt;/span>
&lt;span style="color:#000">X&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#000">rain_y_n_name&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">loc&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#000">X&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#000">col_name&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>
&lt;span style="color:#000">X&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#000">rain_y_n_name&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">loc&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#000">X&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#000">col_name&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">==&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;span style="color:#000">X&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#000">rain_y_n_name&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">X&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#000">rain_y_n_name&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">astype&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;object&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="53-generic-pipeline">5.3 Generic Pipeline&lt;/h3>
&lt;p>After doing operations that are specific to the current dataset, some built in processors from sklearn are used to make sure the data can be used in a machine learning model. This means that for numerical data types, the pipeline will fill in missing values with 0 instead of leaving them as NaN. Also, the various numerical fields must be standardized, this is important for models such as linear regression so one large variable isn&amp;rsquo;t dominating the model.&lt;/p>
&lt;p>As far as text and categorical features, the imputer will be used to fill in missing data as well. Then a process called one hot encoding will be used to handle the categorical variables so that they can be read into sklearns estimators. Lastly, these two main processes will be put together to make a single pipeline step. Then this pipeline step will be added to a regressor of some sort to create the entire process.&lt;/p>
&lt;h2 id="6-multiple-models-for-multiple-soil-depths">6. Multiple Models for Multiple Soil Depths&lt;/h2>
&lt;p>There are a few different approaches for modeling for this particular problem. The issue is that we have multiple things we would like to predict with the same predictors. It is unlikely that the model that predicts for a depth of 30 cm, would accurately predict for a depth of 150 cm. In order to adjust the models, a separate model will be created for each depth, with that said, the predictors are all the same for each depth, but the trained output is different. To accomplish this, five different datasets were constructed, each one representing a depth. All rows in which the predicted value is not available for that depth were pruned from the dataset.&lt;/p>
&lt;p>In each experiment, there will be 5 different models created. Initially, these 5 models will use the same hyper-parameters for all the depths. It might turn out that all the models will need the same hyper-parameters, or each soil depth could be different. This will be examined through experimentation.&lt;/p>
&lt;h2 id="7-splitting-data-into-train-and-test">7. Splitting Data into Train and Test&lt;/h2>
&lt;p>In order to test any model created, there must be a split between test and training data. This is done by using a function in sklearn. In this case, there are about 76k rows in the data set. For the training data, 80% of the total data will be used, or about 60.8k records. The split is done after shuffling the rows so that it does not just pick the top 80% every time. Lastly the data is split using a stratified method. As we want to have models that take the specific area of the field into account, that means that we need to have the different areas of the field represented equally in both the training and testing dataset. This means that if 10% of the data came from sensor CAF0003, then roughly 10% of the training data will come from CAF0003 as well as 10% of the test data will be from this location.&lt;/p>
&lt;h2 id="8-preliminary-analysis-and-eda">8. Preliminary Analysis and EDA&lt;/h2>
&lt;p>Before building a machine learning model, it is important to get a general idea of how the data looks, to see if any insights can be made right away. The actual visualizations were built using a python package called Altair. This created the visualizations well, but the actual notebook that would contain these images was too large to include in their entirety.&lt;/p>
&lt;p>The first two visualizations (&lt;a href="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/main/project/images/one.png">viz_1&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/images/two.png">viz_2&lt;/a>) are grids that show the entire distribution of measurements across each sensor. The first grid is the volume of water at 30 cm, and the second grid is the water volume at 150 cm. Each chart could be looked at and examined on it&amp;rsquo;s own, but what is most important to note is the variability of the measures from location to location. These different sensors are not that far away, but show that different areas of the farm do retain water in different ways. See Figure 3 for a small section of the grid from the visualization on the sensors at 30cm.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/main/project/images/one_small.png" alt="Figure 3">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Six locations soil moisture level over time at 30 cm depth&lt;/p>
&lt;p>The third and fourth grid shows the temperature at 150 cm, the results are what would logically be expected. The different sensors do not show much variance from location to location.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/main/project/images/four_small.png" alt="Figure 4">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Six locations soil temperature over time at 150 cm depth&lt;/p>
&lt;h2 id="9-initial-model-testing-regressor">9. Initial Model Testing (Regressor)&lt;/h2>
&lt;p>Once the pipelines were setup, the first model could be tested for accuracy. As the output data is continuous in nature, the easiest machine learning algorithm to test to make sure everything is correct, was a linear regression model. It seems fairly likely that a linear regression model would do rather well with this data. The weather is the driving factor in soil moisture in a non-irrigated field, so this test is a litmus test to make sure that the data is good and provide a baseline measurement for future models. The experiment log below shows the returned values from the test that was run. Over the course of experimentation, a log such as this will be kept.&lt;/p>
&lt;p>The results are as follows:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Experiment&lt;/th>
&lt;th>Depth&lt;/th>
&lt;th>Fit_Time&lt;/th>
&lt;th>Pred_Time&lt;/th>
&lt;th>r2_score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>First Linear Reg&lt;/td>
&lt;td>30cm&lt;/td>
&lt;td>2.029387&lt;/td>
&lt;td>0.169824&lt;/td>
&lt;td>9.16E-01&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>First Linear Reg&lt;/td>
&lt;td>60cm&lt;/td>
&lt;td>2.002373&lt;/td>
&lt;td>0.17377&lt;/td>
&lt;td>-1.42E+15&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>First Linear Reg&lt;/td>
&lt;td>90cm&lt;/td>
&lt;td>2.080393&lt;/td>
&lt;td>0.162992&lt;/td>
&lt;td>9.49E-01&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>First Linear Reg&lt;/td>
&lt;td>120cm&lt;/td>
&lt;td>2.299457&lt;/td>
&lt;td>0.18056&lt;/td>
&lt;td>9.46E-01&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>First Linear Reg&lt;/td>
&lt;td>150cm&lt;/td>
&lt;td>2.573193&lt;/td>
&lt;td>0.186042&lt;/td>
&lt;td>9.43E-01&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Baseline experiment results&lt;/p>
&lt;p>These results show that the data is pretty well correlated and that there is reason to believe that we could predict soil moisture from weather alone. Although an r^2 of around 0.916-0.949 are pretty good, with such highly related predictors, there is definitely room for model improvement. Also for a depth of 60 cm, something is not predicting correctly and is resulting in a small negative r^2&lt;/p>
&lt;h2 id="10-classifier-vs-regressor">10. Classifier vs. Regressor&lt;/h2>
&lt;p>While the output is continuous, there is an argument to use a categorical classifier model. For a specific plant, an optimal moisture range could be studied. For examples sake, the range could be 0.2-0.4 units. Then it would not matter if the soil is 0.2 or 0.3, both would be in the acceptable range. With this in mind, certain levels could be created to alert the farmer of which category they could be experiencing. For example there might be five levels: too dry, acceptable dryness, optimal, acceptable wetness, and too wet. The training data could be adjusted to fit into these categories.&lt;/p>
&lt;p>Code to create a categorical variable for each of the depth measurements can be found in the section &amp;ldquo;Make Classifier Label&amp;rdquo; in the file: &lt;a href="https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/code/ml_pipeline.ipynb">ml_pipeline.ipynb&lt;/a>.&lt;/p>
&lt;p>In the end, the decision to not use classifier methods was made. After using a regressor, the output could be converted to a categorical feature if the user or application so desired this. As our output is continuous in nature, precision would be lost.&lt;/p>
&lt;h2 id="11-various-other-linear-regression-model-experiments">11. Various Other Linear Regression Model Experiments&lt;/h2>
&lt;p>The next set of experiments came up with the results in the following table. This was a test to see if baseline Lasso or Ridge Regression would improve on the basic linear regression model. Results and code for this portion can be found in &lt;a href="https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/code/ml_pipeline.ipynb">ml_pipeline.ipynb&lt;/a> under the &amp;ldquo;Linear Regression Tests&amp;rdquo; section.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Experiment&lt;/th>
&lt;th>Depth&lt;/th>
&lt;th>Fit_Time&lt;/th>
&lt;th>Pred_Time&lt;/th>
&lt;th>r2_score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Ridge Reg, Alpha = 1&lt;/td>
&lt;td>30cm&lt;/td>
&lt;td>1.321553&lt;/td>
&lt;td>0.173714&lt;/td>
&lt;td>0.916211&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ridge Reg, Alpha = 1&lt;/td>
&lt;td>60cm&lt;/td>
&lt;td>1.29167&lt;/td>
&lt;td>0.187392&lt;/td>
&lt;td>0.942757&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ridge Reg, Alpha = 1&lt;/td>
&lt;td>90cm&lt;/td>
&lt;td>1.393526&lt;/td>
&lt;td>0.197152&lt;/td>
&lt;td>0.94879&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ridge Reg, Alpha = 1&lt;/td>
&lt;td>120cm&lt;/td>
&lt;td>1.307926&lt;/td>
&lt;td>0.176656&lt;/td>
&lt;td>0.946032&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ridge Reg, Alpha = 1&lt;/td>
&lt;td>150cm&lt;/td>
&lt;td>1.33738&lt;/td>
&lt;td>0.179585&lt;/td>
&lt;td>0.94332&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Lasso Reg, Alpha = 1&lt;/td>
&lt;td>30cm&lt;/td>
&lt;td>1.45102&lt;/td>
&lt;td>0.170752&lt;/td>
&lt;td>-0.00018&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Lasso Reg, Alpha = 1&lt;/td>
&lt;td>60cm&lt;/td>
&lt;td>1.419546&lt;/td>
&lt;td>0.174177&lt;/td>
&lt;td>-4.6E-05&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Lasso Reg, Alpha = 1&lt;/td>
&lt;td>90cm&lt;/td>
&lt;td>1.4632&lt;/td>
&lt;td>0.176657&lt;/td>
&lt;td>-5.7E-06&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Lasso Reg, Alpha = 1&lt;/td>
&lt;td>120cm&lt;/td>
&lt;td>1.553091&lt;/td>
&lt;td>0.182349&lt;/td>
&lt;td>-1.1E-06&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Lasso Reg, Alpha = 1&lt;/td>
&lt;td>150cm&lt;/td>
&lt;td>1.437419&lt;/td>
&lt;td>0.163967&lt;/td>
&lt;td>-0.00018&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ridge Reg - GSCV&lt;/td>
&lt;td>30cm&lt;/td>
&lt;td>3.914718&lt;/td>
&lt;td>0.203007&lt;/td>
&lt;td>0.916235&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ridge Reg - GSCV&lt;/td>
&lt;td>60cm&lt;/td>
&lt;td>3.726651&lt;/td>
&lt;td>0.172752&lt;/td>
&lt;td>0.942757&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ridge Reg - GSCV&lt;/td>
&lt;td>90cm&lt;/td>
&lt;td>4.135154&lt;/td>
&lt;td>0.200589&lt;/td>
&lt;td>0.948796&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ridge Reg - GSCV&lt;/td>
&lt;td>120cm&lt;/td>
&lt;td>4.03203&lt;/td>
&lt;td>0.193512&lt;/td>
&lt;td>0.946032&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ridge Reg - GSCV&lt;/td>
&lt;td>150cm&lt;/td>
&lt;td>4.361977&lt;/td>
&lt;td>0.191296&lt;/td>
&lt;td>0.943328&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Further Linear Regression Experiment Results&lt;/p>
&lt;p>For the first two experiments, an alpha of 1 was used for both ridge and lasso regression. The third experiment used a special regressor that uses cross validation to try to find the best alpha value and then fit the model based on that. The best alpha value seemed to not have much effect at all on the results. Still the ridge regression so far was the best performing model.&lt;/p>
&lt;h2 id="12-other-models">12. Other Models&lt;/h2>
&lt;p>While there were great results in the different linear regression models, other models should be evaluated to make sure that something is not missed. Three models were chosen to check, Stochastic Gradient Descent, Support Vector Machine, and Random Forest. All of these models were tested with default parameters and their results are shown below in Figure 7, and the code can be found in the section called &amp;ldquo;Other Regressors Tests&amp;rdquo;.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Experiment&lt;/th>
&lt;th>Depth&lt;/th>
&lt;th>Fit_Time&lt;/th>
&lt;th>Pred_Time&lt;/th>
&lt;th>r2_score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Random Forest&lt;/td>
&lt;td>30cm&lt;/td>
&lt;td>60.06952&lt;/td>
&lt;td>0.250543&lt;/td>
&lt;td>0.977118&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Random Forest&lt;/td>
&lt;td>60cm&lt;/td>
&lt;td>62.17435&lt;/td>
&lt;td>0.216641&lt;/td>
&lt;td>0.989113&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Random Forest&lt;/td>
&lt;td>90cm&lt;/td>
&lt;td>62.29475&lt;/td>
&lt;td>0.243051&lt;/td>
&lt;td>0.99158&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Random Forest&lt;/td>
&lt;td>120cm&lt;/td>
&lt;td>64.48227&lt;/td>
&lt;td>0.256666&lt;/td>
&lt;td>0.991274&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Random Forest&lt;/td>
&lt;td>150cm&lt;/td>
&lt;td>68.47001&lt;/td>
&lt;td>0.240149&lt;/td>
&lt;td>0.991748&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SVM&lt;/td>
&lt;td>30cm&lt;/td>
&lt;td>38.83822&lt;/td>
&lt;td>5.712513&lt;/td>
&lt;td>0.676934&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SVM&lt;/td>
&lt;td>60cm&lt;/td>
&lt;td>106.2816&lt;/td>
&lt;td>7.897556&lt;/td>
&lt;td>0.766008&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SVM&lt;/td>
&lt;td>90cm&lt;/td>
&lt;td>102.9438&lt;/td>
&lt;td>7.763206&lt;/td>
&lt;td>0.788833&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SVM&lt;/td>
&lt;td>120cm&lt;/td>
&lt;td>79.76476&lt;/td>
&lt;td>6.985236&lt;/td>
&lt;td>0.760895&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SVM&lt;/td>
&lt;td>150cm&lt;/td>
&lt;td>96.46352&lt;/td>
&lt;td>7.548365&lt;/td>
&lt;td>0.760936&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SGD&lt;/td>
&lt;td>30cm&lt;/td>
&lt;td>1.382992&lt;/td>
&lt;td>0.171777&lt;/td>
&lt;td>0.89019&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SGD&lt;/td>
&lt;td>60cm&lt;/td>
&lt;td>1.392753&lt;/td>
&lt;td>0.15128&lt;/td>
&lt;td>0.931394&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SGD&lt;/td>
&lt;td>90cm&lt;/td>
&lt;td>1.399587&lt;/td>
&lt;td>0.142493&lt;/td>
&lt;td>0.941092&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SGD&lt;/td>
&lt;td>120cm&lt;/td>
&lt;td>1.438626&lt;/td>
&lt;td>0.150302&lt;/td>
&lt;td>0.936692&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SGD&lt;/td>
&lt;td>150cm&lt;/td>
&lt;td>1.403488&lt;/td>
&lt;td>0.14933&lt;/td>
&lt;td>0.92957&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Further Linear Regression Experiment Results&lt;/p>
&lt;p>The random forest regressor performed amazingly in predicting the soil moisture. While the lower depths of soil did perform better than the depth of 30 cm. As random forests performed so well out of the box, some attempts were made to tune the hyperparameters, but most experiments turned out to be computationally expensive.&lt;/p>
&lt;h2 id="13-conclusion">13. Conclusion&lt;/h2>
&lt;p>The end results of all experimentation was a process in which two datasets could be joined and fed into a model to predict the soil moisture with great accuracy, an r^2 score of between 0.977 and 0.991 depending on the depth using a Random Forest Regressor with default settings. This process could be a repeatable process in which a farmer contracts a company to gather training data on their land specifically for a growing season. As the collection of the sensor data could be cumbersome and expensive to deal with as a farmer, so this is an alternative that is cheaper and still gives nearly the same results as having sensors constantly running. Alternatively, this process could be a subprocess in a larger suite of software that farmers could use for predictive analysis or even to have data on soil moisture from a grow season to use in post season analysis of their crop produced. As long as large scale AI programs are still expensive and cumbersome for farmers to deal with, there will be a low rate of adoption. This project has shown that a solution for large scale soil moisture prediction software could be done with relatively low computational cost.&lt;/p>
&lt;h2 id="14-acknowledgements">14. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="15-references">15. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>O. Denmead and R. Shaw, &amp;ldquo;The Effects of Soil Moisture Stress at Different Stages of Growth on the Development and Yield of Corn 1&amp;rdquo;, Agronomy Journal, vol. 52, no. 5, pp. 272-274, 1960. Available: 10.2134/agronj1960.00021962005200050010x. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>K. Shang, S. Wang, Y. Ma, Z. Zhou, J. Wang, H. Liu and Y. Wang, &amp;ldquo;A scheme for calculating soil moisture content by using routine weather data&amp;rdquo;, Atmospheric Chemistry and Physics, vol. 7, no. 19, pp. 5197-5206, 2007 [Online]. Available: &lt;a href="https://hal.archives-ouvertes.fr/hal-00302825/document">https://hal.archives-ouvertes.fr/hal-00302825/document&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>W. Capehart and T. Carlson, &amp;ldquo;Estimating near-surface soil moisture availability using a meteorologically driven soil-water profile model&amp;rdquo;, Journal of Hydrology, vol. 160, no. 1-4, pp. 1-20, 1994 [Online]. Available: &lt;a href="https://tinyurl.com/yxjyuy5x">https://tinyurl.com/yxjyuy5x&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>F. Pan, C. Peters-Lidard and M. Sale, &amp;ldquo;An analytical method for predicting surface soil moisture from rainfall observations&amp;rdquo;, Water Resources Research, vol. 39, no. 11, 2003 [Online]. Available: &lt;a href="https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2003WR002142">https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2003WR002142&lt;/a>. [Accessed: 08- Nov- 2020] &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>C. Gasch, D. Brown, C. Campbell, D. Cobos, E. Brooks, M. Chahal and M. Poggio, &amp;ldquo;A Field-Scale Sensor Network Data Set for Monitoring and Modeling the Spatial and Temporal Variation of Soil Water Content in a Dryland Agricultural Field&amp;rdquo;, Water Resources Research, vol. 53, no. 12, pp. 10878-10887, 2017 [Online]. Available: &lt;a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2017WR021307">https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2017WR021307&lt;/a>. [Accessed: 08- Nov- 2020] &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>N. (NCEI), &amp;ldquo;Climate Data Online (CDO) - The National Climatic Data Center&amp;rsquo;s (NCDC) Climate Data Online (CDO) provides free access to NCDC&amp;rsquo;s archive of historical weather and climate data in addition to station history information. | National Climatic Data Center (NCDC)&amp;rdquo;, Ncdc.noaa.gov, 2020. [Online]. Available: &lt;a href="https://www.ncdc.noaa.gov/cdo-web/">https://www.ncdc.noaa.gov/cdo-web/&lt;/a>. [Accessed: 19- Oct- 2020]. &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>&amp;ldquo;Data from: A field-scale sensor network data set for monitoring and modeling the spatial and temporal variation of soil moisture in a dryland agricultural field&amp;rdquo;, USDA: Ag Data Commons, 2020. [Online]. Available: &lt;a href="https://data.nal.usda.gov/dataset/data-field-scale-sensor-network-data-set-monitoring-and-modeling-spatial-and-temporal-variation-soil-moisture-dryland-agricultural-field">https://data.nal.usda.gov/dataset/data-field-scale-sensor-network-data-set-monitoring-and-modeling-spatial-and-temporal-variation-soil-moisture-dryland-agricultural-field&lt;/a>. [Accessed: 19- Oct- 2020]. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-305/test/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-305/test/</guid><description>
&lt;p>Testing if I have write access to this repo.&lt;/p></description></item><item><title>Report:</title><link>/report/fa20-523-307/assignment6/assignment6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-307/assignment6/assignment6/</guid><description>
&lt;h1 id="ai-in-precision-medicine">AI in Precision Medicine&lt;/h1>
&lt;p>In recent years, precision medicine has started to become the new standard when it comes to healthcare. This is moving us from a one size fits all approach to a more personal, data-driven approach that allows hospitals and treatment centers to spend more efficiently and have a higher patient outcome. Precision medicine is using knowledge that is specific to one patient, such as biomarkers, rather than the generic approach to an issue. The overall goal is to &amp;quot;design and optimize the pathway for diagnosis and prognosis through the use of large multidimensional biological datasets that capture different variables such as genes&amp;quot; [1].&lt;/p>
&lt;p>Artificial intelligence (AI) has been increasingly growing in business, society and now is emerging in healthcare. The potential that AI has can completely transform patient care. These technologies can perform to or exceed human capability when it comes to different medical tasks such as cancer diagnosis or disease diagnosis as well as patient engagement and administration tasks. AI has the potential to offer automated care to individuals by providing precision medicine.&lt;/p>
&lt;p>Precision medicine enables patients to not only recover from illnesses faster but to also stay healthy longer. However, with the increased use of precision medicine new challenges arise such as the increasing amount of data, a lack of specialists and ever increasing drug development costs. &amp;quot;Healthcare data is projected to grow by 43 percent by 2020, to roughly 2.3 zettabytes. The size of the data is not the only problem; it's the kind of data as well. Eighty percent of it is unstructured and mostly unlabeled, making it hard to extract value from the datasets&amp;quot; [2].&lt;/p>
&lt;p>Artificial intelligence (AI) has helped reshape how precision medicine is distributed. AI is able to solve many of the problems that have arisen. For big data challenges, AI methods are able to clear up obstacles that large and unstructured data present. In medical imaging, machine learning can be introduced to help classify what type of issue is present by training a model over thousands of images and predicting on the patient's image. Neural networks have also been able to make predictions when it comes to precision medicine.&lt;/p>
&lt;p>Neural networks are a more advanced form of AI. The uses in precision medicine is for categorisation applications such as the likelihood of a patient developing a disease. Neural networks look at problems from inputs, outputs, and weights of features to try and associate inputs with the corresponding outputs. &amp;quot;It has been likened to the way that neurons process signals, but the analogy to the brain's function is relatively weak&amp;quot; [3].&lt;/p>
&lt;p>Deep learning is one of the most complex forms of AI. This involves hundreds or thousands of models with numerous levels of features that are needed to predict the outcomes. Precision medicine takes advantage of this technology through the &amp;quot;recognition of potentially cancerous lesions in radiology images&amp;quot; [4]. Deep learning is able to be applied to fields such as radiomics. This is the practice of detecting features in image data that cannot be detected with the human eye. &amp;quot;Their combination appears to promise greater accuracy in diagnosis than the previous generation of automated tools for image analysis, known as computer-aided detection or CAD&amp;quot; [4].&lt;/p>
&lt;p>AI plays a pivotal role in the future of healthcare. In the development of precision medicine, it is one of the primary components in order to advance care for patients. Efforts to help classify medical imagery more quickly and accurately have proven more effective with the amount of data used to train such models. A big challenge that AI is facing in precision medicine is whether or not this technology will be widely adopted. These systems will need to have some regulations in order to have a universal standard. This will allow doctors and medical personnel to train with this technology so they will be able to provide the care their patients deserve. AI will never replace the human aspect of precision medicine but over time AI will be able to make the jobs and lives of the doctors and patients better and healthier.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;p>[1] M. Uddin, Y. Wang, and M. Woodbury-Smith, &amp;quot;Artificial intelligence for precision medicine in neurodevelopmental disorders,&amp;quot; &lt;em>Nature News&lt;/em>, 21-Nov-2019. [Online]. Available: &lt;a href="https://www.nature.com/articles/s41746-019-0191-0">https://www.nature.com/articles/s41746-019-0191-0&lt;/a>. [Accessed: 11-Oct-2020].&lt;/p>
&lt;p>[2] H. Chamraj, &amp;quot;Powering Precision Medicine with Artificial Intelligence,&amp;quot; &lt;em>Intel&lt;/em>. [Online]. Available: &lt;a href="https://www.intel.com/content/www/us/en/artificial-intelligence/posts/powering-precision-medicine-artificial-intelligence.html">https://www.intel.com/content/www/us/en/artificial-intelligence/posts/powering-precision-medicine-artificial-intelligence.html&lt;/a>. [Accessed: 12-Oct-2020].&lt;/p>
&lt;p>[3] T. Davenport and R. Kalakota, &amp;quot;The potential for artificial intelligence in healthcare,&amp;quot; &lt;em>Future healthcare journal&lt;/em>, Jun-2019. [Online]. Available: &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6616181/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6616181/&lt;/a>. [Accessed: 12-Oct-2020].&lt;/p></description></item><item><title>Report:</title><link>/report/fa20-523-307/project/project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-307/project/project/</guid><description>
&lt;h1 id="analysis-of-financial-markets-based-on-president-trumps-tweets">Analysis of Financial Markets based on President Trump&amp;rsquo;s Tweets&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-307/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-307/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-307/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-307/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Alex Baker, fa20-523-307, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-307/blob/master/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>President Trump has utilized the social media platform Twitter as a way to convey his message to the American people. The tweets he has published during his presidency cover a vast array of topics and issues from MAGA rallies to impeachment. This analysis investigates the relationship of the NASDAQ and the sentiment of President Trump&amp;rsquo;s tweets during key events in his presidency. NASDAQ data was gathered though Yahoo Finance&amp;rsquo;s API while President Trump&amp;rsquo;s tweets were gathered from Kaggle. The results observed show that during certain events, a correlation emerges of the NASDAQ data and the sentiment of President Trump&amp;rsquo;s tweets.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. DataSets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-data-cleaning-and-preprocessing">3. Data Cleaning and Preprocessing&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-twitter-data">3.1 Twitter Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-stock-data">3.2 Stock Data&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-methodologyprocess">4. Methodology/Process&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-preliminary-analysis-and-eda">5. Preliminary Analysis and EDA&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-twitter-data">5.1 Twitter Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-stock-data">5.2 Stock Data&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-defining-events-during-trumps-presidency">6. Defining events during Trump&amp;rsquo;s presidency&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-the-impeachment-of-president-trump">6.1 The Impeachment of President Trump&lt;/a>&lt;/li>
&lt;li>&lt;a href="#62-the-dakota-access-and-keystone-xl-pipelines-approval">6.2 The Dakota Access and Keystone XL pipelines approval&lt;/a>&lt;/li>
&lt;li>&lt;a href="#63-the-government-shutdown">6.3 The Government Shutdown&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> analysis, finance, stock markets, twitter, politics&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Financial markets have been an area of research in both academia and business. Analysis and predictions has been growing in its accuracy with an every increasing amount of data used to test these models. &amp;ldquo;The Efficient Market Hypothesis (EMH) states that stock market prices are largely driven by &lt;em>new&lt;/em> information and follow a random walk pattern&amp;rdquo;&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This shows that prices will follow news rather than previous and present prices. Information is unpredictable in terms of its release/publication showing market prices will follow a random walk pattern and the prediction can not be high.&lt;/p>
&lt;p>There are some problems that arise with EMH. One problem is that &amp;ldquo;stock prices does not follow a random walk pattern and can be predicted to a certain degree&amp;rdquo;&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Another problem associated with EMH is with the information&amp;rsquo;s unpredictability, the unpredictability is called into question with the introduction of social media (Facebook, Twitter, blogs). The rise of social media can be a early indicator for news before it is released/published. This project will analyze the market based on how the President tweets during certain events.&lt;/p>
&lt;h2 id="2-datasets">2. DataSets&lt;/h2>
&lt;p>In this project, two datasets will be used -&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The NASDAQ values from November 2016 to January 2020. This data was obtained through Yahoo! Finance and includes Date, Open, High, Low, Close, Adj Close, and Volume for a given day.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>President Trump&amp;rsquo;s tweets during the periods of November 2016 to January 2020 is over 41,000 tweets. The data includes id, link, content, date, retweets, favorites, mentions, hashtags, and geo for every tweet in the time frame. Since the performance of the analysis is on a daily basis, tweets will be split up by Date. This data is available on Kaggle (&lt;a href="https://www.kaggle.com/austinreese/trump-tweets?select=trumptweets.csv)">https://www.kaggle.com/austinreese/trump-tweets?select=trumptweets.csv)&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>To strengthen the analysis, even more, some code from the 2016 electionâ€™s analysis of markets may be utilized but the focus will be on the markets during the Trump administration. Rally data maybe introduced in order to have a deeper sense of some of the tweets when it comes to important news that is announces at President Trump&amp;rsquo;s rallies. In order to have a realistic and strong analysis, the financial data needs to be aligned with the timing of tweets but news that has already started to affect the markets before a tweet has been sent out needs to be taken into account.&lt;/p>
&lt;h2 id="3-data-cleaning-and-preprocessing">3. Data Cleaning and Preprocessing&lt;/h2>
&lt;p>The data required for this project is stock market data and Twitter data from President Trump. Stock market data was collected from Yahoo Finance&amp;rsquo;s API. This data was saved to a CSV file then imported using Pandas. The Twitter data was collected by a Kaggle user and is imported though Kaggle&amp;rsquo;s API or through the use of a local copy saved from the site. The data obtained needs to be cleaned and pre-processed in order to make it reliable for analysis through the use of Pandas, Regex, and Matplotlib.&lt;/p>
&lt;h3 id="31-twitter-data">3.1 Twitter Data&lt;/h3>
&lt;p>When importing the Twitter data, there are several things that are noticed when printing the first five rows. Three of the columns mention, hashtags, and geo are currently showing NaN. After calculating the missing values, all the values in these columns are missing or are zero so we can drop these columns from the dataframe.&lt;/p>
&lt;p>The tweets are one of the last columns needed to be cleaned. The text of the tweets needs to be uniformed in order to conduct analysis. Removing punctuations was the first step followed by removing content specifically seen in tweets. These could be the word retweet, the hashtag symbol(#), the @ symbol followed by a username, and any hyperlinks that could be in a tweet.&lt;/p>
&lt;h3 id="32-stock-data">3.2 Stock Data&lt;/h3>
&lt;p>Stock data has a unique set of challenges when it comes to cleaning. Unlike tweets, stock data is only available Monday through Friday and is not available for holidays that the market is closed. In order to have a complete dataset, several options are available. One option is to drop the tweets that fall on a weekend. This would not be useful since markets can react to news that happens on the weekend. Another option is that &amp;ldquo;if the NASDAQ value on a given day is x and the next available data point is y with n days missing in between, we approximate the missing data by estimating the first day after x to be (y+x)/2 and then following the same method recursively till all gaps are filled&amp;rdquo; &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-methodologyprocess">4. Methodology/Process&lt;/h2>
&lt;p>The collection of finance and Twitter data will be used to visualize the results. Some of Twitter or dataset data will need to be cleaned and classified to build the model. The methodology is composed of the following steps:&lt;/p>
&lt;ul>
&lt;li>Use data from President Trump&amp;rsquo;s personal twitter and data from Yahoo Finance API to help visualize&lt;/li>
&lt;li>Data cleaning and extraction&lt;/li>
&lt;li>Sentiment Analysis&lt;/li>
&lt;/ul>
&lt;p>Sentiment analysis is a key component to categorize President Trump&amp;rsquo;s tweets. Polarity and subjectivity are the two metrics that are used to classify each tweet. Polarity measures the opinion or emotion expressed in a piece of text; the value is returned as a float within the range of -1.0 to 1.0. Subjectivity, on the other hand, reflects the feelings or beliefs in a piece of text; the value is returned as a float within the range 0.0 to 1.0 where 0.0 is very objective and 1.0 is very subjective. TextBlob is the library utilized for processing the tweet&amp;rsquo;s polarity and subjectivity. The sentiment method is called along with the methods for polarity and subjectivity in their own functions. The returned values are added into two columns in the dataframe.&lt;/p>
&lt;p>The plot used for the sentiment analysis is a scatter plot. This will allow for each tweet to be plotted with their respected polarity and subjectivity. A line plot is the best plot to used in order to easily visualize market price verses the sentiment of the tweets. In plotting the line for the sentiment of tweets, several issues arise. The first major issues is that multiple tweets are published on a given day with varying degrees of sentiment. The line graph will display a vertical line for all the points represented. One solution is to take the average of the days tweets and use this new value on the graph. This method can be aligned on the same axes as the stock market data.&lt;/p>
&lt;h2 id="5-preliminary-analysis-and-eda">5. Preliminary Analysis and EDA&lt;/h2>
&lt;h3 id="51-twitter-data">5.1 Twitter Data&lt;/h3>
&lt;p>When starting to conduct preliminary analysis and exploratory data analysis (EDA), it is helpful to first check for any null values in the data and there are no null values in the twitter data.&lt;/p>
&lt;p>The date column is a column that is needed to track the amount of tweets per month and year. In the column, the timestamp and the date are combined so this need to be separated in several ways. The first being separating the date from the timestamp into its own column. This is followed up by separating the date into 4 columns for day, month, year and month-year in order to track tweets based on specified criteria.&lt;/p>
&lt;p>After graphing the amount of tweets per year, the observation is that 2016 and 2020 have a low tweet count. The reminder is that the data starts in November 2016 making 2016 have two months of data compared to 2020 with only one month being January. From 2017 through 2019, we can see that the amount of tweets increases by almost a thousand every year. The tweets per month tell a different story. The amount varies greatly over the years with the greatest amount being near the end of 2016 and the beginning of 2017. The sentiment of the tweets show that a majority of the tweets are a little skewed to the right of the graph. This shows that may of the President tweets are positive in some aspect as well as have a personal opinion, emotion or judgement.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/year_tweets.png" alt="Figure 1: Number of Tweets per Year">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Number of Tweets per Year&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/month_tweets.png" alt="Figure 2: Number of Tweets per Month">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Number of Tweets per Month&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/sentiment.png" alt="Figure 3: Sentiment Analysis of Tweets">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Sentiment Analysis of Tweets&lt;/p>
&lt;h3 id="52-stock-data">5.2 Stock Data&lt;/h3>
&lt;p>Similar to the twitter data, checking for null values is important but since the data is from Yahoo! Finance there are no missing values on the days that the markets are opened.&lt;/p>
&lt;p>Once graphing the open and closed prices of the NASDAQ, there seems to be an general upwards trend in the market over the time period.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/market.png" alt="Figure 4 Open and Close Price of the NASDAQ">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Open and Close Price of the NASDAQ&lt;/p>
&lt;h2 id="6-defining-events-during-trumps-presidency">6. Defining events during Trump&amp;rsquo;s presidency&lt;/h2>
&lt;h3 id="61-the-impeachment-of-president-trump">6.1 The Impeachment of President Trump&lt;/h3>
&lt;p>After weeks of talks among Congress, the House of Representatives have voted to impeach President Trump on two charges: abuse of power and obstruction of Congress on December 18, 2019. Since the country&amp;rsquo;s founding in 1776, only three presidents have faced impeachment from Congress: Andrew Johnson, Bill Clinton and now Donald Trump. This move has been widely advocated for since his election in 2016. &amp;ldquo;In September 2019, news leaked of a phone call between President Trump and Ukrainian President Volodymyr Zelensky regarding an investigation into Hunter Biden, son of then Democratic candidate Joe Biden, for his dealings in Ukraine&amp;rdquo; &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The sentiment analysis preformed on tweets that are published at the start of November though the end of 2019. The graph shows a steady amount of subjectivity and polarity through much of November. In the final week of November, when the House Intelligence Committee&amp;rsquo;s public hearings were concluding, a large spike in subjectivity as well as polarity shows that President Trump was trying to discredit the individuals testifying, declaring the whole impeachment a witch hunt, or making a case of all the good he has done for teh country. The NASDAQ shows that the opening price went down when the Articles of Impeachment were announced but when the House voted to approve the articles started in mid December, the stock price is on an upward trend. The daily change points out that the price of the stock was fluctuating quite a bit during the initial stages of the impeachment hearing as well as the House vote on the Articles of Impeachment.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/impeachment_sentiment.png" alt="Figure 5: Sentiment Analysis during Impeachment">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Sentiment Analysis during Impeachment&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/impeachment_stock.png" alt="Figure 6: Open and Daily Change Price during Impeachment">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Open and Daily Change Price during Impeachment&lt;/p>
&lt;h3 id="62-the-dakota-access-and-keystone-xl-pipelines-approval">6.2 The Dakota Access and Keystone XL pipelines approval&lt;/h3>
&lt;p>One of the first moves President Trump made when arriving into office was to approve the Dakota Access and Keystone XL pipelines. &amp;ldquo;Both of the pipelines were blocked by the Obama administration due to environmental concerns, but President Trump has questioned climate change and promised to expand energy infrastructure and create jobs&amp;rdquo;&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The Keystone pipeline would span 1,200 miles across six states, moving over 800,000 barrels of oil daily from Canada to the Gulf coast. The Dakota Access pipeline would move oil from North Dakota all the way to Illinois. &amp;ldquo;The Standing Rock Sioux tribe, whose reservation is adjacent to the pipeline, staged protests that drew thousands of climate change activists to the rural area of Cannon Ball, North Dakota&amp;rdquo; &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The sentiment analysis shows a high polarity and subjectivity three days before the signing of the presidential memorandum but drops sharply during the event. Subjectivity has a quick resurgence but polarity stays low as time goes on. In the days leading up to the signing of the pipelines on January 24th, the opening price has an upward trend and stays fairly consistent in the following days. Daily change has a big jump at the end of January but falls dramatically and does not reach the level it was once at, this could be due to the protests that followed the approval or companies reevaluating their positions on the pipeline.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/dakota_sentiment.png" alt="Figure 7: Sentiment Analysis during Dakota Approval">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Sentiment Analysis during Dakota Approval&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/dakota_stock.png" alt="Figure 8: Open and Daily Change Price during Dakota Approval">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Open and Daily Change Price during Dakota Approval&lt;/p>
&lt;h3 id="63-the-government-shutdown">6.3 The Government Shutdown&lt;/h3>
&lt;p>On December 21, 2018 the United States Government shutdown. &amp;ldquo;At the heart of the dispute is Trump&amp;rsquo;s demand for just over $5 billion toward a long-promised wall along the US-Mexico border&amp;rdquo; &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. The shutdown affected a part of the federal government such as homeland security, transportation, and agriculture. &amp;ldquo;The problems caused by the shutdown are wide-ranging, from waste piling up in national parks to uncertainty for 800,000 federal workers about when their next paycheck will come&amp;rdquo; &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. This shutdown was the longest shutdown in the modern era coming to an end on January 25, 2019 after 35 days. The sentiment analysis tells that the tweets shared during the shutdown are lower in terms of polarity with a majority of tweets being higher in subjectivity. A interesting note is that subjectivity on the day of the shutdown dropped to zero but shot up quickly the next day. There was no significant movement during the whole shutdown until the start of February when polarity soared and subjectivity dropped. A potential reason why there was no significant movement in the analysis was that the president wanted to get his piece of legislation through Congress but Congress was not going to approve his legislation. Prior to the government shutdown, the opening prices fell by 10 dollars with the lowest being around the time the shutdown began. The new year shows stock on a steady increase during the month of January, when the shutdown was lifted. Daily change was on the rise when the shutdown began but swiftly dropped at the start of the new year but recovered and stayed relatively stabled throughout the shutdown.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/shutdown_sentiment.png" alt="Figure 9: Sentiment Analysis during the Government Shutdown">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> Sentiment Analysis during the Government Shutdown&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/shutdown_stock.png" alt="Figure 10: Open and Close Price during the Government Shutdown">&lt;/p>
&lt;p>&lt;strong>Figure 10:&lt;/strong> Open and Close Price during the Government Shutdown&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>The investigation showed the relation between President Trump&amp;rsquo;s tweets and the NASDAQ during certain events. The results pointed that a majority of tweets were positive in polarity with subjectivity being higher or sometimes lower depending on the event. The NASDAQ had some interesting reactions based on the events. In highly important events, the stock price tended to have an upward trajectory but leading up to the event the price would go down. These results show that the content of the President&amp;rsquo;s tweets have some impact in terms of the market movements, but many factors go into the price of the market such as foreign relations and how companies are preforming. Finally, it is worth mentioning that the analysis doesnâ€™t take into account some factors. Weekends were a factor that was not included into the stock market data. Tweets from the President&amp;rsquo;s official account were not taken into account in this analysis. All of these remaining areas can be added in future research.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Goel, A. and Mittal, A., 2011. Stock Prediction Using Twitter Sentiment Analysis. [online] cs229.stanford.edu. Available at: &lt;a href="http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf">http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf&lt;/a>. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>J. Bollen, H. Mao, and X. Zeng, Twitter mood predicts the stock market. Journal of Computational Science, vol. 2, no. 1, pp. 1â€“8, 2011. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>President Donald Trump impeached, History.com, 05-Feb-2020. [online]. Available at: &lt;a href="https://www.history.com/this-day-in-history/president-trump-impeached-house-of-representatives">https://www.history.com/this-day-in-history/president-trump-impeached-house-of-representatives&lt;/a>. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>D. Smith and A. Kassam, Trump orders revival of Keystone XL and Dakota Access pipelines, The Guardian, 24-Jan-2017. [online]. Available at: &lt;a href="https://www.theguardian.com/us-news/2017/jan/24/keystone-xl-dakota-access-pipelines-revived-trump-administration">https://www.theguardian.com/us-news/2017/jan/24/keystone-xl-dakota-access-pipelines-revived-trump-administration&lt;/a>. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Bryan, B., The government shutdown is now the longest on record and the fight between Trump and Democrats is only getting uglier. Here&amp;rsquo;s everything you missed. 21-Jan-2019. [online]. Available at: &lt;a href="https://www.businessinsider.com/government-shutdown-timeline-deadline-trump-democrats-2019-1">https://www.businessinsider.com/government-shutdown-timeline-deadline-trump-democrats-2019-1&lt;/a>. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-308/hw7/task_3_next_steps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-308/hw7/task_3_next_steps/</guid><description>
&lt;h1 id="next-steps">Next Steps&lt;/h1>
&lt;p>I am still not 100% that this is the project I want to complete. As the instructions for HW 5 laid out, we do not have to fully commit at this point to the project. I may try to work on a basic deep learning project that can introduce me to that type of work. Next steps for the project I have started here would be to complete the basic descriptive data modeling in charts. Then would be to model the data and pull in the playoff teams from 2009-2019 to compare the model output with the playoff team that qualified for the playoffs. I want to work on this over the next month before moving onto the writing portion of the project.&lt;/p></description></item><item><title>Report:</title><link>/report/fa20-523-308/project/project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-308/project/project/</guid><description>
&lt;h1 id="nfl-regular-season-skilled-position-player-performance-as-a-predictor-of-playoff-appearance-overtime">NFL Regular Season Skilled Position Player Performance as a Predictor of Playoff Appearance Overtime&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-308/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-308/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Travis Whitaker, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-308">fa20-523-308&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-308/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The present research investigates the value of in-game performance metrics for NFL skill position players (i.e., Quarterback, Wide Receiver, Tight End, Running Back and Full Back) in predicting post-season qualification. Utilizing nflscrapR-data that collects all regular season in-game performance metrics between 2009-2018, we are able to analyze the value of each of these in-game metrics by including them in a regression model that explores each variables strength in predicting post-season qualification. We also explore a comparative analysis between two time periods in the NFL (2009-2011 vs 2016-2018) to see if there is a shift in the critical metrics that predict post-season qualification for NFL teams. Theoretically, this could help inform the debate as to whether there has been a shift in the style of play in the NFL across the previous decade and where those changes may be taking place according to the data. Implications and future research are discussed.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-results">5. Results&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#inference">Inference&lt;/a>&lt;/li>
&lt;li>&lt;a href="#preliminary-results">Preliminary Results&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#2009-2011-skill-position-player-performance-as-playoff-predictor">2009-2011 Skill Position Player Performance as Playoff Predictor&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2016-2018-skill-position-player-performance-as-playoff-predictor">2016-2018 Skill Position Player Performance as Playoff Predictor&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#comparative-results">Comparative Results&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-discussion">6. Discussion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#limitations-and-future-research">Limitations and Future Research&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgements">8. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> ANOVA, Comparative Analysis, Exploratory Analysis, Football, Sports, Metrics&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>In the modern NFL the biggest negotiating tools for players in signing a new contract is their on-field performance. Many players choose to &amp;ldquo;hold-out&amp;rdquo; of pre-season practice or regular season games as a negotiating tool in their attempt to sign a more lucrative contract. In this situation players feel as though their exceptional performance on the field is not reflected in the monetary compensation structure of their contract. This is most often reflected in skill position players such as wide receivers or running backs whose play on the field is most often celebrated (e.g., touchdowns) and discussed by fans of the game. While these positions are no doubt important to a teamâ€™s success, the question remains how important is one players contribution to a teamâ€™s overall success? The current project will attempt to evaluate the importance of skill position players' (i.e., Quarterback (QB), Wide Receiver (WR), Running Back (RB), and Tight End (TE)) performance during the regular season and use in-game performance metrics as a predictive factor for their team making the playoffs. This is an attempt to answer the question can qualifying for the post-season be predicted by skill position metrics measured during the regular season? If so, then which metrics are most crucial to predicting post-season qualification?&lt;/p>
&lt;p>A secondary analysis in this project will look at a comparison between 2009-2011 vs. 2016-2018 regular season metrics and build separate models for each three year span to investigate whether a shift in performance metric importance has occurred over the past decade. NFL insiders and journalists have noted the shift in play-calling over the past decade in the NFL as well as the change at the quarterback position to more &amp;ldquo;dual-threat&amp;rdquo; (running and throwing) quarterbacks that have transitioned the game to a more &amp;ldquo;aggressive&amp;rdquo; play-style &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Yet punditry and reporting have not always been supported by performance metrics and this specific claim of a transition over the past decade needs some exploring. Therefore, we will be investigating whether there has been a shift in the performance metrics that are important in predicting team success. Again, team success will be measured by making the post-season.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>There are many playoff predictor models that focus on team performance or team wins vs losses as a predictor of making the playoffs &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. However, few take into consideration individual player performance as an indicator of their team making the post-season playoffs in the NFL. The most famous model that takes into consideration player performance is ELO rating &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The first ELO rating was a straightforward model that took head-to-head results and player-vs-team model to predict win probability in an NFL game. However, in 2019 Silver and his team at FiveThirtyEight updated their ELO model to give a value rating to the quarterback position for each team. This quarterback value included metrics such as pass attempts, completions, passing yards, passing touchdowns, interceptions, sacks, rush attempts, rushing yards, and rushing touchdowns. Taking these metrics along with the defensive quality metrics, which is an adjustment of quarterback value based on the opposing defense ranking, gives you an overall value for your quarterback. Thus, this widely accepted model takes head-to-head team comparisons on a week-to-week basis and includes the quarterback value in predicting the winners of these head-to-head matchups. However, no model has taken just player performance and tried to predict team success for an entire regular season based on each of their individual players. These previous models primarily look at offensive vs. defensive units and try to predict win/loss records based off each of these units.&lt;/p>
&lt;p>The goal of the present research is not to compare our model vs previous models, as these standing models are not meant for playoff prediction, rather these previous models are used for a game-by-game matchup comparison. The present research investigates whether looking at position players at each of the skilled positions, maps onto predicting the post-season qualifying teams. Further, how does this predictive model change over time? Looking at 2009-2011 NFL skill player performance vs 2016-2018 skill player performance we will investigate if there are differences in metrics that predict post-season appearance. This will show us whether there are shifts in skill position play that impact predicting post-season playoff appearance. Specifically, by comparing the two models which use two different periods of time (i.e., 2009-2011 vs 2016-2018) we will be able to better investigate specific metrics at each position that are important for predicting success. For example, if the wide receiver position is important, what is most important about that position, yards-after-catch or number of receptions? Further, how might the importance of those metrics shift over time? We hope to explore and understand better the impact of skill players and the metrics that they are measured on in terms of making the post-season.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>The datasets used are in a github folder that holds nflscrapR-data that originates from NFL.com &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The folder includes play-by-play data, including performance measures, for all regular season games from 2009 to 2019. This file was paired with week-by-week regular season roster data for each team in the NFL. This allowed us to track skilled position player performance during the regular season and then compare this regular season file with the files that contain playoff teams for each year from 2009-2019. Supplemental data was pulled from Pro-Football-Reference.com &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;p>The first step we took to understand the data was to use various slices of the data put into scatterplots and bar charts to find trends, as well as various time series charts. This was an exploratory step in understanding the data.&lt;/p>
&lt;p>Then each metric from player performance during the regular season was included in the analysis or models that were built to predict post-season appearance. Post-season appearance is a designation for a team qualifying for the post-season or playoffs. We think it is important to engineer some new features to potentially provide insights. For instance, it is possible to determine whether a play was during the final two minutes of a half and if a play was in the red zone. During these critical points of a game a win or lose is often determined. Our thought is by weighing these moments and performance metrics with more importance the model will better predict a teamâ€™s likelihood of making the playoffs. Another secondary metric that may strengthen the predictive ability of the model would be to use Football Outsiderâ€™s Success Rate, which is a determination of a playâ€™s success rate for the offense that is on the field &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. This can also provide me with the down and distance to go for the offense and players that are on the field. We will also use college position designations as way to normalize the positions performance across teams. Many NFL teams utilize different player sets. Thus, it is important to use a standard, which college football uses across all teams. Since we are only interested in skill position players this will include Wide Receiver (WR), Running Back (RB), Full Back (FB), Quarterback (QB), and Tight End (TE). These designations will allow the model to compare player performance by position.&lt;/p>
&lt;p>After breaking down the data into key categorical variables to see if there was an impact for these performance variables in making the playoffs for the NFL teams. These individual position statistics were analyzed as a group and then separated into &amp;ldquo;Post Season&amp;rdquo; meaning the player&amp;rsquo;s team qualified for the playoffs, or &amp;ldquo;No Post&amp;rdquo; meaning the player&amp;rsquo;s team did not qualify for the playoffs. By doing this we were able to verify that a reasonable number of players fell into the &amp;ldquo;Post Season&amp;rdquo; group, as only 12 out of 32 teams qualify for the post-season. Further we were able to use these designations in the next step of modeling, where the data was analyzed in an ANOVA to see how important each metric was in predicting post-season appearance for each player.&lt;/p>
&lt;p>Metric measurement needs to be consistent across years. A comparison of year-to-year metrics was completed comparing each years measurements from 2009-2011 and 2016-2018 in order to make sure that the measurement techniques were stable and do not vary across time. If there were changes in the way metrics are measured than either that year will need to be dropped from the model or adjustments will need to be made to the metric to balance it with the other years included in the model. Luckily, there were no variation in metric measurement across years, so all measurements initially included in the model were kept.&lt;/p>
&lt;p>Finally, once all metrics were balanced and the team performance metrics had been aggregated. The ANOVA model was built to analyze metric measurement as a predictor of a player making post-season play. This ANOVA model was created twice, once for the 2009-2011 players and then again for the 2016-2018 players. Once this analysis was run, we were able to see the strength of the model in predicting playoff appearance by player, based on metric measurement.&lt;/p>
&lt;h2 id="5-results">5. Results&lt;/h2>
&lt;h3 id="inference">Inference&lt;/h3>
&lt;p>An individual player-performance model for NFL skill position players (i.e., quarterback, wide receiver, tight end, and running back) will perform better than chance level (50%) of identifying playoff teams from the NFL during the season&amp;rsquo;s of 2009-2011 and 2016-2018. Further, an exploratory analysis of time periods (2009-2011 vs 2016-2018) will expose differences in the key metrics that are used to predict playoff appearance over time. This will give us a better understanding of the shifts in performance in the NFL at each skill position.&lt;/p>
&lt;h3 id="preliminary-results">Preliminary Results&lt;/h3>
&lt;p>The descriptive statistics for the player performance revealed no issues with normality or different metric standards across seasons for player performance measurements. Figure 1 represents a count check for players qualifying for the post-season in the seasons of 2009-2011. Based on the NFL post-season structure 12 out of 32 teams qualify for the post-season, or 37.5%. Figure 1 shows that roughly 1/3 of players qualify for the post season at each position. However, it is important to note that each team structure and roster is different. For example, one team may carry 7 receivers, 2 running backs, 2, quarter backs, 2 tight ends, and 0 full backs, where another team may carry 4 receivers, 4 running backs, 3 quarter backs, 4 tight ends, and 1 full back. This is an important distinction to make because the &amp;ldquo;Post Season&amp;rdquo; players shown in figure 1 are not at an equal percentage across position.&lt;/p>
&lt;h4 id="2009-2011-skill-position-player-performance-as-playoff-predictor">2009-2011 Skill Position Player Performance as Playoff Predictor&lt;/h4>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Player_summary_2009_2011.png" alt="Player Qualifying for Post-Season">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Breakdown of Players by Skill Position That Qualified for Post-Season Play (2009-2011)&lt;/p>
&lt;p>We also wanted to investigate whether play-count at each position was balanced across post-season players and players who did not qualify for the post-season. Figure 2 shows that players on teams who did qualify for the post-season were involved in more plays at their position than players at their position who did not qualify for the playoffs. Thinking about this finding as a result of the regular season, players in skilled positions on post-season qualifying teams play on offenses that won more games than teams who did not qualify for the playoffs. While we did not look at time of possession for players by position, it seems fairly reasonable through logic and the results in figure 2 that play count is higher because these teams are more successful and the players on post-season qualifying teams are on the field more than teams with more losses who did not qualify for the post-season.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Play_count_position_2009.png" alt="Count of Play-type by Post-Season Qualification category">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Count of Play-type by Post-Season Qualification category (2009-2011)&lt;/p>
&lt;p>Figure 3 below is a reference table for the features included in the ANOVA regression model determining the key features that predict post-season qualification. These features are used for reference in figures 4 and 7.&lt;/p>
&lt;p>Using the player performance metrics for the regular season, an ANOVA was run to see if these metrics placed together would be a successful predictor of post-season qualification. Figure 4 shows the top 10 metrics that had the highest f-values for predicting post-season qualification, all of which were all significant (p&amp;lt;.05) in the model. Reviewing the model, the most significant factors for predicting post-season qualification for teams in order were; 1. Successful Reception (wide receiver or tight end), 2. Total Receiving Yards (Wide Receiver or Tight End), 3. Yards After Catch (wide receiver or tight end), 4. Total Receiving Touchdowns (Wide Receiver or Tight End), 5. Total Touchdowns (All positions), 6. Receiver Plays (Wide Receiver), 7. Redzone Plays (All positions), 8. Successful Plays (All positions), 9. Yards Gained (All positions), 10. Total Offensive Plays in the 3rd Quarter (All positions). The model accounted for 78.2% of variance. The model successfully accounted for predicting post-season qualifying teams in 78.2% of instances.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/figure_description_table.png" alt="Feature Descriptions">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Description for top features included in ANOVA regression model&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_pvalue_table_2009.png" alt="ANOVA for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> ANOVA Table for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2009-2011)&lt;/p>
&lt;h4 id="2016-2018-skill-position-player-performance-as-playoff-predictor">2016-2018 Skill Position Player Performance as Playoff Predictor&lt;/h4>
&lt;p>We paralleled our analysis from the 2009-2011 analysis above in completing the 2016-2018 analysis represented in Figures 5-7. Figure 5 represents the player count that qualified for post-season play (orange) and the non-post-season players (blue). Again, player count was compared to the roughly 37.5% rate that should be expected for 12 teams out of 32 qualifying for post-season play. However, the rates were a bit below the 37.5% rate. This can be explained by the number of injuries and roster changes that occur throughout the season. Teams shuffle in-and-out players at each position based on injury or performance. Teams will not have a static roster throughout the season, this includes post-season teams who cut players or put players on IR. These players, even though they played for post-season qualifying teams, would be in blue because they are not on the post-season rosters for the teams who qualify for post-season play. This along with the roster structure described for figure 1 explains the lower than 1/3 rate of players qualifying for post-season play.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Player_summary_2016-2018.png" alt="Player Qualifying for Post-Season">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Breakdown of Players by Skill Position That Qualified for Post-Season Play (2016-2018)&lt;/p>
&lt;p>Figure 6 investigates play-count at each position, like figure 2, but this time for the seasons of 2016-2018. Again, the analysis was balanced across post-season players and players who did not qualify for the post-season. Figure 6 shows that players on teams who did qualify for the post-season were involved in more plays at their position than players at their position who did not qualify for the playoffs. Figure 6 shows a consistent pattern with figure 2. Descriptive statistic comparison between the 2009-2011 seasons and the 2016-2018 seasons will be revisited in the discussion section.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Play_Count_Position_2016.png" alt="Count of Play-type by Post-Season Qualification category">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Count of Play-type by Post-Season Qualification category (20016-2018)&lt;/p>
&lt;p>Using the player performance metrics for the regular season, an ANOVA was run to see if these metrics placed together would be a successful predictor of post-season qualification. Figure 7 shows the top 10 metrics that had the highest f-values for predicting post-season qualification, all of which were significant (p&amp;lt;.05) in the model. The model successfully accounted for predicting post-season qualifying teams in 77.8% of instances.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_pvalue_table_2016.png" alt="ANOVA for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> ANOVA Table for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2016-2018)&lt;/p>
&lt;h3 id="comparative-results">Comparative Results&lt;/h3>
&lt;p>Comparing the 2016-2018 with the 2009-2011 season model, certain shifts have occurred from the 2009-2011 seasons model. Namely, yards-after-catch has become the strongest predictor of post-season qualification, flipping positions with successful reception in the 2009-2011 model. Another notable shift is the importance of number of plays run in the second quarter in the 2016-2018 model, overtaking number of plays run in the third quarter from the 2009-2011. The models themselves also shift in their strength of prediction. The 2009-2011 model shows stronger predictive capability (78.2% vs 77.8%), which is reflected in the f-values for the top 10 metrics of the model. The 2009-2011 model has four variables with f-values above 50 and one above 60. The 2016-2018 model only has one variable with an f-value above 50. These values represent the variance accounted for in the model by a variable. The higher the f-value the more variance accounted for in the model by that specific variable. Since the f-values were so high for many of the top 10 variables listed in each model, the p-values showed highly significant far exceeding the p=.05 level that was needed. The f-values were high because they accounted for so much of the variance in the model, meaning the predictive nature of the model was due in large part to many of the variables in the top 10. Another way to state this is that each of these top 10 variables were significantly better at predicting post-season qualification than would be expected due to chance.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_sig_features_2009.png" alt="ANOVA Chart for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> ANOVA Chart for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2009-2011)&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_sig_features_2016.png" alt="ANOVA Chart for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> ANOVA Chart for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2016-2018)&lt;/p>
&lt;p>Cloudmesh Comon &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> is used to create the benchmark.&lt;/p>
&lt;h2 id="6-discussion">6. Discussion&lt;/h2>
&lt;p>The first inference of this project was investigating the possibility of using in-game performance metrics as a competent and better-than-chance predictor of selecting skill position players making the NFL post-season. Both the 2009-2011 and the 2016-2018 season models were able to predict player post-season qualification at 78.2% and 77.8% levels of success, both above chance level. This success highlights the critical nature of skill performance players and provides confidence to the modern metric model of NFL players as a useful and qualified tool to evaluate player performance as a measure of success. This also gives clout to the skill position players who believe their contributions on the field are deserving of top dollar compensation in the NFL. According to our models, wide receivers are deserving of high compensation as their game play impacts the likelihood of their team making the playoff more than running backs. However, it is hard to discriminate whether quarterback play is also key to the success of wide receivers. It could well be that these two positions work hand-in-hand.&lt;/p>
&lt;p>Investigating the second inference regarding changes in the predictive model across time. In comparing the descriptive statistic models (figs. 1, 2 vs. 5, 6). There are some noticeable, but not significant differences in the two-time ranges. First, there are more receivers in the 2016-2018 time range, which reflects the NFLâ€™s shift towards a more pass prone league. Since there was not an increase in roster size between the two-time ranges, the increase in receivers lead to a decrease in the number of quarterbacks and fullbacks on a roster, but these additional receivers carried probably took the roster spots of non-skill positions players that are not accounted for here. Both models show the importance of pass plays, successful pass plays, receiving touchdowns and yards, yards after catch, and other passing variables that highlight the importance of wide receivers and tight ends. The NFL has shifted towards a more pass-friendly league &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, and the models built here highlight the reasons why that occurred. Receiver plays are significantly more important in predicting post-season qualification than any other skill position metrics. It is likely that the shift towards receivers and away from running backs has taken place over time. It is possible that we have pulled two time periods that are too close together to reflect the shift in NFL play, and if we had pulled data from the 1990â€™s or 1980â€™s (unfortunately this data is not available in the needed metrics) we would see more running back heavy metrics at the tops of our models and significant changes in the two time periods.&lt;/p>
&lt;h3 id="limitations-and-future-research">Limitations and Future Research&lt;/h3>
&lt;p>Metrics are not provided for non-skill position players who could be critical in predicting playoff qualification. For instance, if we could include offensive linemen metrics, we would have a stronger model that would be better able to predict post-season qualification. Further, the NFL data we had access to does not measure defensive player metrics that we believe are critical in being able to predict post-season qualification for NFL teams. Future work should look to include defensive player metrics into their model, as well as non-skill position players to improve on this model.&lt;/p>
&lt;p>Though we were able to build a model to predict player qualification for the post-season, future research can build on this model by making a composite of players on a team to then predict a team making the playoffs. The present study is a nice first step in understanding the capabilities of game performance for predicting player success, but NFL teams are equally interested in a team&amp;rsquo;s success, not just individual skill players. Therefore, future research can build on this project by incorporating defensive player metrics, non-skill position offensive metrics, and composites of players on one team to predict a team&amp;rsquo;s projected chances of making the playoffs.&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>Utilizing skill position performance metrics shows to be a successful predictor of player post-season qualification above chance level (50%). Further, there are slight shifts in which metrics are best at predicting post-season qualification between the 2009-2011 and 2016-2018 time periods. However, the key metrics that were significant in both models from the two time periods (2009-2011 and 2016-2018) did not change. Therefore, we cannot say definitively that there has been a shift in style of play from 2009 to 2018.&lt;/p>
&lt;h2 id="8-acknowledgements">8. Acknowledgements&lt;/h2>
&lt;p>Thank you to my friends and family who supported me through working on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Seifert, K. (2020, June 18). How pro football has changed in the past 10 years: 12 ways the NFL evolved this decade. Retrieved November 17, 2020, from &lt;a href="https://www.espn.com/nfl/story/_/id/28373283/how-pro-football-changed-10-years-12-ways-nfl-evolved-decade">https://www.espn.com/nfl/story/_/id/28373283/how-pro-football-changed-10-years-12-ways-nfl-evolved-decade&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Zita, C. (2020, September 16). Improving a Famous NFL Prediction Model. Retrieved November 02, 2020, from &lt;a href="https://towardsdatascience.com/improving-a-famous-nfl-prediction-model-1295a7022859">https://towardsdatascience.com/improving-a-famous-nfl-prediction-model-1295a7022859&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Silver, N. (2018, September 05). How Our NFL Predictions Work. Retrieved November 02, 2020, from &lt;a href="https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/">https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Ryurko. Ryurko/NflscrapR-Data. 2 Mar. 2020, &lt;a href="https://github.com/ryurko/nflscrapR-data">https://github.com/ryurko/nflscrapR-data&lt;/a>. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Sports Reference, LLC. Pro Football Statistics and History. Retrieved October 09, 2020. &lt;a href="https://www.pro-football-reference.com/">https://www.pro-football-reference.com/&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a> &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-309/project/project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-309/project/project/</guid><description>
&lt;h1 id="analysis-of-various-machine-learning-classification-techniques-in-detecting-heart-disease">Analysis of Various Machine Learning Classification Techniques in Detecting Heart Disease&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-309/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-309/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Ethan Nguyen, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-309">fa20-523-309&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>As cardiovascular diseases are the number 1 cause of death in the United States, the study of the factors and early detection and treatment could improve quality of life and lifespans. From investigating how the variety of factors related to cardiovascular health relate to a general trend, it has resulted in general guidelines to reduce the risk of experiencing a cardiovascular disease. However, this is a rudimentary way of preventative care that allows for those who do not fall into these risk categories to fall through. By applying machine learning, one could develop a flexible solution to actively monitor, find trends, and flag patients at risk to be treated immediately. Solving not only the risk categories but has the potential to be expanded to annual checkup data revolutionizing health care.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-dataset-cleaning">2.1 Dataset Cleaning&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-dataset-analysis">2.2 Dataset Analysis&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-machine-learning-algorithms-and-implementation">3. Machine Learning Algorithms and Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-scikit-learn-and-algorithm-types">3.1 Scikit-Learn and Algorithm Types&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-classification-algorithms">3.2 Classification Algorithms&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#321-support-vector-machines">3.2.1 Support Vector Machines&lt;/a>&lt;/li>
&lt;li>&lt;a href="#322-k-nearest-neighbors">3.2.2 K-Nearest Neighbors&lt;/a>&lt;/li>
&lt;li>&lt;a href="#323-gaussian-naive-bayes">3.2.3 Gaussian Naive Bayes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#324-decision-trees">3.2.4 Decision Trees&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#33-clustering-algorithms">3.3 Clustering Algorithms&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#331-k-means">3.3.1 K-Means&lt;/a>&lt;/li>
&lt;li>&lt;a href="#332-mean-shift">3.3.2 Mean-shift&lt;/a>&lt;/li>
&lt;li>&lt;a href="#333-spectral-clustering">3.3.3 Spectral Clustering&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#34-implementation">3.4 Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#341-dataset-preprocessing">3.4.1 Dataset Preprocessing&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-results--discussion">4. Results &amp;amp; Discussion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-algorithm-metrics">4.1 Algorithm Metrics&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#411-support-vector-machines">4.1.1 Support Vector Machines&lt;/a>&lt;/li>
&lt;li>&lt;a href="#412-k-nearest-neighbors">4.1.2 K-Nearest Neighbors&lt;/a>&lt;/li>
&lt;li>&lt;a href="#413-gaussian-naive-bayes">4.1.3 Gaussian Naive Bayes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#414-decision-trees">4.1.4 Decision Trees&lt;/a>&lt;/li>
&lt;li>&lt;a href="#415-k-means">4.1.5 K-Means&lt;/a>&lt;/li>
&lt;li>&lt;a href="#416-mean-shift">4.1.6 Mean-shift&lt;/a>&lt;/li>
&lt;li>&lt;a href="#417-spectral-clustering">4.1.7 Spectral Clustering&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#42-system-information">4.2 System Information&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-discussion">4.3 Discussion&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgements">6. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#references">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> health, healthcare, cardiovascular disease, data analysis&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Since cardiovascular diseases are the number 1 cause of death in the United States, early prevention could help in extending oneâ€™s life span and possibly quality of life &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Since there are cases where patients do not show any signs of cardiovascular trouble until an event occurs, having an algorithm predict from their medical history would help in picking up on early warning signs a physician may overlook. Or could also reveal additional risk factors and patterns for research on prevention and treatment. In turn this would be a great tool to apply in preventive care, which is the type of healthcare policy that focuses in diagnosing and preventing health issues that would otherwise require specialized treatment or is not treatable &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This also has the potential to trickle down and increase the quality of life and lifespan of populations at a reduced cost as catching issues early most likely results in cheaper treatments &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This project will take a high-level overview of common, widely available classification algorithms and analyze their effectiveness for this specific use case. Notable ones include, Gaussian Naive Bayes, K-Nearest Neighbors, and Support Vector Machines. Additionally, two data sets that contain common features will be used to increase the training and test pool for evaluation. As well as to explore if additional feature types contribute to a better prediction. The goal of this project being a gateway to further research in data preprocessing, tuning, or development of specialized algorithms as well as further ideas on what data could be provided.&lt;/p>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.kaggle.com/johnsmith88/heart-disease-dataset">https://www.kaggle.com/johnsmith88/heart-disease-dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kaggle.com/sulianova/cardiovascular-disease-dataset">https://www.kaggle.com/sulianova/cardiovascular-disease-dataset&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The range of creation dates are 1988 and 2019 respectively with different features of which 4 are common between. This does bring up a small hiccup in preprocessing to consider. Namely the possibility of changing diet and culture trends resulting in significantly different trends/patterns within the same age group. As well as possible differences in measurement accuracy. However this large gap is within the scope of the project in exploring which features can help provide an accurate prediction.&lt;/p>
&lt;p>This possible phenomenon may be of interest to explore closely if time allows. Whether a trend itself is even present or there is an overarching trend across different cultures and time periods. Or to consider if this difference is significant enough that the data from the various sets needs to be adjusted to normalize the ages to present day.&lt;/p>
&lt;h3 id="21-dataset-cleaning">2.1 Dataset Cleaning&lt;/h3>
&lt;p>The datasets used have already been significantly cleaned from the raw data and has been provided as a csv file. These files were then imported into the python notebook as pandas dataframes for easy manipulation.&lt;/p>
&lt;p>An initial check was made to ensure the integrity of the data matched the description from the source websites. Then some preprocessing was completed to normalize the common features between the datasets. These features were gender, age, and cholesterol levels. The first two adjustments were trivial in conversion however, in the case of cholesterol levels, the 2019 set is on a 1-3 scale while the 1988 dataset provided them as real measurements. A conversion of the 1988 dataset was done based on guidelines found online for the age range of the dataset &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="22-dataset-analysis">2.2 Dataset Analysis&lt;/h3>
&lt;p>From this point on, the 1988 dataset will be referred to as &lt;code>dav_set&lt;/code> and 2019 data set will be referred to as &lt;code>sav_set&lt;/code>.&lt;/p>
&lt;p>To provide further insight on what to expect and how a model would be applied, the population of the datasets was analysed first. As depicted in Figure 2.1 the population samples of both datasets of gender vs age show the majority of the data is centered around 60 years of age with a growing slope from 30 onwards.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/agevssex.jpg" alt="Figure 2.1">&lt;/p>
&lt;p>&lt;strong>Figure 2.1&lt;/strong>: Age vs Gender distributions of the dav_set and sav_set.&lt;/p>
&lt;p>This trend appears to signify that the datasets focused solely on an older population or general trend in society of not monitoring heart conditions as closely in the younger generation.&lt;/p>
&lt;p>Moving on to Figure 2.2, we see an interesting trend with a significant growing trend in the sav_set in older population having more cardiovascular issues compared to the dav_set. While this cannot be seen in the dav_set. This may be caused by the additional life expectancy or a change in diet as noted in the introduction.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/agevstarget.jpg" alt="Figure 2.2">&lt;/p>
&lt;p>&lt;strong>Figure 2.2&lt;/strong>: Age vs Target distributions of the dav_set and sav_set.&lt;/p>
&lt;p>In Figure 2.3, the probability of having cardiovascular issues between the sets are interesting. In the dav_set the inequality of higher probability could be attributed to the larger female samples in the dataset. With the sav_set having a more equal probability between the genders.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/gendervsprobability.jpg" alt="Figure 2.3">&lt;/p>
&lt;p>&lt;strong>Figure 2.3&lt;/strong>: Gender vs Probability of cardiovascular issues of the dav_set and sav_set.&lt;/p>
&lt;p>Finally, in Figure 2.4 is the probability vs cholesterol levels. This one is very interesting between the two datasets in terms of trend levels. With the dav_set having a higher risk at normal levels compared to the sav_set. This could be another hint of a societal change across the years or may in fact be due to the low sample size. Especially since the sav_set matches the general consensus of higher cholesterol levels increasing risk of cardiovascular issues &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/cholesterolvsprobability.jpg" alt="Figure 2.4">&lt;/p>
&lt;p>&lt;strong>Figure 2.4&lt;/strong>: Cholesterol levels vs Probability of cardiovascular issues of the dav_set and sav_set.&lt;/p>
&lt;p>To close out this initial analysis is the correlation map of each of the features. From Figure 2.5 and 2.6 it can be concluded that both of these datasets are viable to conduct machine learning as the correlation factor is below the recommended value of 0.8 &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Although we do see the signs of a low sample amount in the dav_set with a higher correlation factor compared to the sav_set.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davsetcorrelation.jpg" alt="Figure 2.5">&lt;/p>
&lt;p>&lt;strong>Figure 2.5&lt;/strong>: dav_set correlation matrix.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savsetcorrelation.jpg" alt="Figure 2.6">&lt;/p>
&lt;p>&lt;strong>Figure 2.6&lt;/strong>: sav_set correlation matrix.&lt;/p>
&lt;h2 id="3-machine-learning-algorithms-and-implementation">3. Machine Learning Algorithms and Implementation&lt;/h2>
&lt;p>With many machine learning algorithms already available and many more in development. Selecting the optimal one for an application can be a challenging balance since each algorithm has both its advantages and disadvantages. As mentioned in the introduction, we will explore applying the most common and established algorithms available to the public.&lt;/p>
&lt;p>Starting off, is selecting a library from the most popular ones available. Namely Keras, Pytorch, Tensorflow, and Scikit-Learn. Upon further investigation it was determined that Scikit-Learn would be used for this project. The reason being Scikit-Learn is a great general machine learning library that also includes pre and post processing functions. While Keras, Pytorch, and Tensorflow are targeted for neural networks and other higher-level deep learning algorithms which are outside of the scope of this project at this time &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="31-scikit-learn-and-algorithm-types">3.1 Scikit-Learn and Algorithm Types&lt;/h3>
&lt;p>Diving further into the Scikit-Learn library, its key strength appears to be the variety of algorithms available that are relatively easy to implement against a dataset. Of those available, they are classified under three different categories based on the approach each takes. They are as follows:&lt;/p>
&lt;ul>
&lt;li>Classification
&lt;ul>
&lt;li>Applied to problems that require identifying the category an object belongs to.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Regression
&lt;ul>
&lt;li>For predicting or modeling continuous values.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Clustering
&lt;ul>
&lt;li>Grouping similar objects into groups.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For this project, we will be investigating the Classification and Clustering algorithms offered by the library due to the nature of our dataset. Since it is a binary answer, the continuous prediction capability of regression algorithms will not fair well. Compared to classification type algorithms which are well suited for determining binary and multi-class classification on datasets &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Along with Clustering algorithms being capable of grouping unlabeled data which is one of the key problem points mentioned in the introduction &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="32-classification-algorithms">3.2 Classification Algorithms&lt;/h3>
&lt;p>The following algorithms were determined to be candidates for this project based on the documentation available on the Scikit-learn for supervised learning &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="321-support-vector-machines">3.2.1 Support Vector Machines&lt;/h4>
&lt;p>This algorithm was chosen because classification is one of the target types and has a decent list of advantages that appear to be applicable to this dataset &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;ul>
&lt;li>Effective in high dimensional spaces as well as if the number dimensions out number samples.&lt;/li>
&lt;li>Is very versatile.&lt;/li>
&lt;/ul>
&lt;h4 id="322-k-nearest-neighbors">3.2.2 K-Nearest Neighbors&lt;/h4>
&lt;p>This algorithm was selected due to being a non-parametric method that has been successful in classification applications &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. From the dataset analysis, it is appears that the decision boundary may be very irregular which is a strong point of this type of method.&lt;/p>
&lt;h4 id="323-gaussian-naive-bayes">3.2.3 Gaussian Naive Bayes&lt;/h4>
&lt;p>Is an implementation of the Naive Bayes theorem that has been targeted for classification. The advantages of this algorithm is its speed and requires a small training set compared to more advanced algorithms &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="324-decision-trees">3.2.4 Decision Trees&lt;/h4>
&lt;p>This algorithm was chosen to investigate another non-parametric method to determine their efficacy against this dataset application. This algorithm also has some advantages over K-Nearest namely &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>.&lt;/p>
&lt;ul>
&lt;li>Simple to interpret and visualize&lt;/li>
&lt;li>Requires little data preparation
&lt;ul>
&lt;li>Handles numerical and categorical data instead of needing to normalize&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Can validate the model and is possible to audit from a liability standpoint.&lt;/li>
&lt;/ul>
&lt;h3 id="33-clustering-algorithms">3.3 Clustering Algorithms&lt;/h3>
&lt;p>The following algorithms were determined to be candidates for this project based on the table of clustering algorithms available on the Scikit-learn &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="331-k-means">3.3.1 K-Means&lt;/h4>
&lt;p>The usecase for this algorithm is general purpose with even and low number of clusters &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Of which the sav_set appears to have with the even distribution across most of the features.&lt;/p>
&lt;h4 id="332-mean-shift">3.3.2 Mean-shift&lt;/h4>
&lt;p>This algorithm was chosen for its strength in dealing with uneven cluster sizes and non-flat geometry &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Though it is not easily scalable the application of our small dataset size might be of interest.&lt;/p>
&lt;h4 id="333-spectral-clustering">3.3.3 Spectral Clustering&lt;/h4>
&lt;p>As an inverse, this algorithm was chosen for its strength with fewer uneven clusters &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. In comparison to Mean-shift, this maybe the better algorithm for this application.&lt;/p>
&lt;h3 id="34-implementation">3.4 Implementation&lt;/h3>
&lt;p>The implementation of these algorithms were done under the direction of the documentation page for each respective algorithm. The jupyter notebook used for this project is available at &lt;a href="https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/data_analysis/ml_algorithms.ipynb">https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/data_analysis/ml_algorithms.ipynb&lt;/a> with each algorithm having a corresponding cell. A benchmarking library is also included to determine the efficiency of each algorithm in processing time. One thing of note is the lack of functions used for the classification compared to the clustering algorithms. The justification for this discrepancy is due to inexperience in creating optimal implementations as well as determining that not being implemented in a function would not have a significant impact on performance. Additionally, graphs representing the test data were included to help visualize the performance of the clustering algorithms utilizing example code from the documentation &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="341-dataset-preprocessing">3.4.1 Dataset Preprocessing&lt;/h4>
&lt;p>Pre-processing of the cleaned datasets for the classification algorithms was done under guidance of the scikit learn documentation &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. Overall, each algorithm was trained and tested with the same split for each run. While the split data could have been passed directly to the algorithms, they were normalized further using the built-in fit_transform function for the best results possible.&lt;/p>
&lt;p>Pre-processing of the cleaned datasets for the clustering algorithms was done under guidance of the scikit learn documentation &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Compared to the classification algorithms, a dimensionality reduction was conducted using Principal component analysis (PCA). This step condenses the multiple features into a 2 feature array which the clustering algorithms were optimized for, increasing the odds for the best results possible. Another note is the dataset split was conducted during execution of the algorithm. Upon further investigation, it was determined that this does not have an effect on the ending results as the randomization was disabled due to setting the same random_state parameter for each call.&lt;/p>
&lt;h2 id="4-results--discussion">4. Results &amp;amp; Discussion&lt;/h2>
&lt;h3 id="41-algorithm-metrics">4.1 Algorithm Metrics&lt;/h3>
&lt;p>The metrics used to determine the viability of each of the algorithms are precision, recall, and f1-score. These are simple metrics based on the values from a confusion matrix which is a visualization of the False and True Positives and Negatives. Precision is essentially how accurate was the algorithm in classifying each data point. This however, is not a good metric to solely base performance as precision does not account for imbalanced distributions within a dataset &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This is where the recall metric comes in which is defined as how many samples were accurately classified by the algorithm. This is a more versatile metric as it can compensate for imbalanced datasets. While it may not be in our case as seen in the dataset analysis where we have a relatively balanced ratio. It still gives great insight on the performance for our application.&lt;/p>
&lt;p>Finally is the f1-score which is the harmonic mean of the precision and recall metric &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. This will be the key metric we will mainly focus on as it strikes a good balance between the two more primitive metrics. Since one may think in medical applications one would want to maximize recall, it is at the cost of precision which ends up in more false predictions which is essentially an overfitting scenario &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. Something that reduces the viability of the model to the application especially since we have a relatively balanced dataset, more customized weighting is not as necessary.&lt;/p>
&lt;p>The metrics for each algorithm implementation are as follows. The training time metric is provided by the cloudmesh.common benchmark library &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="411-support-vector-machines">4.1.1 Support Vector Machines&lt;/h4>
&lt;p>&lt;strong>Table 4.1:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.94&lt;/td>
&lt;td>0.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.95&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.97&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.038 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.2:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.94&lt;/td>
&lt;td>0.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.95&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>0.97&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>167.897 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="412-k-nearest-neighbors">4.1.2 K-Nearest Neighbors&lt;/h4>
&lt;p>&lt;strong>Table 4.3:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.88&lt;/td>
&lt;td>0.86&lt;/td>
&lt;td>0.87&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.87&lt;/td>
&lt;td>0.90&lt;/td>
&lt;td>0.88&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.025 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.4:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.62&lt;/td>
&lt;td>0.74&lt;/td>
&lt;td>0.67&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.67&lt;/td>
&lt;td>0.54&lt;/td>
&lt;td>0.60&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>10.116 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="413-gaussian-naive-bayes">4.1.3 Gaussian Naive Bayes&lt;/h4>
&lt;p>&lt;strong>Table 4.5:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.88&lt;/td>
&lt;td>0.81&lt;/td>
&lt;td>0.84&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.83&lt;/td>
&lt;td>0.90&lt;/td>
&lt;td>0.86&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.011 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.6:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.90&lt;/td>
&lt;td>0.69&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.72&lt;/td>
&lt;td>0.28&lt;/td>
&lt;td>0.40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.057 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="414-decision-trees">4.1.4 Decision Trees&lt;/h4>
&lt;p>&lt;strong>Table 4.7:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.92&lt;/td>
&lt;td>0.97&lt;/td>
&lt;td>0.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.97&lt;/td>
&lt;td>0.93&lt;/td>
&lt;td>0.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.009 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4.8:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.71&lt;/td>
&lt;td>0.80&lt;/td>
&lt;td>0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>0.66&lt;/td>
&lt;td>0.71&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.272 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="415-k-means">4.1.5 K-Means&lt;/h4>
&lt;p>&lt;strong>Figure 4.1:&lt;/strong> dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davkmeans.jpg" alt="Figure 4.1">&lt;/p>
&lt;p>&lt;strong>Table 4.9:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.22&lt;/td>
&lt;td>0.29&lt;/td>
&lt;td>0.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.12&lt;/td>
&lt;td>0.09&lt;/td>
&lt;td>0.10&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.376 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 4.2:&lt;/strong> sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savkmeans.jpg" alt="Figure 4.2">&lt;/p>
&lt;p>&lt;strong>Table 4.10:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.51&lt;/td>
&lt;td>0.69&lt;/td>
&lt;td>0.59&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.52&lt;/td>
&lt;td>0.34&lt;/td>
&lt;td>0.41&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>1.429 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="416-mean-shift">4.1.6 Mean-shift&lt;/h4>
&lt;p>&lt;strong>Figure 4.3:&lt;/strong> dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davmeanshift.jpg" alt="Figure 4.3">&lt;/p>
&lt;p>&lt;strong>Table 4.11:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.47&lt;/td>
&lt;td>1.00&lt;/td>
&lt;td>0.64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.461 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 4.4:&lt;/strong> sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savmeanshift.jpg" alt="Figure 4.4">&lt;/p>
&lt;p>&lt;strong>Table 4.12:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.50&lt;/td>
&lt;td>1.00&lt;/td>
&lt;td>0.67&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;td>0.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>193.93 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="417-spectral-clustering">4.1.7 Spectral Clustering&lt;/h4>
&lt;p>&lt;strong>Figure 4.5:&lt;/strong> dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davspectral.jpg" alt="Figure 4.5">&lt;/p>
&lt;p>&lt;strong>Table 4.13:&lt;/strong> dav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.86&lt;/td>
&lt;td>0.74&lt;/td>
&lt;td>0.79&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.79&lt;/td>
&lt;td>0.89&lt;/td>
&lt;td>0.84&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>0.628 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 4.6:&lt;/strong> sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savspectral.jpg" alt="Figure 4.6">&lt;/p>
&lt;p>&lt;strong>Table 4.14:&lt;/strong> sav_set metrics&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Precision&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>f1-score&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>No Disease&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.57&lt;/td>
&lt;td>0.57&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Has Disease&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.56&lt;/td>
&lt;td>0.56&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Time&lt;/td>
&lt;td>208.822 sec&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="42-system-information">4.2 System Information&lt;/h3>
&lt;p>Google Collab was used to train and evaluate the models selected. The specifications of the system in use is provided by the cloudmesh.common benchmark library and is listed in Table 4.15 &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Table 4.15&lt;/strong>: Training and Evaluation System Specifications&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attribute&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BUG_REPORT_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://bugs.launchpad.net/ubuntu/%22">https://bugs.launchpad.net/ubuntu/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_DESCRIPTION&lt;/td>
&lt;td>&amp;ldquo;Ubuntu 18.04.5 LTS&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_ID&lt;/td>
&lt;td>Ubuntu&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DISTRIB_RELEASE&lt;/td>
&lt;td>18.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HOME_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://www.ubuntu.com/%22">https://www.ubuntu.com/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ID&lt;/td>
&lt;td>ubuntu&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ID_LIKE&lt;/td>
&lt;td>debian&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NAME&lt;/td>
&lt;td>&amp;ldquo;Ubuntu&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PRETTY_NAME&lt;/td>
&lt;td>&amp;ldquo;Ubuntu 18.04.5 LTS&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PRIVACY_POLICY_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy%22">https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SUPPORT_URL&lt;/td>
&lt;td>&amp;ldquo;&lt;a href="https://help.ubuntu.com/%22">https://help.ubuntu.com/&amp;quot;&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UBUNTU_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION&lt;/td>
&lt;td>&amp;ldquo;18.04.5 LTS (Bionic Beaver)&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION_CODENAME&lt;/td>
&lt;td>bionic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VERSION_ID&lt;/td>
&lt;td>&amp;ldquo;18.04&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu_count&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.active&lt;/td>
&lt;td>698.5 MiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.available&lt;/td>
&lt;td>11.9 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.free&lt;/td>
&lt;td>9.2 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.inactive&lt;/td>
&lt;td>2.6 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.percent&lt;/td>
&lt;td>6.5 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.total&lt;/td>
&lt;td>12.7 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mem.used&lt;/td>
&lt;td>1.6 GiB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>platform.version&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python&lt;/td>
&lt;td>3.6.9 (default, Oct 8 2020, 12:12:24) [GCC 8.4.0]&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.pip&lt;/td>
&lt;td>19.3.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>python.version&lt;/td>
&lt;td>3.6.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sys.platform&lt;/td>
&lt;td>linux&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.machine&lt;/td>
&lt;td>x86_64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.node&lt;/td>
&lt;td>bc15b46ebcf6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.processor&lt;/td>
&lt;td>x86_64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.release&lt;/td>
&lt;td>4.19.112+&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.system&lt;/td>
&lt;td>Linux&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uname.version&lt;/td>
&lt;td>#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>user&lt;/td>
&lt;td>collab&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="43-discussion">4.3 Discussion&lt;/h3>
&lt;p>In analyzing the resulting metrics in section 4.1, two major trends between the algorithms are apparent.&lt;/p>
&lt;ol>
&lt;li>The classification algorithms perform significantly better than the clustering algorithms.&lt;/li>
&lt;li>Significant signs of overfitting for the dav_set.&lt;/li>
&lt;/ol>
&lt;p>Addressing the first point, it is obvious from the metric performance where on average the classification algorithms were higher than the clustering algorithms. At a lower training time cost as well, which indicates that classification algorithms are well suited for this application than clustering. Especially when looking at the results for Mean-Shift in section 4.1.6 where the algorithm failed to identify any patient with a disease. This also illustrates the discussion on the metrics used to determine performance as the recall was 100% at the cost of missing every patient that would have required treatment illustrated by Figure 4.3 and 4.4. On this topic, comparing the actual data graphs for each of the clustering algorithms and comparing them to the example clustering figures within the scikit documentation, it solidifies that this is not the correct algorithm type for this dataset &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Moving on to the next point, it can be seen that overfitting is occurring for the dav_set in comparing the performance to the sav_set for the same algorithm which can be seen in the corresponding tables in sections 4.1.2, 4.1.3, and 4.1.4. Here the performance gap is at least 20% between the two compared to what one would assume should be relatively close to each other. While this could also illustrate the affect the various features have on the algorithm, it was determined that this is most likely due to the small dataset size having a larger influence than anticipated.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>Reviewing these results, a clear conclusion cannot be accurately be determined due to the considerable amount of variables involved that were not able to be isolated to a desirable level. Namely the compromises that were mentioned in section 2.1 and general dataset availability. However, it was determined that the main goal of this project was accomplished where the Support Vector Machine algorithm was narrowed down as a viable candidate for future work. Due in part to the overall f1-score performance for both datasets, providing confidence that overfitting may not occur. While there is a downside in scalability due to the significant increase in training time between the smaller dav_set and larger sav_set. This could indicate that further research should be focused on either improving this algorithm or creating a new one based on the underlying mechanism.&lt;/p>
&lt;p>In relation to the types of features, it could be interpreted from this project that further efforts require a more expansive and modern dataset to perform to a level suitable for real world applications. As possible factors affecting the performance are in the accuracy and granularity of the measurements and factors available to learn from. This however, is seen to be a difficult challenge due to the nature of privacy laws on health data but, as proposed in the introduction. It would be very interesting to apply this project&amp;rsquo;s findings on more general health data that is retrieved in annual visits.&lt;/p>
&lt;h2 id="6-acknowledgements">6. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their assistance and suggestions with regard to this project.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Centers for Disease Control and Prevention. 2020. Heart Disease Facts | Cdc.Gov. [online] Available at: &lt;a href="https://www.cdc.gov/heartdisease/facts.htm">https://www.cdc.gov/heartdisease/facts.htm&lt;/a> [Accessed 16 November 2020]. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Amadeo, K., 2020. Preventive Care: How It Lowers Healthcare Costs In America. [online] The Balance. Available at: &lt;a href="https://www.thebalance.com/preventive-care-how-it-lowers-aca-costs-3306074">https://www.thebalance.com/preventive-care-how-it-lowers-aca-costs-3306074&lt;/a> [Accessed 16 November 2020]. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>WebMD. 2020. Understanding Your Cholesterol Report. [online] Available at: &lt;a href="https://www.webmd.com/cholesterol-management/understanding-your-cholesterol-report">https://www.webmd.com/cholesterol-management/understanding-your-cholesterol-report&lt;/a> [Accessed 21 October 2020]. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>R, V., 2020. Feature Selection â€” Correlation And P-Value. [online] Medium. Available at: &lt;a href="https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf">https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf&lt;/a> [Accessed 21 October 2020]. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Stack Overflow. 2020. Differences In Scikit Learn, Keras, Or Pytorch. [online] Available at: &lt;a href="https://stackoverflow.com/questions/54527439/differences-in-scikit-learn-keras-or-pytorch">https://stackoverflow.com/questions/54527439/differences-in-scikit-learn-keras-or-pytorch&lt;/a> [Accessed 27 October 2020]. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.4. Support Vector Machines â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/svm.html#classification">https://scikit-learn.org/stable/modules/svm.html#classification&lt;/a> [Accessed 27 October 2020]. &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 2.3. Clustering â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/clustering.html#clustering">https://scikit-learn.org/stable/modules/clustering.html#clustering&lt;/a> [Accessed 27 October 2020]. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1. Supervised Learning â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning">https://scikit-learn.org/stable/supervised_learning.html#supervised-learning&lt;/a> [Accessed 27 October 2020]. &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.6. Nearest Neighbors â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/neighbors.html">https://scikit-learn.org/stable/modules/neighbors.html&lt;/a> [Accessed 27 October 2020]. &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.9. Naive Bayes â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/naive_bayes.html">https://scikit-learn.org/stable/modules/naive_bayes.html&lt;/a> [Accessed 27 October 2020]. &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 1.10. Decision Trees â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/tree.html">https://scikit-learn.org/stable/modules/tree.html&lt;/a> [Accessed 27 October 2020]. &lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. A Demo Of K-Means Clustering On The Handwritten Digits Data â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py">https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py&lt;/a> [Accessed 17 November 2020]. &lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>Scikit-learn.org. 2020. 6.3. Preprocessing Data â€” Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href="https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing">https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing&lt;/a> [Accessed 17 November 2020]. &lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Mianaee, S., 2020. 20 Popular Machine Learning Metrics. Part 1: Classification &amp;amp; Regression Evaluation Metrics. [online] Medium. Available at: &lt;a href="https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce">https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce&lt;/a> [Accessed 10 November 2020]. &lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a> &lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-312/assignment6/assignment6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-312/assignment6/assignment6/</guid><description>
&lt;h1 id="engr-e-534-assignment-6-ai-in-health-and-medicine">ENGR-E 534 Assignment 6: AI in Health and Medicine&lt;/h1>
&lt;h1 id="ai-enabled-covid-19-diagnostic-framework-utilizing-smartphone-based-embedded-sensors">AI-enabled COVID-19 diagnostic framework utilizing Smartphone-based Embedded Sensors&lt;/h1>
&lt;p>Saptarshi Sinha, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/">fa20-523-312&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/blob/master/project/project.md">Edit&lt;/a>&lt;/p>
&lt;p>&lt;strong>Keywords:&lt;/strong> smartphone, neural networks, CNN, RNN, embedded sensors, symptom detection, cloud computing&lt;/p>
&lt;h2 id="1-background-the-need-for-smarter-and-more-pervasive-covid-19-monitoring">1. Background: The need for smarter and more pervasive COVID-19 monitoring&lt;/h2>
&lt;p>As mankind grapples with the menacing threat of an ongoing pandemic involving the novel COVID-19 (coronavirus infection), researchers and clinicians across the board have tirelessly involved themselves in myriad efforts for controlling the relentless proliferation of this virus so as to check the viral-driven casualties across the globe. It might seem that for the very first time, science and technology have been put to its greatest test ever. It seems that only time can tell if our scientific valor is indeed powerful enough to succeed in such a test, or if the virus would instead claim a major portion of the worldâ€™s population as its unfortunate casualty.&lt;/p>
&lt;p>Numerous scientific approaches have been fielded in a relatively short amount of time to deal with the current problem. Many approaches involve novel technologies such as remote video surveillance using assistive robots that monitor virus-inflicted patients, while also protecting those healthcare workers by not involving them in such in-person diagnostic processes. Other approaches involve using machine learning based methodologies for sorting out patents with the virus from those without it simply by using an efficient algorithmic procedure of analyzing different aspects of patientsâ€™ CT scans. Major companies have also stepped in to assist in a war-footing format. As an example, Amazon Care is providing pick-up and delivery-based services of test-kits in particular virus-prone locations. Appleâ€™s Siri is now able to provide symptom-based guidance in relation to COVID-19. Microsoft helped creating the Adaptive Biotechnologies platform that studies how our immune system responds to the virus which can provide insights for establishing drug development procedures. Finally, various biotechnological companies all across the world have started conducting extensive research into vaccine development and drug development procedures to combat this novel strain of the virus.&lt;/p>
&lt;p>As amazing these techniques might seem at a superficial glance, the major setback the world is suffering from is with the extent of the viral spread that is amplified due to the lack of testing capabilities. They are either inadequate or cannot handle an entire nationâ€™s population. Although proactive actions have been employed in many nations, testing kits are still being produced slowly. This gives the virus an unfair advantage as time is of the essence. People (esp. asymptomatic individuals) with the virus remain undiagnosed for a greater length of time during which they can inadvertently aid with the proliferation of the viral disease. In this particular context, a very novel strategy for COVID-19 testing and diagnosis will be discussed that utilizes something that we all possess â€“ a smartphone device.&lt;/p>
&lt;h2 id="2-design--working-principle-ai-based-diagnostic-framework-for-covid-19-utilizing-smartphone-based-embedded-sensors-and-artificial-neural-networks">2. Design &amp;amp; Working Principle: AI-based diagnostic framework for COVID-19 utilizing Smartphone-based Embedded Sensors and Artificial Neural Networks&lt;/h2>
&lt;p>Cornell Universityâ€™s archive on Human Computer Interaction (HCI) features a recent article that discusses a strategy involving COVID-19 diagnosis with smartphone-based sensors. In its simplest form, the framework includes the smartphone, and its accompanying sensors and algorithms. External hardware accessories with high-power consumption, or access to specialized equipment is not required for this design &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Since the application framework involves something that common people use on a daily basis, no tutorials or expert assistance is required to work with such an application. To understand the framework better, we must first note the various symptom types that are exhibited by COVID-19 patients which include high fever, tiredness, dry cough, intense headache, shortness of breath, nausea, etc. To efficiently capture the symptoms, an essential piece of information to keep in mind here is that modern smartphone devices come equipped with various in-built sensors viz. camera sensor, inertial sensor, temperature sensor, accelerometer sensor, microphones, etc. Many previous endeavors utilized such sensors to detect symptoms for other diseases &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. For instance, temperature-fingerprint sensor was used previously for measuring fever-levels; camera sensors (with accelerometers) were utilized earlier to analyze fatigue levels via pattern-recognition algorithms for human-gait analysis; camera sensor (with inertial sensor) were also used for analyzing neck posture to evaluate the headache severities; and, even the microphone was utilized previously for analyzing a patientâ€™s cough-noise in a diagnostic process &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The research article describes a strategy which uses these various smartphone sensors and their respective algorithms. This is followed up by creating a dataset record comprising predicted levels of the different symptoms which are collected from different patients and studied using deep learning approaches &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Chiefly, it uses Convolutional Neural Networks (CNN) to analyze spatial data (viz. imaging data from the camera sensor), and Recurrent Neural Networks (RNN) for temporal data (viz. signal or text-based measurements) &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The entire prediction-based framework can be summarized as follows:&lt;/p>
&lt;p>&lt;img src="images/Picture1.png" alt="Smartphone-based framework for COVID-19 testing">&lt;/p>
&lt;p>
&lt;img src="path_to_image" alt>
&lt;em>Figure 1: Smartphone-based framework for COVID-19 testing; Source: Adapted from [^1]&lt;/em>
&lt;/p>
&lt;p>The above framework can be sub-divided into four important layers which provides further insights into the different procedures going on in the background while the system makes the disease predictions.&lt;/p>
&lt;h3 id="i-reading">i. Reading&lt;/h3>
&lt;p>The first layer involves reading based functionalities for the data coming from different smartphone sensors. This could refer to arrays of different types of data coming from different sources (viz. CT scan imageries, accelerometer readings, microphone sound signals, etc.).&lt;/p>
&lt;h3 id="ii-configurations">ii. Configurations&lt;/h3>
&lt;p>The second layer deals with configuring onboard sensors for varied metrics such as time intervals, image resolution, etc. Readings from these first two steps are fed as inputs for the â€œsymptoms algorithmâ€ that can be executed as a smartphone application.&lt;/p>
&lt;h3 id="iii-symptoms-prediction">iii. Symptoms Prediction&lt;/h3>
&lt;p>The third layer deals with symptoms-level evaluation. The result is stored as a record that can be fed as an input for the next layer.&lt;/p>
&lt;h3 id="iv-covid-19-prediction">iv. COVID-19 Prediction&lt;/h3>
&lt;p>Finally, the last layer involves the application of deep learning (DL) based algorithms to the input data for predicting whether the patient has been afflicted with the virus. A CNN and RNN based combined process is utilized here such that the system can analyze both the spatial data (viz. image pixels) as well as the temporal data (viz. text/signal information) [1].&lt;/p>
&lt;h2 id="3-discussions-augmentation-with-cloud-comuputing-capabilities">3. Discussions: Augmentation with Cloud-Comuputing capabilities&lt;/h2>
&lt;p>To enhance the performance of this framework, the recorded data and predicted results can be uploaded to cloud-computing servers. This can help researchers and medical professionals from all around the globe in exchanging information and insights involving accurate patient diagnosis. Such applications are already developing. For instance, IBM recently launched the COVID-19 High Performance Computing Consortium &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. As the name suggests, this consortium has been explicitly designed to tackle the threat of COVID-19 by harnessing enormous computing power for streamlining the search for more information, aiding the hunt of possible treatment paths, and creating drug-and-disease based informational repositories that are made available to appropriate and eligible researchers and institutions strewn all across the globe &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>All in all, it is indeed very commendable on the part of these researchers to facilitate the design of such a low-cost yet effective method of diagnosing COVID-19 when testing capacities are severely limited. If used appropriately, it can stem the spread of this virus by making it possible to diagnose patients sooner and quarantining them. Of course, the strategy does not focus on the treatment itself. But in the current scenario, where we have arrived at a breaking point with this disease, it would greatly assist healthcare personnel with locating and quarantining patients, that would indirectly help saving scores of other lives.&lt;/p>
&lt;h2 id="references">References:&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Maghdid, Halgurd S., et al. â€œA Novel AI-Enabled Framework to Diagnose Coronavirus COVID 19 Using Smartphone Embedded Sensors: Design Study.â€ ArXiv:2003.07434 [Cs, q-Bio], May 2020. arXiv.org, &lt;a href="http://arxiv.org/abs/2003.07434">http://arxiv.org/abs/2003.07434&lt;/a> &lt;/br> &lt;/br> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>D. Gil, â€œIBM Releases Novel AI-Powered Technologies to Help Health and Research Community Accelerate the Discovery of Medical Insights and Treatments for COVID-19â€, ibm.com, Apr. 3, 2020. [Online]. Available: &lt;a href="https://www.ibm.com/blogs/research/2020/04/ai-powered-technologies-accelerate-discovery-covid-19/">https://www.ibm.com/blogs/research/2020/04/ai-powered-technologies-accelerate-discovery-covid-19/&lt;/a> [Accessed Oct. 17, 2020] &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-312/project/project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-312/project/project/</guid><description>
&lt;h1 id="aquatic-toxicity-analysis-with-the-aid-of-autonomous-surface-vehicle-asv">Aquatic Toxicity Analysis with the aid of Autonomous Surface Vehicle (ASV)&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Saptarshi Sinha, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/">fa20-523-312&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>With the passage of time, human activities have created and contributed much to the aggrandizing problems of various forms of environmental pollution. Massive amounts of industrial effluents and agricultural waste wash-offs, that often comprise pesticides and other forms of agricultural chemicals, find their way to fresh water bodies, to lakes, and eventually to the oceanic systems. Such events start producing a gradual increase in the toxicity levels of marine ecosystems thereby perturbing the natural balance of such water-bodies. In this endeavor, an attempt will be made to analyze the various water quality metrics (viz. temperature, pH, dissolved-oxygen level, and conductivity) that are measured with the help of autonomous surface vehicles (ASV). The collected data will undergo big data analysis tasks so as to find the general trend of values for the water quality of the given region. These obtained values will then be compared with sample water quality values obtained from neighboring sources of water for ascertaining if these sample values exhibit aberration from the established values that were found earlier from the big data analysis tasks for water-quality standards. In the event, the sample data popints significantly deviate from the standard values established earlier, it can then be successfully concluded that the aquatic system in question, from which the water sample was sourced from, has been degraded and may no longer be utilized for any form of human usage, such as being used for drinking water purposes.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-hardware-component">4.1 Hardware Component&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-software-component">4.2 Software Component&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#421-data-pre-processing">4.2.1 Data Pre-processing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#422-attributes-of-the-preliminary-data">4.2.2 Attributes of the preliminary data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#423-unsupervised-learning-k-means-clustering-analysis-safe--unsafe-centroid-calculation">4.2.3 Unsupervised learning: K-means clustering analysis (&amp;ldquo;Safe&amp;rdquo; &amp;amp; &amp;ldquo;Unsafe&amp;rdquo; Centroid calculation)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#424-display-of-results--analysis-of-any-given-sample-values-set">4.2.4 Display of results &amp;amp; analysis of any given sample values set&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-inference">5. Inference&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-analysis-of-extracted-data-and-statistical-information">5.1 Analysis of extracted data and statistical information&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-centroids--predicted-classesclusters">5.2 Centroids &amp;amp; Predicted Classes/Clusters&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53-heatmaps-for-the-years---2017-2018-2019-and-2020">5.3 Heatmaps for the years - 2017, 2018, 2019, and 2020&lt;/a>&lt;/li>
&lt;li>&lt;a href="#54-analysis-of-sample-set-of-values">5.4 Analysis of sample set of values&lt;/a>&lt;/li>
&lt;li>&lt;a href="#55-benchmark-information">5.5 Benchmark information&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> toxicology, pollution, autonomous systems, surface vehicle, sensors, arduino, water quality, data analysis, environment, big data, ecosystem&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>When it comes to revolutionizing our qualities of life and improving standards, there is not another branch of science and technology that has made more impact than the myriad technological capabilities offered by the areas of Artificial Intelligence (AI) and its sub-fields involving Computer Vision, Robotics, Machine Learning, Deep Learning, Reinforcement Learning, etc. It should be borne in mind that AI was developed to allow machines/computer processors to work in the same way as the human brain works and which could make intelligent decisions at every conscious level. It was meant to help with tasks for rendering scientific applications more smarter and efficient. There are many tasks that can be performed in a far more dexterous fashion by employing smart-machines and algorithms than by involving human beings. But even more importantly, AI has also been designed to perform tasks that cannot be successfully completed by employing human beings. This could either be due to the prolonged boredom of the task itself, or a task that involves hazardous environments that cannot sustain life-forms for a long time. Some examples in this regard would involve exploring deep mines or volcanic trenches for mineral deposits, exploring the vast expanse of the universe and heavenly bodies, etc. And this is where the concept employing AI/Robotics based technology fits in perfectly for aquatic monitoring and oceanographical surveillance based applications.&lt;/p>
&lt;p>Toxicity analysis of ecologically vulnerable water-bodies, or any other marine ecosystem for that matter, could give us a treasure trove of information regarding biodiversity, mineral deposits, unknown biophysical phenomenon, but most importantly, it could also provide meaningful and scientific information related to the degradation of the ecosystem itself. In this research endeavor, an attempt will be made to utilize aquatic Autonomous Surface Vehicle (ASV) that will be deployed in marine ecosystems and which can continue collecting data for a prolonged period of time. Such vehicles are typically embedded with different kinds of electronic sensors, that are capable of measuring physical quantities such as temperature, pH, specific conductance, dissolved oxygen level, etc. The data collected by such a system can either be over a period of time (temporal data), or it could cover a vast aquatic geographical region (spatial data). This is the procedure by which environmental organizations record and store massive amounts of data that can be analyzed to obtain useful information about the particular water body in question. Such analytical work will provide us with statistical results that can then be compared with existing sample values so as to decipher whether the water source, from where the sample was obtained, manifests normal trend of values or shows large deviations from established trends that can signify an anomaly or biodegradation of the ecosystem. The datasets used in this endeavor are provided publicly by environmental organizations in the United States, such as the US Geological Survey (USGS). While the primary goal involves conducting big data analysis tasks for the databases so as to obtain useful statistical results, a secondary goal in this project involves finding out the extent to which a particular sample of water, obtained from a specific source of water, deviates from the normal trend of values. The extent of such deviations can then give us an indication about the status of the aquatic degradation of the ecosystem in question. The data analysis framework will be made as robust as possible and in this effort, we will work with data values that are spread over multiple years and not just focused on a single year.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>After reviewing the necessary background literature and previous work that has been done in this field, it can be stated that most of such endeavors focused majorly on environmental data collection with the help of sensors attached to a navigational buoy in a particular location of a water-body. Such works did not involve any significant data analysis framework and focused on particular niche areas. For instance, a particular research effort involved deploying a surface vehicle that collected data from large swaths of geographical areas in various water bodies but concentrated primarily on different algorithms employed for vehicular navigation and their relative success rates &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Other research attempts focussed on even more niche areas such as study of the migration pattern exhibited by zooplanktons upon natural and aritifical irradiance &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>, and detection and monitoring of marine fauna &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Although these are interesting projects and can provide us with novel information about various aspects of biological and aquatic research, such research attempts neither focused much on the data analysis portion for multiple sensory inputs (viz. temperature, pH, specific conductance, and dissolved oxygen level, which are the four most important water quality parameters) nor did they involve an intricate procedure to compare the data with sample observations so as to arrive at a suitable conclusion regarding the extent of environmental degradation of a particular water body.&lt;/p>
&lt;p>As mentioned in the previous section, this research endeavor will exhaustively focus not just on the working principles and deployment of surface vehicles to collect data, but it will also involve employing deeper study towards the subject of big-data analysis of both the current data of the system in question and the past data obtained for the same aquatic profile. In this way, it would be possible to learn more about the toxicological aspects of the ecosystem in question and which can then be also applied to neighboring regions.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>Upon exploring a wide array of available datasets, the following two data repositories were given consideration to get the required water quality based data over a particular period of time and for a particular geographical region:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://waterdata.usgs.gov/nwis/qw">USGS Water Quality Data&lt;/a> &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/li>
&lt;li>&lt;a href="https://www.epa.gov/waterdata/water-quality-data-download">EPA Water Quality Data&lt;/a> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>After going through the sample data values than can be visualized from the respective websites of USGS and EPA, the USGS datasets were chosen over the EPA datasets. This is mainly because the USGS datasets are more commensurate with the research goal of this endeavor, especially since it contains a huge array of databases that focuses on the four most important water quality data which are - temperature, pH, specific conductance, and dissolved oxygen level. Some previous work was conducted on similar USGS datasets by a particular research team &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. However, such work was drastically different in nature, when compared with this research attempt, since its emphasis was on a very broad perspective so as to create an overview of how to use and visualize the data from the USGS water quality portal. Besides, such work emphasizes on characterizing the seasonal variation of lake water clarity in different regions throughout the continental US, something that is very deviant from what would be addressed in this particular article which majorly involves studying environmental degradation and aquatic toxicology from the context of big data analytical tasks.&lt;/p>
&lt;p>To address the questions involving existence of multiple data-sets and motivation of using multiple data-sets, we must keep in mind that the very nature of this study is based on historical trends of the nature of water-quality in a particular region from the past and to this effect, emphasis has been given to use data values from the past years as well in addition to the current year. The geographical location for this analysis was chosen to be the East Fork Whitewater River that is located at Richmond, IN (USA). The years that have been chosen in this case are 2017, 2018, 2019, and 2020. For all these years, focus would be placed on the same time-period, that spans from November 1 to November 14, for all these years so as to establish consistency and high fidelity across the borad. Having multiple data-sets in this way will help us in achieving robust data-analytical results. It would also ensure that too much focus is not given on outlier cases, that may be relevant to just a particular time and day on a given year, or an aberration in the data that may only have surfaced due to an unknown underlying phenomenon or some form of cataclysmic event from the past. Using multiple datasets would help to get a resultant data structure that is more likely to converge towards an approximate level of historical thresholds and which can then be used to find out how a current sample data-point deviates from such established trends of previous patterns.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;h3 id="41-hardware-component">4.1 Hardware Component&lt;/h3>
&lt;p>Although this project focuses more on the data analysis portion than mechanical details, some information relating to the design and working principle of ASVs is provided herewith.The rough outline of an autonomous surface vehicle (ASV) in question has been perceived in Autodesk Fusion 360, which is a software package that helps creating and printing three-dimensional custom designs. A preliminary model has been designed in this software and after printing, it can be interfaced with the appropriate sensors in question. The system can be driven by an Arduino-Uno based microcontroller or even a Raspberry Pi based microprocessor, and it can comprise different types of environmental sensors that helps with collecting and offloading data to remote servers/machines. Some of these sensors can be purchased commercially from the vendor, &amp;ldquo;Atlas Scientific&amp;rdquo; &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. For instance, the following sensors can be used with an ASV to measure the four most important water quality parameters involving temperature, pH, dissolved oxygen level, and specific conductance values:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://atlas-scientific.com/kits/pt-1000-temperature-kit/">PT-1000 Temperature sensor kit&lt;/a> &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/li>
&lt;li>&lt;a href="https://atlas-scientific.com/kits/ph-kit/">Potential of Hydrogen (pH) sensor kit&lt;/a> &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/li>
&lt;li>&lt;a href="https://atlas-scientific.com/kits/dissolved-oxygen-kit/">Dissolved Oxygen (DO) sensor kit&lt;/a> &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/li>
&lt;li>&lt;a href="https://atlas-scientific.com/kits/conductivity-k-1-0-kit/">Conductivity K 1.0 sensor kit&lt;/a> &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/li>
&lt;/ul>
&lt;p>A very rudimentary framework of such a system has been realized in the Autodesk Fusion 360 software architecture as shown below. A two-hull framework is usually more helpful than a single hull based design since the former would help with stability issues especially while navigating through choppy waters. Figure 1 shows the design in the form of a very simplistic platform but which definitely lays down the foundation for a more complex structure for an ASV system.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/asvdesign.png" alt="ASV from Fusion 360">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Nascent framework of an ASV system in Fusion 360&lt;/p>
&lt;p>With the chassis framework out of the way, a careful analysis could be conducted towards the other successful components of such a vehicle so as to complete the entire build process for a fully functional prototype ASV. In essence, an ASV can be thought of being composed of certain key sub-elements. From a broad perspective, they comprise the hardware makeup, a suitable propulsion system, a sensing system, a communication system, and an appropriate source of onboard power source. The hardware makeup being out of the way, the other aspects can now be elaborated as follows:&lt;/p>
&lt;p>&lt;strong>Propulsion System:&lt;/strong> Primarily, the two major possibilities for propulsion systems in an ASV involve using either a single servo motor with an assortment of rudders and propellers for appropriate steering, or using two separate servo motors, one of which will drive the left-hand side of the system and the other would drive the right-hand side. The second arrangement is preferred in many scenarios as it provides with better maneuverability and control of the system as a whole. For instance, to move forward in a rectilinear fashion, both the motors would be given the same level of power. Whereas for steering the system in a particular direction, one of the motors would be assigned a lower power level than the other, thereby enabling the system to curve inwards on the side which has the motor with a lower power level. Of course, there will always be perturbations and natural disturbances that will deter the system from making these correct path changes. For this reason, a Proportional-Integral-Derivative (PID) controlled response could be augmented with the locomotion algorithms.&lt;/p>
&lt;p>&lt;strong>Sensing System:&lt;/strong> An ASV can have as many sensors as possible (dependent upon physical and electrical constraints of microcontroller/microprocessor) but for a study like this, an arrangement involving four different sensors needs to be integrated in the ASV which measures the four principle water quality parameters. This way, when the entire ASV system is deployed in an aquatic environment, it will be able to simultaneously provide readings for all four water-quality parameters in this case. Precisely, these water-quality parameters would be temperature, potential of hydrogen (pH), dissolved oxygen level, and specific conductance value. It should be noted in this perspective that it is possible to include even more sensors in this ASV system. However, the reason why it is generally not helpful to go beyond a certain number of sensors is primarily because of two reasons. Firstly, these parameters are important to most toxicological analysis studies &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, and the readings provided by such a sensory system could be considered as a foundation which could provide future directions (including adding more sensors, if needed). Secondly, we should also keep in mind that the hardware system has certain constraints. In this scenario, it involves a sensory shield (that can be integrated with a microcontroller or microprocessor) which can be used for incorporating multiple sensors. But it also has a maximum of four ports for four different sensors only. Though it is possible to add multiple layers of shield on top of the others (thereby raising the capability of integrating the number of sensors to eight or even more), it leads to unnecessary bandwidth issues, memory depletion possibilities, along with an increased demand of higher power supply. These issues will especially be more consequential if we are dealing with a microcontroller that has very limited memory and power, unlike a microprocessor. Hence, the decision to stick with only a certain number of sensors is really an important one.&lt;/p>
&lt;p>&lt;strong>Communication System:&lt;/strong> This is possibly the most important part of the ASV system as we need to device a technique to offload the data that is collected by the vehicle back to a remote computer/server that would likely be located at a considerable distance away from the ASV. There are different options that can be considered in this regard for establishing a proper communicative functionality between the ASV and the remote computer. Some options that are typically considered involve Bluetooth, IR signals, RF signals, GPS-based system, satellite communication, etc. There are both pros and cons when it comes to using any of these different communication systems for the ASV. However, the most important metric in this case involves the maximum range the communication system could span over. Obviously, some of the options (viz. Bluetooth) would not be possible in this regard as they have a very limited communication range. Some others (viz. satellite communication systems) have a very high range but are nevertheless not feasible for small-scale research endeavors as they require too much onboard processing power to even carry out their most basic operations. Hence, a balanced approach is normally followed in these scenarios and a GPS/RF-based system is often found to be a reliable candidate for carrying out the proposed tasks of an ASV.&lt;/p>
&lt;p>&lt;strong>Power Source:&lt;/strong> Finally, we certainly need an onboard processing power system that can provide the required amount of power to all the functional entities housed in the ASV system. The characteristic of such a desired power source would be that it would not require frequent charging and can sustain proper ASV operations for at least five to six hours. Additionally, the weight of the power source should also not be too clumsy that might put the stability of the entire ASV system in jeopardy. It must have a suitable weight, and should also come in an appropriate shape and size such that the weight of the entire power module is evenly distributed over a large area, thereby further reinforcing the stability of the system.&lt;/p>
&lt;h3 id="42-software-component">4.2 Software Component&lt;/h3>
&lt;p>With the help of the collected data from ASVs, the datasets of USGS are then prepared which meticulously tabulates all the readings from the different water sensors of the ASV. Such tabular data is made public for research and other educational purposes. In this endeavor, such datasets will be analyzed to decipher the median convergent values of the water body for the four different parameters that have been measured (i.e. temperature, pH, dissolved oxygen level, and specific conductance). The results of this data analysis task will manifest the water quality parametric values and standards for the particular aquatic ecosystem. Such a result will then be used to find out if a different water sample value sourced from a particular region deviates by a large proportion from the established standards which was obtained after analyzing the historical data from USGS for a regional source of water. The USGS website makes it easier to find data from a nearby geographical region by making it possible to enter the desired location prior to searching for water quality data in their huge databases. In this way, one can also use these databases to figure out if the water quality parameters of the particular ecological system varies wildly from a neighboring system that has almost the same geographical and ecological attributes.&lt;/p>
&lt;p>The establishment of the degree of variance of the sample data from the normal standards will be carried out by deciphering the number of water quality paramteric values that are aberrant in nature. For instance, a sample value with only an aberrant pH value could be classified as &amp;ldquo;Critical Category 1&amp;rdquo; whereas, a sample value with aberrant values for pH, temperature, and specific conductance would be classified as &amp;ldquo;Critical Category 3&amp;rdquo;. The aberrant nature of a particular parameter is postulated by enumerating how far the values are away from the established median data, which was obtained from the past/historical datasets. This will involve centroid-based calculations for the k-means algorithm (discussed in the next section). Such aberrant nature of a particular water quality paramteric value can also be figured out by using the context of standard deviations and quartile ranges. For instance, if the current data resides in the second quartile, it can be demarcated as being more or less consistent with previously established values. However, if it resides in the first or third quartile then it might be that the particular ecosystem has aberrant aspects, which would then need to be investigated for possible effects of outside pollutants (viz. industrial effluents, agricultural wash-off, etc.), or presence of harmful invasive species that might be altering the delicate natural balance of the ecosystem in question.&lt;/p>
&lt;p>The software logic is located at this &lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/blob/main/project/code/toxicologyASV.ipynb">link&lt;/a> &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. It has been created using the aid of Google Colaboratory platform, or simply Colab &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. The code has been appropriately commented and documented in such a way that it can be easily reproduced. Important instructions and vital information about different aspects of the code have been properly written down wherever necessary. The coding framework helps in corroborating the inferences and conclusions made by this research attempt, and which are described in detail in the subsequent sections. The major steps that have been followed in establishing the software logic for this big-data analytical task are discussed below.&lt;/p>
&lt;h4 id="421-data-pre-processing">4.2.1 Data Pre-processing&lt;/h4>
&lt;p>&lt;strong>Meta-data&lt;/strong>: As with any other instance of big data, the data obtained from the USGS website is rife with many unnecessary information. Most of these information relate to meta-data for the particular database and it also contains detailed logistical information such as location information, units used for measurement, contact information, etc. Fortunately, all these information have been bunched up nicely at the beginning of the database and they were conveniently filtered out by skipping the first thirty-four (34) rows while reading the corresponding comma separated value (csv) files.&lt;/p>
&lt;p>&lt;strong>Extraction of required water-quality parameters (Temperature, Specific Conductance, pH, Dissolved Oxygen)&lt;/strong>: After filtering out the meta-data, it is very essential to focus only on the relevant portion of the database which contains the required information that is needed for the data analysis tasks. In this case, if we observe carefully, we will notice that not all the columns contain the required information relating to water-quality parameters. Some of them include information such as date, time, system&amp;rsquo;s unit, etc. Since these will not be required for our analysis task, we extract only those columns that contain information relating to the four primary water-quality parameters. Figure 2 displays the layout of the USGS dataset files containing all the extraneous information that are deemed unnecessary for the big data analysis task.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/sampledatabase.png" alt="database sample">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Sample Database file obtained from the USGS water-quality database for the year 2017&lt;/p>
&lt;h4 id="422-attributes-of-the-preliminary-data">4.2.2 Attributes of the preliminary data&lt;/h4>
&lt;p>&lt;strong>Seasonal Consistency&lt;/strong>: The preliminary data, that was pre-processed and extracted, was plotted to visualize the basic results. The data pertains to a particular duration of time in a specific seasonal time of the year, more importantly that spans the first two weeks of November for all the four years. This has been done to maintain consistency of results across the platform and to create as much as a robust framework as possible that can have high fidelity.&lt;/p>
&lt;p>&lt;strong>Data-points as x-axis&lt;/strong>: Additionally, it should be kept in mind that the data has already been time-stamped rigorously. More precisely, the databases that are uploaded to the USGS website have data tabulated in them that are arranged in a chronological manner. As a result, having the x-axis refer to actual data-points means the same if we had the x-axis refer to time instead. These plotted results have been provided in the next section.&lt;/p>
&lt;p>&lt;strong>Visualization of trends&lt;/strong>: The preliminary plotting of the data helps us to visualize the overall trends of the variation of the four important water-quality parameters. This gives an approximate idea regarding what we should normally expect from the water-quality data, the approximate maximum and minimum range of values, and it further helps in detecting any kind of outlier situations that might arise either due to the presence of artifacts, or nuisance environmental variables.&lt;/p>
&lt;h4 id="423-unsupervised-learning-k-means-clustering-analysis-safe--unsafe-centroid-calculation">4.2.3 Unsupervised learning: K-means clustering analysis (&amp;ldquo;Safe&amp;rdquo; &amp;amp; &amp;ldquo;Unsafe&amp;rdquo; Centroid calculation)&lt;/h4>
&lt;p>&lt;strong>General Clustering Analysis&lt;/strong>: The concept behind any clustering based approach involves an unsupervised learning mechanism. This means that the dataset traditionally does not come labelled and the task in hand is to find patterns within the data-set based on suitable metrics. These patterns help delineate the different clusters and classify the data by assigning them to one of these clusters. This process is usually carried out by measuring the euclidean distance of each point from the &amp;ldquo;centroids&amp;rdquo; of every cluster. The resultant clusters are created in such a way that the distance within the points in a particular cluster are minimized as much as possible whereas, the corresponding distance between points from different clusters are maximized. Figure 3 summarizes this concept of clustering technique.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/clusteranalysis.png" alt="clustering concept">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Concept of Clustering analysis adopted as an unsupervised learning process &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>&lt;/p>
&lt;p>&lt;strong>K-means Classification: Centroid Calculation&lt;/strong>: The algorithm for this step first starts with selecting a random centroid value to start with to begin the iteration process. In order to be rigorous in this regard, the initial points for centroid values were chosen to be one standard deviation away from the median values for the four water-quality parameters. More importantly, two initial centroid values were chosen that would result in two clusters, one belonging to the safe water-standard cluster and the other belonging to the unsafe category. For the safe centroid value, the point chosen was such that it was one standard deviation lower than the median values for the temperature and specific conductance parameters, whereas it was one standard deviation higher than the median values for the pH and disolved oxygen level parameters. This logic was reversed in case of the unsafe centroid starting value. The intuition behind this approach comes from the fact that a lower temperature and specific conductance value means lower degree of exothermic reactions and lower amount dissolved salts which are typically the traits of unpolluted water sources, and which also have higher pH value (that is, less acidic) and higher level of dissolved oxygen. Hence, the initial centroid value for the &amp;ldquo;safe&amp;rdquo; category was chosen in this way. For the unsafe cateory, the metrics were simply reversed.&lt;/p>
&lt;p>&lt;strong>Iteration process&lt;/strong>: The next steps for the centroid calculation involves creating an iteration process which will keep updating and perfecting the centroid values upon every execution of the iteration. In this case, the condition for ending the iteration involved either exceeding fifty iterations or reaching the ideal situation where the new centroid value equals the previous centroid value. In the later case, it can be mentioned that the centroid calculation has converged to a specific ideal value. Fortunately, in the case of this project, it was possible to arrive at this convergence of centroid values with not too many iteration steps. The details of this process has been explained in the coding framework with appropriate comments.&lt;/p>
&lt;p>&lt;strong>Assigning of Clusters&lt;/strong>: At the end of the iteration step, we end up with the final centroid values for the safe and unsafe category. With the help of these values, we calculate the euclidean distance for every points and assign them to appropriate clusters to which they are closest to. In this way, we complete the unsupervised algorithm of clustering process for any unlabelled data that is provided to us. In this particular endeavor, we assign the value &amp;ldquo;0&amp;rdquo; for a predicted cluster indicating a safe set of water-quality values for a given point, and a value of &amp;ldquo;1&amp;rdquo; for an unsafe set of water-quality values for a given point. Figure 4 summarizes this idea behind the K-means clustering method that chiefly works as an unsupervised learning algorithm.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/kmeansclustering.png" alt="k-means idea">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Concept of Clustering analysis adopted as an unsupervised learning process &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>&lt;/p>
&lt;h4 id="424-display-of-results--analysis-of-any-given-sample-values-set">4.2.4 Display of results &amp;amp; analysis of any given sample values set&lt;/h4>
&lt;p>In the final step, we display all the results and relevant plots for this big-data analysis task. Additionally, based on the labelled data and predicted classes for safe and unsafe water-quality standards, it will now be possible to find whether an arbitrary set of data values, that represent water-quality data for a particular region, would be classified as safe or unsafe as per this technique. This has also been shown in the results section.&lt;/p>
&lt;h2 id="5-inference">5. Inference&lt;/h2>
&lt;h3 id="51-analysis-of-extracted-data-and-statistical-information">5.1 Analysis of extracted data and statistical information&lt;/h3>
&lt;p>The first preliminary set of results were analyzed to get a general idea of how the water quality paramters vary for the system in question. For the purposes of data visualization, the processed data (viewed as a data-frame in python) was first analyzed to understand the four primary water-quality parameters that are being worked upon. The data-frames for the big data sets are shown below, along with the corresponding statistical information that were evalauted for such attributes. Figure 5 shows the preliminary results that were obtained for the data visualization and statistical processing tasks.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/prelimresults.png" alt="Preliminary results">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Displaying results of data visualization and statistical information for the water-quality parameters&lt;/p>
&lt;p>In the above set of results, it should be worthwhile to note that temperature is measured in the celsius scale, specific conductance is measured in microsiemens per centimeter at 25 degree celsius, pH is measured in the usual standard range (between 0-14), and the level of dissolved oxygen is measured in milligrams per liter.&lt;/p>
&lt;p>Next, the content of the dataset, after it is processed in the software architecture, is plotted. It displays the alteration of the values (expressed in scatter plots) of the four main water-quality parameters (viz. Temperature, Specific Conductance, pH, and Dissolved Oxygen) over the period of time that starts from November 1 to November 14 for the four years involving 2017, 2018, 2019, and 2020.&lt;/p>
&lt;p>Figure 6 displays the scatter plot data for the &amp;ldquo;temperature&amp;rdquo; attribute in the year 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/seventeentemp.png" alt="Temperature 2017">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Temperature&amp;rdquo; (2017)&lt;/p>
&lt;p>Figure 7 displays the scatter plot data for the &amp;ldquo;specific conductance&amp;rdquo; attribute in the year 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/seventeencond.png" alt="Conductance 2017">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Specific Conductance&amp;rdquo; (2017)&lt;/p>
&lt;p>Figure 8 displays the scatter plot data for the &amp;ldquo;pH&amp;rdquo; attribute in the year 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/seventeenph.png" alt="pH 2017">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;pH&amp;rdquo; (2017)&lt;/p>
&lt;p>Figure 9 displays the scatter plot data for the &amp;ldquo;dissolved oxygen&amp;rdquo; attribute in the year 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/seventeendox.png" alt="Dissolved Oxygen 2017">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Dissolved Oxygen&amp;rdquo; (2017)&lt;/p>
&lt;p>Figure 10 displays the scatter plot data for the &amp;ldquo;temperature&amp;rdquo; attribute in the year 2018.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/eighteentemp.png" alt="Temperature 2018">&lt;/p>
&lt;p>&lt;strong>Figure 10:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Temperature&amp;rdquo; (2018)&lt;/p>
&lt;p>Figure 11 displays the scatter plot data for the &amp;ldquo;specific conductance&amp;rdquo; attribute in the year 2018.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/eighteencond.png" alt="Conductance 2018">&lt;/p>
&lt;p>&lt;strong>Figure 11:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Specific Conductance&amp;rdquo; (2018)&lt;/p>
&lt;p>Figure 12 displays the scatter plot data for the &amp;ldquo;pH&amp;rdquo; attribute in the year 2018.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/eighteenph.png" alt="pH 2018">&lt;/p>
&lt;p>&lt;strong>Figure 12:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;pH&amp;rdquo; (2018)&lt;/p>
&lt;p>Figure 13 displays the scatter plot data for the &amp;ldquo;dissolved oxygen&amp;rdquo; attribute in the year 2018.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/eighteendox.png" alt="Dissolved Oxygen 2018">&lt;/p>
&lt;p>&lt;strong>Figure 13:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Dissolved Oxygen&amp;rdquo; (2018)&lt;/p>
&lt;p>Figure 14 displays the scatter plot data for the &amp;ldquo;temperature&amp;rdquo; attribute in the year 2019.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/nineteentemp.png" alt="Temperature 2019">&lt;/p>
&lt;p>&lt;strong>Figure 14:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Temperature&amp;rdquo; (2019)&lt;/p>
&lt;p>Figure 15 displays the scatter plot data for the &amp;ldquo;specific conductance&amp;rdquo; attribute in the year 2019.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/nineteencond.png" alt="Conductance 2019">&lt;/p>
&lt;p>&lt;strong>Figure 15:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Specific Conductance&amp;rdquo; (2019)&lt;/p>
&lt;p>Figure 16 displays the scatter plot data for the &amp;ldquo;pH&amp;rdquo; attribute in the year 2019.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/nineteenph.png" alt="pH 2019">&lt;/p>
&lt;p>&lt;strong>Figure 16:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;pH&amp;rdquo; (2019)&lt;/p>
&lt;p>Figure 17 displays the scatter plot data for the &amp;ldquo;dissolved oxygen&amp;rdquo; attribute in the year 2019.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/nineteendox.png" alt="Dissolved Oxygen 2019">&lt;/p>
&lt;p>&lt;strong>Figure 17:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Dissolved Oxygen&amp;rdquo; (2019)&lt;/p>
&lt;p>Figure 18 displays the scatter plot data for the &amp;ldquo;temperature&amp;rdquo; attribute in the year 2020.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/twentytemp.png" alt="Temperature 2020">&lt;/p>
&lt;p>&lt;strong>Figure 18:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Temperature&amp;rdquo; (2020)&lt;/p>
&lt;p>Figure 19 displays the scatter plot data for the &amp;ldquo;specific conductance&amp;rdquo; attribute in the year 2020.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/twentycond.png" alt="Conductance 2020">&lt;/p>
&lt;p>&lt;strong>Figure 19:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Specific Conductance&amp;rdquo; (2020)&lt;/p>
&lt;p>Figure 20 displays the scatter plot data for the &amp;ldquo;pH&amp;rdquo; attribute in the year 2020.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/twentyph.png" alt="pH 2020">&lt;/p>
&lt;p>&lt;strong>Figure 20:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;pH&amp;rdquo; (2020)&lt;/p>
&lt;p>Figure 21 displays the scatter plot data for the &amp;ldquo;dissolved oxygen&amp;rdquo; attribute in the year 2020.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/twentydox.png" alt="Dissolved Oxygen 2020">&lt;/p>
&lt;p>&lt;strong>Figure 21:&lt;/strong> Scatter plot for the water-quality parameter involving &amp;ldquo;Dissolved Oxygen&amp;rdquo; (2020)&lt;/p>
&lt;h3 id="52-centroids--predicted-classesclusters">5.2 Centroids &amp;amp; Predicted Classes/Clusters&lt;/h3>
&lt;p>Figure 22 shows the final centroid values for the safe and unsafe water-quality standards for the year 2017. Furthermore, Figure 23 shows the predicted classes for the safe and unsafe clusters, which were calculated based on the results of the centroid values, for the same year of 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/centroidsseventeen.png" alt="centroids for 2017">&lt;/p>
&lt;p>&lt;strong>Figure 22:&lt;/strong> Safe and unsafe centroid values for the year 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/classpredictions.png" alt="clusters for 2017">&lt;/p>
&lt;p>&lt;strong>Figure 23:&lt;/strong> Predicted classes for safe (&amp;ldquo;0&amp;rdquo;) and unsafe (&amp;ldquo;1&amp;rdquo;) clusters for the year 2017.&lt;/p>
&lt;h3 id="53-heatmaps-for-the-years---2017-2018-2019-and-2020">5.3 Heatmaps for the years - 2017, 2018, 2019, and 2020&lt;/h3>
&lt;p>For the all the four years, heatmaps were plotted to get more information about the trend of the data. Chiefly, the heatmaps give us an empirical form of ideology relating to the degree of correlation between the different water-quality parameters in this data analysis task.&lt;/p>
&lt;p>Figure 24 visualizes the heat-map and shows the relationships between the various aquatic parameters for the year of 2017.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/heatmapseventeen.png" alt="hmap2017">&lt;/p>
&lt;p>&lt;strong>Figure 24:&lt;/strong> Heatmap for the water-quality parameters (Year - 2017)&lt;/p>
&lt;p>Figure 25 visualizes the heat-map and shows the relationships between the various aquatic parameters for the year of 2018.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/heatmapeighteen.png" alt="hmap2018">&lt;/p>
&lt;p>&lt;strong>Figure 25:&lt;/strong> Heatmap for the water-quality parameters (Year - 2018)&lt;/p>
&lt;p>Figure 26 visualizes the heat-map and shows the relationships between the various aquatic parameters for the year of 2019.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/heatmapnineteen.png" alt="hmap2019">&lt;/p>
&lt;p>&lt;strong>Figure 26:&lt;/strong> Heatmap for the water-quality parameters (Year - 2019)&lt;/p>
&lt;p>Figure 27 visualizes the heat-map and shows the relationships between the various aquatic parameters for the year of 2020.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/heatmaptwenty.png" alt="hmap2020">&lt;/p>
&lt;p>&lt;strong>Figure 27:&lt;/strong> Heatmap for the water-quality parameters (Year - 2020)&lt;/p>
&lt;h3 id="54-analysis-of-sample-set-of-values">5.4 Analysis of sample set of values&lt;/h3>
&lt;p>In this portion, we test the unsupervised learning mechanism on actual sample sets of water-quality values. For this purpose, we feed the sample values to the coding framework and based on the centroids calculated for the past four years, it is able to identify whether the given water sample belong in the safe or unsafe category. Further, if it belongs in the unsafe category, the system can further inform us the degree of criticality of the water-quality degradation. This is carried out by evaluating how many water quality parametric values are beyond the normal range. Based on this analysis, a critical nature is then displayed as an output result. As an example, a critical category of &amp;ldquo;2&amp;rdquo; would signify two of the water-quality parameters were beyond the normal range of values, while the others were normal. Needless to say, higher is the critical category level, the more degraded the water source is.&lt;/p>
&lt;p>Figure 28 displays the results obtained for specific sample values of water quality parameters.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/sampleresults.png" alt="sample values test">&lt;/p>
&lt;p>&lt;strong>Figure 28:&lt;/strong> Analysis of water sample values by ascertaining the degree of degradation based on critical level&lt;/p>
&lt;h3 id="55-benchmark-information">5.5 Benchmark information&lt;/h3>
&lt;p>Finally, the benchmark analysis results are shown below for the respective tasks carried out by the coding platform. Some important facts that should be kept in mind in this regard are as follows:&lt;/p>
&lt;ul>
&lt;li>The benchmark analysis was carried out using the cloudmesh benchmark procedure in Python (executed in Google Colab).&lt;/li>
&lt;li>A sleep-time of &amp;ldquo;5&amp;rdquo; was selected as the standard for the benchmark analysis and it has been adopted consistently for all the other benchmark calculations.&lt;/li>
&lt;li>The time values for the different benchmark results were not rounded off in this case as it was both important and interesting to know the minute differences between the different kinds of tasks that were carried out in this regard. It should be noted that the final benchmark value is exceptionally high since this step involves the part where the human user inputs the sample values for ascertaining the safety standard for a water source.&lt;/li>
&lt;/ul>
&lt;p>Figure 29 shows the benchmark results that were obtained for specific sections in the coding framework.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/bmresults.png" alt="benchmark results">&lt;/p>
&lt;p>&lt;strong>Figure 29:&lt;/strong> Benchmark results for all the tasks that were carried out for this big data analysis task using Google Colab&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>This research endeavor implements a big data analysis framework for analyzing toxicity of aquatic systems. It is an attempt where hardware and software meet together to give reliable results, and on the basis of which we are able to design an elaborate mechanism that can analyze other sample water sources and provide decisions regarding its degradation. Although the endeavor carried out in this example might not involve a plethora of high-end applications or intricate logic frameworks, it still provides a very decent foundational approach for carrying out toxicological analysis for water sources. Most importantly, it should be noted that the results obtained for sample values correspond to what we would normally expect for polluted and non-polluted sources of water. For instance, it was found that water sources with high values of specific conductance were categorized in the &amp;ldquo;unsafe&amp;rdquo; category which is to be expected since a high conductance value typically signifies the presence of dissolved salts and ions in the water source, which normally indicates that effluents or agricultural run-off might have made its way to the water source. Additionally, it was also found out that water samples with high values of dissolved oxygen levels were categorized in the &amp;ldquo;safe&amp;rdquo; category which is certainly true based on biological postulates.&lt;/p>
&lt;p>Of course, a more diverse array of data coupled with more scientific and enhanced ASV systems would have probably provided us with even better results. But as indicated earlier, this research endeavor provides a pragmatic foundational approach to conducting this kind of big data analytical work. We can build up on the logic presented in this endeavor to come up with even more advanced and robust version of toxicological analysis tasks.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Geoffrey Fox, Dr. Gregor von Laszewski, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Valada A., Velagapudi P., Kannan B., Tomaszewski C., Kantor G., Scerri P. (2014) Development of a Low Cost Multi-Robot Autonomous Marine Surface Platform. In: Yoshida K., Tadokoro S. (eds) Field and Service Robotics. Springer Tracts in Advanced Robotics, vol 92. Springer, Berlin, Heidelberg. &lt;a href="https://doi.org/10.1007/978-3-642-40686-7_43">https://doi.org/10.1007/978-3-642-40686-7_43&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>M. Ludvigsen, J. Berge, M. Geoffroy, J. H. Cohen, P. R. De La Torre, S. M. Nornes, H. Singh, A. J. SÃ¸rensen, M. Daase, G. Johnsen, Use of an Autonomous Surface Vehicle reveals small-scale diel vertical migrations of zooplankton and susceptibility to light pollution under low solar irradiance. Sci. Adv. 4, eaap9887 (2018). &lt;a href="https://advances.sciencemag.org/content/4/1/eaap9887/tab-pdf">https://advances.sciencemag.org/content/4/1/eaap9887/tab-pdf&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Verfuss U., et al., (2019, March). A review of unmanned vehicles for the detection and monitoring of marine fauna. Marine Pollution Bulletin, Volume 140, Pages 17-29. Retrieved from &lt;a href="https://doi.org/10.1016/j.marpolbul.2019.01.009">https://doi.org/10.1016/j.marpolbul.2019.01.009&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>USGS Water Quality Data, Accessed: Nov. 2020, &lt;a href="https://waterdata.usgs.gov/nwis/qw">https://waterdata.usgs.gov/nwis/qw&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>EPA Water Quality Data, Accessed Nov. 2020, &lt;a href="https://www.epa.gov/waterdata/water-quality-data-download">https://www.epa.gov/waterdata/water-quality-data-download&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Read, E. K., Carr, L., De Cicco, L., Dugan, H. A., Hanson, P. C., Hart, J. A., Kreft, J., Read, J. S., and Winslow, L. A. (2017), Water quality data for nationalâ€scale aquatic research: The Water Quality Portal, Water Resour. Res., 53, 1735â€“ 1745, doi:10.1002/2016WR019993. &lt;a href="https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2016WR019993">https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2016WR019993&lt;/a> &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Atlas Scientific team. Atlas Scientific Environmental Robotics. Retrieved from &lt;a href="https://atlas-scientific.com/">https://atlas-scientific.com/&lt;/a> &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Sinha, Saptarshi. (2020) A Big-data analysis framework for Toxicological Study. Retrieved from &lt;a href="https://github.com/cybertraining-dsc/fa20-523-312/blob/main/project/code/toxicologyASV.ipynb">https://github.com/cybertraining-dsc/fa20-523-312/blob/main/project/code/toxicologyASV.ipynb&lt;/a> &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Google Colaboratory team. Google Colaboratory. Retrieved from &lt;a href="https://colab.research.google.com/">https://colab.research.google.com/&lt;/a> &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Tan, Steinback, Kumar (2004, April). Introduction to Data Mining, Lecture Notes for Chapter 8, Page 2. Accessed: Nov. 2020, &lt;a href="https://www-users.cs.umn.edu/~kumar001/dmbook/dmslides/chap8_basic_cluster_analysis.pdf">https://www-users.cs.umn.edu/~kumar001/dmbook/dmslides/chap8_basic_cluster_analysis.pdf&lt;/a> &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>Alan J. (2019, November). K-means: A Complete Introduction. Retrieved from &lt;a href="https://towardsdatascience.com/k-means-a-complete-introduction-1702af9cd8c">https://towardsdatascience.com/k-means-a-complete-introduction-1702af9cd8c&lt;/a> &lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report:</title><link>/report/fa20-523-313/assignment5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-313/assignment5/</guid><description>
&lt;h1 id="homework-5">Homework 5&lt;/h1>
&lt;h2 id="student-name-fauzan-isnaini">Student Name: Fauzan Isnaini&lt;/h2>
&lt;h1 id="predicting-stock-market-recovery-in-indonesia-after-covid-19-crash">Predicting Stock Market Recovery in Indonesia after COVID-19 Crash&lt;/h1>
&lt;h2 id="team">Team&lt;/h2>
&lt;p>I will conduct this study by myself.&lt;/p>
&lt;h2 id="topic">Topic&lt;/h2>
&lt;p>The COVID-19 Pandemic is not just a crisis in the public health sector.
It also impacts unemployment rates, business revenues, and mass
psychology, which in the end lead to crashes in global stock markets.
While some stock indexes like the Dow Jones Industrial Average (DJIA)
and NASDAQ Composite already recovered, the Indonesian Stock Market
Index (IDX Composite) is still far below its price before the pandemic.
Some of the possible causes are: 1. Foreign investments represent about
50% of the total fund in the IDX stock exchange. In a pandemic
situation, foreign investors might choose to withdraw their stocks and
find another safer country to invest in. 2. Unpredictability of the
pandemic situation drives investors to reallocate their funds in safer
assets, such as cash, gold, or USD. 3. Changes in the macroeconomic
situation, such as unemployment rate, Indonesian Rupiah (IDR) exchange
rate, and interest rate. 4. Changes in the consumer buying power also
change the business revenues, thus changing fundamental data. 5. Mass
psychology of investors that the stock market is not safe in this
pandemic situation, holding them from returning to the stock market To
predict the time needed for IDX Composite to recover, there are two
indicators that can be utilized: 1. Fundamental indicators, which
represent the financial aspect. This can be in the form of macroeconomic
data and a company financial report 2. Technical indicators, which
represent the mass psychology of investors. This can be obtained from
news, social media, or statistical analysis of how the stock market
moves&lt;/p>
&lt;h2 id="dataset">Dataset&lt;/h2>
&lt;p>In predicting the outcome, I will utilize these datasets: 1. Yahoo
Finance (finance.yahoo.com). Yahoo Finance contains a lot of both
fundamental and technical data, and they are free of charge. 2. Twitter
(twitter.com). I can conduct a content analysis on Twitter to represent
the mass psychology regarding the economic condition in Indonesia 3.
News channel. I can also utilize data crawler software to conduct
content analysis to compare positive and negative news in Indonesia. 4.
Third party stock data feeder. There are some third parties who provide
more comprehensive stock data on a subscription basis. This is another
option if the above datasets are not sufficient&lt;/p>
&lt;h2 id="what-needs-to-be-done-to-get-a-great-grade">What needs to be done to get a great grade&lt;/h2>
&lt;p>I will build a Python program to learn these data and generate a
prediction on when the IDX Composite will recover. I will also consider
the recovery rate of each of these sectors: 1. Mining 2. Agriculture 3.
Finance 4. Infrastructure 5. Miscellaneous Industries 6. Consumers'
Goods 7. Property 8. Trading 9. Basic Industry While they are
incorporated in the IDX Composite, the recovery rate of each sector may
be different because of their respective nature of the industry. For
example, consumer&amp;rsquo;s goods may be impacted less in this COVID-19
pandemic, thus resulting in a faster recovery. On the other hand, the
property sector might be the most impacted sector in this pandemic, thus
resulting in a long time to recover. The program will be able to learn
continuously, so if new data is available, it can renew the analysis and
give a more accurate prediction.&lt;/p></description></item><item><title>Report:</title><link>/report/fa20-523-313/assignment6/assignment6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-313/assignment6/assignment6/</guid><description>
&lt;h1 id="homework-6">Homework 6&lt;/h1>
&lt;p>Student Name: Fauzan Isnaini&lt;/p>
&lt;h1 id="how-ai-helps-diagnosis-and-decision-making-in-health-care-facilities">How AI Helps Diagnosis and Decision Making in Health Care Facilities&lt;/h1>
&lt;h2 id="radiology-assistant">Radiology Assistant&lt;/h2>
&lt;p>Radiology is a branch of medicine that uses imaging technology to diagnose and treat disease. Diagnostic radiology helps health care providers see structures inside your body. Using the diagnostic images, the radiologist or other physicians can often [3]:&lt;/p>
&lt;ol>
&lt;li>Diagnose the cause of your symptoms&lt;/li>
&lt;li>Monitor how well your body is responding to a treatment you are receiving for your disease or condition&lt;/li>
&lt;li>Screen for different illnesses, such as breast cancer, colon cancer, or heart disease
Within radiology, trained physicians visually assess medical images and report findings to detect, characterize and monitor diseases. Such assessment is often based on education and experience and can be, at times, subjective. In contrast to such qualitative reasoning, AI excels at recognizing complex patterns in imaging data and can provide a quantitative assessment in an automated fashion. More accurate and reproducible radiology assessments can then be made when AI is integrated into the clinical workflow as a tool to assist physicians.[1]
Some examples of AIâ€™s clinical application in radiology are [1]:&lt;/li>
&lt;li>Thoracic imaging
AI can help in identifying pulmonary nodules, which can be applied in early detection of lung cancer&lt;/li>
&lt;li>Abdominal and pelvic imaging
AI can help in detecting lesions in abdominal and pelvic. For example, AI can analyze data from computed topography (CT) and magnetic resonance imaging (MRI) to detect liver lesions, and characterize these lesions as benign or malignant. Furthermore, AI can also help in suggesting the follow-up actions for the patient.&lt;/li>
&lt;li>Colonoscopy
Colonic polyps that are undetected or misclassified pose a potential risk of colorectal cancer. AI can help in making an early detection and consistent monitoring of this risk.&lt;/li>
&lt;li>Mammography
Analyzing mammography is technically challenging, even for a trained expert. AI can assist in interpreting the image. For example, AI can identify and characterize microcalcifications. Microcalcifications are tiny deposits of calcium salts that are too small to be felt but can be detected by imaging, and can be an early sign of breast cancer. They can be scattered throughout the mammary gland, or occur in clusters. [4]&lt;/li>
&lt;li>Brain imaging
AI can help in making diagnostic prediction of brain tumors, which are characterized by abnormal growth of brain tissue.&lt;/li>
&lt;li>Radiation oncology
Radiation treatment planning can be automated by segmenting tumours for radiation dose optimization. Furthermore, assessing response to treatment by monitoring over time is essential for evaluating the success of radiation therapy efforts. AI is able to perform these assessments, thereby improving accuracy and speed.&lt;/li>
&lt;/ol>
&lt;h2 id="ai-in-clinical-decision-support">AI in Clinical Decision Support&lt;/h2>
&lt;p>Other than analyzing radiology images, AI can also digest data from blood tests, electrocardiogram (EKG), genomics, and patient medical history do give a better treatment to the patient. AI-enabled clinical decision support includes diagnosis and prognosis, and involves classification or regression algorithms that can predict the probability of a medical outcome or the risk for a certain disease.[5]
Here are some examples of how AI helps clinical decision [6]:&lt;/p>
&lt;ol>
&lt;li>Accumulation of medical histories from birth alongside linked maternal electronic health record (HER) information in a healthcare facility, enabled the prediction of high obesity risk children as early as two years after birth, possibly allowing life-altering preventative interventions.&lt;/li>
&lt;li>The Advanced Alert Monitoring system developed and deployed by Kaiser Permanente uses Intensive Care Unit (ICU) data to predict fatally deteriorating cases and alert staff to the need of life-saving interventions.&lt;/li>
&lt;/ol>
&lt;h2 id="references">References&lt;/h2></description></item><item><title>Report:</title><link>/report/fa20-523-313/project/project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/report/fa20-523-313/project/project/</guid><description>
&lt;h1 id="analyzing-lstm-performance-on-predicting-the-stock-market-for-multiple-time-steps">Analyzing LSTM Performance on Predicting the Stock Market for Multiple Time Steps&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-313/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-313/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final Type: Project&lt;/p>
&lt;p>Fauzan Isnaini, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-313/">fa20-523-313&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-313/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Predicting the stock market has been an attractive field of research for a long time because it promises big wealth for anyone who can find the secret. For a long time, traders around the world have been relying on fundamental analysis and technical analysis to predict the market. Now with the advancement of big data, some financial institutions are beginning to predict the market by creating a model of the market using machine learning. While some researches produce promising results, most of them are directed at predicting the next day&amp;rsquo;s market behavior. In this study, we created an LSTM model to predict the market for multiple time frames. We then analyzed the performance of the model for some different time periods. From our observations, LSTM is good at predicting 30 time steps ahead, but the RMSE became larger as the time frame gets longer.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-macd-in-technical-analysis">2.1 MACD in Technical Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-time-series-forecasting">2.2 Time Series Forecasting&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-using-lstm-in-stock-prediction-and-quantitative-trading">2.3 Using LSTM in Stock Prediction and Quantitative Trading&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-technology">4.1 Technology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-data-preprocessing">4.2 Data Preprocessing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-the-lstm-model">4.3 The LSTM Model&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-results">5. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion-and-future-works">6. Conclusion and Future Works&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> stock, market, predictive analytics, LSTM, random forest, regression, technical analysis&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Stock market prediction is a fascinating field of study for many analysts and researchers becauseof the significant amount of money circulating in the market. While there are numerous studies conducted in this field, predicting the stock market remains a challenging task, because of its noisy and non-stationary nature &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The stock market is &amp;ldquo;noisy&amp;rdquo; because it is sensitive to mass psychology. The trends and patterns in the stock market can also change abruptly because of bad news, natural disasters, and some unforeseen circumstances, thus it is considered non-stationary.&lt;/p>
&lt;p>The efficient market hypothesis even suggests that predicting or forecasting the financial market is unrealistic because price changes in the real world are unpredictable. All the changes in prices of the financial market are based on immediate economic events or news. Investors are profit-oriented, their buying or selling decisions are made according to the most recent events regardless of past analysis or plans. The argument about this Efficient Market Hypothesis has never been ended. So far, there is no strong proof that can verify if the efficient market hypothesis is proper or not &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>However, as Mostafa &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> claims, financial markets are predictable to a certain extent. The past experience of many price changes over a certain period of time in the financial market and the undiscounted serial correlations among vital economic events affecting the future financial market are two main pieces of evidence opposing the Efficient Market Hypothesis.&lt;/p>
&lt;p>The most popular methods in predicting the stock markets are technical and fundamental analysis. Fundamental analysis is mainly based on three essential aspects &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>: (i) macroeconomic analysis such as Gross Domestic Products and Consumer Price Index (CPI) which analyses the effect of the macroeconomic environment on the future profit of a company, (ii) industry analysis which estimates the value of the company based on industry status and prospect, and (iii) company analysis which analyses the current operation and financial status of a company to evaluate its internal value.&lt;/p>
&lt;p>On the other hand, technical analysis is grouped into eight domains &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>: sentiment, flow-of-funds, raw data, trend, momentum, volume, cycle, and volatility. Sentiment represents the behaviors of various market participants. Flow-of-funds is a type of indicator used to investigate the financial status of various investors to pre-evaluate their strength in terms of buying and selling stocks, then, corresponding strategies, such as short squeeze can be adopted. Raw data include stock price series and price patterns such as K-line diagrams and bar charts. Trend and momentum are examples of price-based indicators, trend is used for tracing the stock price trends while momentum is used to evaluate the velocity of the price change and judge whether a trend reversal in stock price is about to occur. Volume is an indicator that reflects the enthusiasm of both buyers and sellers for investing, it is also a basis for predicting stock price movements. The cycle is based on the theory that stock prices vary periodically in the form of a long cycle of more than 10 years containing short cycles of a few days or weeks. Finally, volatility is often used to investigate the fluctuation range of stock prices and to evaluate risk and identify the level of support and resistance.&lt;/p>
&lt;p>While those two are still the most popular approaches, the age of big data has brought a new method to predict the stock market: quantitative analysis. In this new method, the stock market is captured into a mathematical model, and machine learning is used to predict its behavior. Research by Alzazah and Cheng &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> analyzed more than 50 articles to compare various machine learning (ML) and deep learning (DL) methods used to find which method could be more effective in prediction and for which types and amount of data. This research has proven that quantitative analysis with LSTM gives a promising result as the predictor of a stock market.&lt;/p>
&lt;p>In this study, we analyzed the performance of LSTM in predicting the stock market for multiple time frames. LSTM model is used because it is designed to forecast, predict, and classify time series data &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Despite the promising result in LSTM, most of the previous studies are conducted in building a model to predict the next day&amp;rsquo;s price. Thus, we wanted to know how accurate the LSTM model in predicting the stock market for a longer time frame (i.e. from daily to monthly time frame). We also chose to incorporate technical analysis rather than fundamental analysis in our model, because while fundamental analysis tends to be accurate in the yearly period, it could not predict the fluctuation in the given time frame.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;h3 id="21-macd-in-technical-analysis">2.1 MACD in Technical Analysis&lt;/h3>
&lt;p>MACD &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> is an acronym for moving average convergence/divergence. It is a widely used technical indicator to confirm either the bullish or bearish phase of the market. In essence, the MACD indicator shows the perceived strength of a downward or upward movement in price. Technically, itâ€™s an oscillator, which is a term used for indicators that fluctuate between two extreme values, for example, from 0 to 100.
MACD evolved from the exponential moving average (EMA), which was proposed by Gerald Appel in the 1970s. The standard MACD is the 12-day EMA subtracted by the 26-day EMA, which is also called the DIF. The MACD histogram, which was developed by T. Aspray in 1986, measures the signed distance between the MACD and its signal line calculated using the 9-day EMA of the MACD, which is called the DEA. Similar to the MACD, the MACD histogram is an oscillator that fluctuates above and below the zero line. The construction formula of MACD is given in figure 1.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/MACDFormula.png" alt="MACD Formula">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> MACD formula &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The number of the MACD histogram is usually called the MACD bar or OSC. The analysis process of the cross and deviation strategy of DIF and DEA includes the following three steps: (i) Calculate the values of DIF and DEA, (ii)When DIF and DEA are positive, the MACD line cuts the signal line in the uptrend, and the divergence is positive, there is a buy signal confirmation, and (iii)When DIF and DEA are negative, the signal line cuts the MACD line in the downtrend, and the divergence is negative, there is a sell signal confirmation.&lt;/p>
&lt;h3 id="22-time-series-forecasting">2.2 Time Series Forecasting&lt;/h3>
&lt;p>Time series analysis and dynamic modeling &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> is an interesting research area with a great number of applications in business, economics, ï¬nance, and computer science. The aim of time series analysis is to study the path observations of time series and build a model to describe the structure of data and then predict the future values of time series. Due to the importance of time series forecasting in many branches of applied sciences, it is essential to build an effective model with the aim of improving the forecasting accuracy. A variety of time series forecasting models have been evolved in the literature.&lt;/p>
&lt;p>Time series forecasting is traditionally performed in econometric using ARIMA models, which is generalized by Box and Jenkins. ARIMA has been a standard method for time series forecasting for a long time. Even though ARIMA models are very prevalent in modeling economical and ï¬nancial time series, they have some major limitations. For instance, in a simple ARIMA model, it is hard to model the non-linear relationships between variables. Furthermore, it is assumed that there is a constant standard deviation in errors in ARIMA model, which is in practice may not be satisï¬ed. When an ARIMA model is integrated with a Generalized Auto-regressive Conditional Heteroskedasticity(GARCH) model, this assumption can be relaxed. On the other hand, the optimization of a GARCH model and its parameters might be challenging and problematic. There are several other applications of ARIMA for modeling short and long-run effects of economics parameters.&lt;/p>
&lt;p>Recently, new techniques in deep learning have been developed to address the challenges related to the forecasting models. LSTM (Long Short-Term Memory) is a special case of the Recurrent Neural Network (RNN) method that was initially introduced by Hochreiter and Schmidhuber. Even though it is a relatively new approach to address prediction problems. Deep learning-based approaches have gained popularity among researchers.&lt;/p>
&lt;p>LSTM is designed to forecast, predict, and classify time series data even long time lags between vital events that happened before. LSTMs have been applied to solve several problems; among those, handwriting Recognition and speech recognition made LSTM famous. LSTM has copious advantages compared with traditional back-propagation neural networks and normal recurrent neural networks. The constant error backpropagation inside memory blocks enables LSTM ability to overcome long time lags in case of problems similar to those discussed above; LSTM can handle noise, distributed representations, and continuous values; LSTM requires no need for parameter fine-tuning, it works well over a broad range of parameters such as learning rate, input gate bias, and output gate bias &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="23-using-lstm-in-stock-prediction-and-quantitative-trading">2.3 Using LSTM in Stock Prediction and Quantitative Trading&lt;/h3>
&lt;p>During the pre-deep learning era, Financial Time Series modeling has mainly concentrated in the field of ARIMA and any modifications on this, and the result has proved that the traditional time series model does provide decent predictive power to a limit. More recently, deep learning methods have demonstrated better performances thanks to improved computational power and the ability to learn non-linear relationships enclosed in various financial features.&lt;/p>
&lt;p>The direction of the financial market is always stochastic and volatile and the return of the security return is deemed to be unpredictable. Analysts now are trying to apply the modeling techniques from Natural Language Processing into the field of Finance as the similarity of having the sequential property in the data. Zhou &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup> has constructed and applied the Long Short Term Memory Model (LSTM) and the traditional ARIMA model, into the prediction of stock prices on the next day. It was proven that the LSTM model performed better than the ARIMA model.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>This project used the historical data of the Jakarta Composite Index (JKSE) from Yahoo Finance &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. The JKSE is a national stock index of Indonesia, which consists of 700 companies. We choose to incorporate the composite index because it has a beta value of 1, which means it has neutral volatility compared to an individual stock to be incorporated into a model. The dataset contains the Open, High, Low, Close, and Volume data for daily time period on the stock index. The daily data is taken from January 1st, 2013 until November 17th, 2020. We choose the daily data over the monthly data because it offers a more complete pattern. Figure 2 and 3 provides a snapshot of the first few rows of the daily and monthly data respectively.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newdailyhead.png" alt="Head of Daily Data">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Snapshot of the first rows of the daily data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newMonthlyHead.png" alt="Head of Monthly Data">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Snapshot of the first rows of the monthly data&lt;/p>
&lt;p>We also used the MACD technical indicator as an input to our model. The MACD parameters are generated using the ta-lib library &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> based on the Yahoo Finance data. Figure 4 and 5 provides a snapshot of the first few rows of the daily and monthly data respectively after incorporating the MACD technical indicator.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/MACDonDaily.png" alt="Daily MACD">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> MACD on the daily data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newMACDonMonthly.png" alt="Monthly MACD">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> MACD on the monthly data&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;h3 id="41-technology">4.1 Technology&lt;/h3>
&lt;p>Python &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup> was the language of choice for this project. This was an easy decision for these reasons &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>:&lt;/p>
&lt;ol>
&lt;li>Python as a language has an enormous community behind it. Any problems that might be encountered can be easily solved with a trip to Stack Overflow. Python is among the most popular languages on the site which makes it very likely there will be a direct answer to any query.&lt;/li>
&lt;li>Python has an abundance of powerful tools ready for scientific computing. Packages such as Numpy, Pandas, and SciPy are freely available and well documented. Packages such as these can dramatically reduce, and simplify the code needed to write a given program. This makes iteration quick.&lt;/li>
&lt;li>Python as a language is forgiving and allows for programs that look like pseudo code. This is useful when pseudocode given in academic papers needs to be implemented and tested. Using Python, this step is usually reasonably trivial.&lt;/li>
&lt;/ol>
&lt;p>In building the LSTM model, Keras &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup> library is used. It contains numerous implementations of commonly used neural network building blocks such as layers, objectives, activation functions, optimizers, and a host of tools to make working with image and text data easier. The code is hosted on GitHub, and community support forums include the GitHub issues page, a Gitter channel, and a Slack channel.&lt;/p>
&lt;h3 id="42-data-preprocessing">4.2 Data Preprocessing&lt;/h3>
&lt;p>After downloading the historical datasets from Yahoo Finance, the MACD technical indicator is generated using the ta-lib library. Because MACD needs to capture data from the previous time period, the MACD values on the first rows of the data are missing. These rows are then removed before being split into 8:2 proportions for training and testing purposes in the LSTM model.&lt;/p>
&lt;h3 id="43-the-lstm-model">4.3 The LSTM Model&lt;/h3>
&lt;p>A multivariate LSTM model with two hidden layers is used, with a dropout parameter of 0.2. Adam is used as the optimization algorithm. The model uses 90 days time steps, which means it uses the past 90 days of data to predict the output. It has 8 features, which are the Close, Low, High, Open, Volume, MACD, MACD Signal, and MACD Histogram. It then gives one output, which is the open price for the given time frame. We then analyze the performance of our model for each of the time frames.&lt;/p>
&lt;h2 id="5-results">5. Results&lt;/h2>
&lt;p>We used callback function to find the best number of epochs in the model. Figure 6 gives the mean squared error (MSE) curve of the prediction in the training dataset for each given epoch. It shows that the MSE converges after 20 epochs, with a value of 0.0243.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newlossepochs.png" alt="Epoch Loss">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> MSE on the training data for each given epoch&lt;/p>
&lt;p>Figure 7 shows the root mean squared error (RMSE) on the testing dataset for each time frame. It clearly shows that the RMSE becomes bigger on a longer time frame. When predicting the next day period, the RMSE is 323.41, while when predicting 30 days ahead, the RMSE increase to 481.32. But overall, these values are still acceptable because they are smaller than the standard deviation of the actual dataset of 667.31.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/RMSEonTimeFrame.png" alt="RMSE">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> RMSE on the training data for each time frame&lt;/p>
&lt;p>Figure 8 and Figure 9 compare the predicted values on the training data for 1 day and 30 days time frames respectively, while Figure 10 and 11 give the comparison on the test data. It can be seen that the model cannot predict steep ramps in the price change, thus it is lagged from the actual price. The predicted price becomes further lagged when predicting for a longer time frame, thus resulting in a bigger RMSE.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newOneDayPredict.png" alt="Next Day Prediction">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Comparison between the next day prediction and its actual values based on the training data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newThirtyDaysPredict.png" alt="30 Days Prediction">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> Comparison between the 30 days time frame prediction and its actual values based on the training data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newonedaytest.png" alt="Next Day Test">&lt;/p>
&lt;p>&lt;strong>Figure 10:&lt;/strong> Comparison between the next day prediction and its actual values based on the testing data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/thirtydaystest.png" alt="30 Days Test">&lt;/p>
&lt;p>&lt;strong>Figure 11:&lt;/strong> Comparison between the 30 days time frame prediction and its actual values based on the testing data&lt;/p>
&lt;p>We also found that a longer training dataset does not always give a better prediction because the model might overfit with the training data. In fact, when we used historical market data from January 2000, the RMSE became close to 3,000. This might be due to overall the stock market tends to always get higher every year. When the data is too old, the model needs to compensate for the time when the price is still very low.&lt;/p>
&lt;p>We also capture the time needed to run each critical process using cloudmesh-common benchmark and stopwatch framework &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. The stopwatch recordings are shown in Table 1. The table shows that training the model took the longest time. It also highlights the system&amp;rsquo;s specification used in running the program.&lt;/p>
&lt;p>&lt;strong>Table 1:&lt;/strong> Benchmark results&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/benchmark1.png" alt="Table 1">&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/stopwatch.png" alt="Stopwatch">&lt;/p>
&lt;h2 id="6-conclusion-and-future-works">6. Conclusion and Future Works&lt;/h2>
&lt;p>We have analyzed the performance of LSTM in predicting the stock price for different time frames. While it gives a promising result in predicting the next day&amp;rsquo;s price, the prediction becomes less accurate for a longer time frame. This might be due to the non-stationarity nature of the stock market. The stock market trends can change abruptly because of a sudden change in the political and economic conditions. Using the daily market data, our model gives promising results within 30 days time frame.
This project has analyzed the performance of LSTM using RMSE, but further research may measure the performance based on the potential financial gain. After all, the stock market is a place to make money, thus financial gain is a better metric of performance.
Further improvement may also be done on our model. We only used price data and MACD technical indicator for the prediction. Further research may utilize other technical indicators, such as RSI and Stochastics to get a better prediction.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Geoffrey Fox, Dr. Gregor von Laszewski, and the associate instructors in the FA20-BL-ENGR-E534-11530: Big Data Applications course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions concerning exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>D. Shah, H. Isah, and F. Zulkernine, &amp;ldquo;Stock Market Analysis: A Review and Taxonomy of Prediction Techniques,&amp;rdquo; International Journal of Financial Studies, vol. 7, no. 2, p. 26, 2019. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>F. S. Alzazah and X. Cheng, &amp;ldquo;Recent Advances in Stock Market Prediction Using Text Mining: A Survey,&amp;rdquo; E-Business [Working Title], 2020. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>A. Mostafa and Y. S., &amp;ldquo;Introduction to financial forecasting. Applied Intelligence,&amp;rdquo; Applied Intelligence, 1996. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>S. Siami-Namini, N. Tavakoli, and A. S. Namin, &amp;ldquo;A Comparison of ARIMA and LSTM in Forecasting Time Series,&amp;rdquo; 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), 2018. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>G. von Laszewski, &amp;ldquo;cloudmesh/cloudmesh-common,&amp;rdquo; GitHub, 2020. [Online]. Available: &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>. [Accessed: 08-Dec-2020]. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>J. Wang and J. Kim, &amp;ldquo;Predicting Stock Price Trend Using MACD Optimized by Historical Volatility.&amp;rdquo; &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>V. Bielinskas, &amp;ldquo;Multivariate Time Series Prediction with LSTM and Multiple features (Predict Google Stock Price),&amp;rdquo; Youtube, 2020. [Online]. Available: &lt;a href="https://www.youtube.com/watch?v=gSYiKKoREFI">https://www.youtube.com/watch?v=gSYiKKoREFI&lt;/a>. [Accessed: 08-Dec-2020]. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>&amp;ldquo;Composite Index (JKSE) Charts, Data &amp;amp; News,&amp;rdquo; Yahoo! Finance, 08-Dec-2020. [Online]. Available: &lt;a href="https://finance.yahoo.com/quote/%5EJKSE/">https://finance.yahoo.com/quote/^JKSE/&lt;/a>. [Accessed: 08-Dec-2020]. &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>J. Bosco and F. Khan, Stock Market Prediction and Efficiency Analysis using Recurrent Neural Network. Berlin, Germany: 2018, 2018. &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>F. Isnaini, &amp;ldquo;cybertraining-dsc/fa20-523-313,&amp;rdquo; GitHub, 08-Dec-2020. [Online]. Available: &lt;a href="https://github.com/cybertraining-dsc/fa20-523-313/blob/main/project/code/multivariate.ipynb">https://github.com/cybertraining-dsc/fa20-523-313/blob/main/project/code/multivariate.ipynb&lt;/a>. [Accessed: 08-Dec-2020]. &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>TA-Lib. [Online]. Available: &lt;a href="https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html">https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html&lt;/a>. [Accessed: 08-Dec-2020]. &lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item></channel></rss>